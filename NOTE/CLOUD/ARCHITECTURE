大型架构及配置
DAY01 ansible
DevOps
    DevOps（Development和Operations的组合词）是一组过程、方法与系统的统称，用于促进开发（应用程序/软件工程）、技术运营和质量保障（QA）部门之间的沟通、协作与整合。
    它是一种重视“软件开发人员（Dev）”和“IT运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。透过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。
    它的出现是由于软件行业日益清晰地认识到：为了按时交付软件产品和服务，开发和运营工作必须紧密合作.
一.ansible
    使用时 千万要小心  rsync rm 这种都是危险操作
    1.1简介
        ansible是it自动化和DevOps软件 基于python开发 批量部署 批量运行命令
        ansible可以实现:
        -自动化部署app
        -自动化管理配置项目
        -自动化持续交互
        -自动化云服务管理

    1.2ansible 特性
         模块化设计,调运特定的模块完成特定任务
         基于python
         -paramiko
         -PyYAML
         -jinja2
         基于模块支持支持JSON 标准输出格式,可以采用任何编程语言重写
         部署简单
         主从模式工作
         自定义模块
         支持playbook
         易用使用
         支持多层部署
         支持异构it环境
    1.3 为什么选择ansible
        特点:
            社区活跃度
            学习成本
            使用成本
            编码语言
            性能
            使用广发
        优点:
            只要支持ssh和python
            无客户端
            ansible功能强大,模块丰富
            上手容易,门槛低
            二次开发容易
            活跃度高
    1.4 工作流程


            host1           cmd1
            host2           cmd2
            host3           cmd3
            ...             ...

            hostx           cmdx

        每台host 会遍历 命令集合,我们就是让一批机器执行一批命令

        ansible 执行流程

            读取配置
            抓起全量机器&分组列表
            使用host-pattern过滤机器列表
            根据参数确定执行模块和配置
            runner执行返回
            输出,结束


    1.5 安装
        软件依赖关系

            管理主机
                python 2.6 /2.7以上
                ansible需要一下模块
                    -paramiko
                    -PyYAML
                    -jinja2
                    -httplib2
                    -six


            被管理主机
                ansible 默认通过ssh协议管理
                被管理机器要开启ssh服务,允许ansible主机登录
                管理节点需要python2.5以上
                若被管理节点开启selinux,需要安装 libselinux-python,所以尽量关闭selinux


        准备 6 台机器 1.40-1.45
        安装ansible
            yum 安装
                拷贝软件包到
                跟新索引文件 createrepo
                配置yum
            安装
                ansible管理主机上
                yum -y install ansible

                vim /etc/ansible/ansible.cfg
                    行取消注释
                    14 inventory      = /etc/ansible/hosts //主机集合
                    71 host_key_checking = False //不检查配置文件
    1.6主机定义与分组
        安装ansible后
            ########################### 记住啊######################
            ansible配置文件顺序查找
            -先检测ansible_config变量定义的配置文件
            -其次检测当前目录下的./ansible.cfg
            -再检查当前用户家目录下~/ansible.cfg文件
            -最后检查/etc/ansible/ansible.cfg文件
            /etc/ansible/ansible.cfg 是ansiblemore配置文件路径
            ansible.cfg 配置文件
                -inventory 定义托管主机地址配置文件路径
            ########################################################
            定义主机格式
                /etc/ansible/hosts 配置文件 位置
                [组名]
                主机名称/ip 其他参数
                vim /etc/ansible/hosts
                    添加
                    [web]
                    web[1:2]  //范围指定也可以向下面一样写
                    [db]
                    db[1:2]
                    [other]   //
                    cache
    1.7 ansible 命令

        ansible 主机集合 -m 模块 -a 模块参宿
            -主机集合 主机名/分组名 , 分割
            -m 模块名称 默认command模块
            -a / -args 模块参数
            -i inventory 文件路径/ 可执行脚本
            -k 使用交互式登录密码
            -e定义变量
            -v 显示详细信息

            显示可执行主机
            ansible web,db --list-hosts
            批量检测主机
            ansible web -m ping -k
            列出所有的可执行主机
            ansible all  --list-hosts

            部署免密码登录

            ssh-keygen -t rsa -b 2048 -N'' -f /root/.ssh/key
            for i in 41 42 43 44 45
            > do ssh-copy-id -i key.pub root@192.168.1.$i
            > done
            -i 指定 公钥文件
            再次检测
            ansible web -m ping

        inventory 扩展参数
            -ansible_ssh_port
                -ssh端口号: 若不是默认端口通过这个变量设置
                /etc/ansible/hosts
                [web]
                web1 ansible_ssh_port=22
            -ansible_ssh_user
                -默认的ssh用户名

            -ansible_ssh_pass
                -ssh密码 建议使用--ask-pass/ ssh秘钥
            -ansible_ssh_private_key_file
                -ssh使用的私钥文件,使用于多个秘钥,并不想使用ssh代理

            vars变量定义,用于用户组名后面
                [all:vars]   //all表示全部组,
                ansible_ssh_private_key_file="/root/.ssh/key"
            children 子组定义,引用其他组名称
                [app:children]
                web    //其他组名.非主机名
                db

            自定义配置文件
                有多少人就可以有多少配置文件,分组只限制与本文件(用户)
                创建 文件夹 myansible
                创建配置文件ansible.cfg
                    sed -n "14p;61p" /etc/ansible/ansible.cfg
                    [defaults]  //配置文件分组名称
                    inventory = myhost      //14 行
                    host_key_checking = False  //61 行

                配置主机文件
                    [app]
                    web1
                    db1
                    cache
                    [app1]
                    web1
                测试 ansible app -m ping

            动态主机
                无限可能
                    ansible inventory 包含静态和动态的inventory,静态的inventory
                    指在文件/etc/ansible/hosts中指定的主机和组
                    动态inventory指通过外部的脚本获取主机列表,按照要求格式返回给ansible命令

                json
                    Javascriptobjectnotation,Javascript对象表示法,一种基于文本独立于语言的轻量级数据交换格式



    1.8ansible 模块

        ansible-doc和ping模块

            ansible-doc
                -模块的手册相当于shell的man,  (非常重要,ansible必备)
                ansible-doc -l          //列出所有模块
                ansible-doc modulename   //查看帮助
            ping 模块
                测试网络连通性,ping模块没有参数,测试的ssh链接是否成功
                ansible host-pattern -m ping


            command模块
                ansible是并发执行
                默认模块,远程执行命令
                    ansible host-pattern -m command -a 'args'
                    查看机器负载
                    ansible all -m command -a 'free -mh'
                    查看当前时间日期
                    ansible all -m command -a 'date +%F_%T'
                command模块注意事项
                    -该模块通过-a 跟上需要执行的命令可以直接执行,若命令中有如下字符则不能执行
                    "<" ">" "|" "&"
                    -command模块不能解析系统变量
                    -该模块不起动shell 直接在ssh进程中执行,所有使用到shell的命令都会失败
                    - 下面命令会失败
                    -ansible all -m command -a 'ps aux |grep ssh'
            shell 模块
                -shell 模块用法基本和command一样,区别是shell模块通过/bin/sh 执行命令,可以执行任意命令
                -不能执行交互命令 如 vim top

                ansible all -m shell -a "echo ${HOSTNAME}"  //本地bash解析
                    db2 | SUCCESS | rc=0 >>
                    ansible
                ansible all -m shell -a 'echo ${HOSTNAME}'  //远程bash解析
                    db1 | SUCCESS | rc=0 >>
                    DB1

                ansible all -m shell -a 'echo \${HOSTNAME}'
                    web1 | SUCCESS | rc=0 >>
                    ${HOSTNAME}

                testfile 文件在哪里
                    ansible cache -m shell -a 'cd /tmp'
                    ansible cache -m shell -a 'touch testfile'

                问题解析
                    变量解析
                        -ansible 执行 命令是二次解析
                        -第一次在本机解析,第二次在执行机器解析
                        -需要第二次解析的变量要转义(\)
                    文件在哪里
                        -文件子用户家目录
                        -ansible是使用ssh多次链接执行
                        -链接退出后之前状态就全部失效
                        -解决方法使用chdir 代替cd

                给web1,db2 添加用户nb ,修改nb密码为123
                    ansible web1,db2 -m shell -a 'useradd nb && echo 123 |passwd --stdin nb'

            script 模块
                通过该模块可以执行脚本
               -在本地写脚本,然后使用script模块批量执行
               -注意:该脚本包含但不限于shell脚本,只要指定sha-bang解释器的脚本都可以
                ansible cache -m script -a 'useradd.sh'
                    vim useradd
                        #!/bin/bash
                        id nb
                        if [ $? != 0 ];then
                        useradd wk
                        echo 456 | passwd --stdin wk
                        fi

            yum模块
                -通过该模块使用yum包管理器
                -name 要进行操作的软件包名字
                -state 动作 (installed ,removed)
                -installed 安装
                -removed  删除

                ansible db -m yum -a 'name=mariadb-server state=installed'

            service模块
                -name 必选项,服务名
                -enabled 是否开机启动 yes/no
                -sleep 执行restarted,会在stop和start之间睡几秒
                -state 对应当前服务执行启动 停止 重启 重新加载
                -started stopped restarted reloaded

                ansible db -m service -a 'name=mariadb state=started enabled=yes'

            copy模块
                -复制文件远程主机
                -src 复制本地文件到远程主机,绝对路径和相对路径都可以,
                路径为目录时会递归复制.若路径以'/'结尾,则复制目录里面的内容,
                若不以'/'结尾,则复制包含目录在内的整个内容类似rsync
                -dest 必选项.远程主机的绝对路径,如源文件是一个目录,那路径必须是目录
                -backup 覆盖前先备份源文件,备份文件包含时间信息 选项:yes/no
                -force 若目标主机包含文件,单内容不同,若设置为yes,则墙纸覆盖,若设置为no,则只有当目标主机目标位置不存在该文件才会复制,默认为yes
                -复制文件
                    ansible all -m copy -a 'src=my.cnf dest=/etc/my.cnf'

                -复制目录
                    ansible all -m copy -a 'src=/etc/yum.repos.d/ dest=/etc/yum.repos.d/'



            lineinfile 模块
                -一个类似与sed的一种行编辑替换模块
                -path  目标文件
                -regexp 正则表达式,要修改的行
                -line 最终的修改结果
                -例如修改my.cnf,中 bin-log
                ansible db -m lineinfile -a 'path="/etc/my.cnf" regexp="^binlog-format" line="binlog-format = row"'

            replace 模块
                -一个类似与sed的一种行编辑替换模块
                -path  目标文件
                -regexp 正则表达式,要修改的行
                -replace 最终的修改结果
                -例如替换指定字符 row-->mixed
                ansible db -m replace -a 'path="/etc/my.cnf" regexp="row" replace="mixed"'


            setup 模块
                -主要用来获取主机信息,playbooks里经常会用的另一个参数gather_facts与该模块相关,
                setup模块下经常用的是filter参数
                -filter 过滤所需的信息

                ansible cache -m setup -a 'filter=ansible_distribution'


DAY02
    PLAYBOOK
    一.playbook 基础
        1.1ansible 七中武器
            第一种  (必须掌握)
                ansible 命令,用于执行零时性工作
            第二种  (必须掌握)
                ansible-doc 是ansible模块的文档说明,类似于man命令
            第三种 (必须掌握)
                ansible-console是ansible为用户提供的交互的工具
                用户可以使用ansible-console虚拟出来的终端上想使用shell一样使用ansible内置命令
            第四种
                ansible-galaxy从github上下载管理Roles的一款工具与python的pip一样
            第五种 (必须掌握)
                ansible-playbook 是日常使用中频率最高的命令,工作机制:通过读取先编写好的PLAYBOOK文件实现批量管理,可以理解为按照一定的条件组成的ansible任务集
            第六种
                ansible-vault主要用户配置文件加密,如编写的PLAYBOOK文件中包含敏感信息,用他可以加密/解密文件
            第七种
                ansible-pull
                ansible 有两种工作模式 pull/push 默认pull.pull和push的工作机制刚好相反
                适用场景: 有大量机器需要配置,即便适用高并发线程也需要花费很多时间
                通常配置大批量机器先适用,灵活性稍有欠缺,但效率几乎可以无限提升.

        1.2json简介
        json是什么
                json是JavaScript对象表示法,一种基于文本独立于语言的轻量级数据交换格式
                分隔符限于   ' ()  [] {} : , 这几种
                独立于语言,与使用什么编程语言无关
                eg:
                [1,2,3,4]
                {key:value}
        json特性
            json是纯文本
            json具有人类可读性
            json具有层级别结构 --可以互相嵌套
            json可通过Javascript进行解析
        json语法
            -数据在名称/值对中
            -数据由"," 分隔
            -{} 保存对象
            -[] 保存数组
        json 数组的书写格式  名称/值对
            名称/值对 包含字段名称(在""中),后面写一个:,然后是值
            "key":"value"
        json语法规则-数组
           { "key":["k1":"v1",["1","2","3","4"],"k2":["k3":"v3"]]
            }
            犇哥: 牛犇
            讲师:[牛犇,王凯,丁丁,静静,xx]
            讲师:[
                {姓名:牛犇,爱好:大锤},
                {姓名:王凯,爱好:吃香蕉},
                {姓名:丁丁,爱好:写书},
                {姓名:静静,爱好:养猫},
                {姓名:xx,爱好:开车}
            ]

        1.3jinja2介绍
            jinja2 是什么
                jinja2

            jinja2 模板基本语法
                -模板的表达式都是包含在分隔符"{{ }}"内的
                -控制语句都是包含在分隔符"{% %}" 内的
                -模板支持注释,都是包含在分隔符"{# #}" 内,支持块注释
                -调用变量
                    {{username}}
                -计算
                    {{2+3}}
                -判断
                     {{1 in [1,2,3]}}
            jinja2控制语句

                if 判断
                    {% if name == '诗仙' %}
                        dc
                    {% elif name == '诗圣' %}
                        ch
                    {% elif name == '诗魔' %}
                          bj
                    {% else %}
                        lh
                    {% endif %}

                    if
                    elif
                    else
                    endif

                for 循环
                    {% if name == .... %}
                        ... ...
                    {% elif name == '于谦' %}
                       {% for method in [抽烟,喝酒,烫头] %}
                        {{do method}}
                       {% endfor %}
                       ... ...

                    {% endif %}

                    for i in xx
                    do i
                    endfor


        1.4YAML简介
            YAML是什么
                高可读,用来表达数据数列的格式
                YAML已有多种编程语言的支持
            YAML基础语法
                YAML的结构通过空格来展示
                数组使用 - 来表示
                键值对使用 : 来表示
                YAML使用一个固定的缩进风格表示数据层级结构关系
                一般每个缩进级别由两个以上空格组成(千万不能使用table,用来就语法错误)
                #表示注释

                注意: 不要使用table ,缩进是初学者易出错的地方
                      同一层级必须对齐
            YAML的键值
                采用: 分隔
                :后面必须有一个空格
                YAML键值对例子

                key: value /
                key:
                  value
            YAML数组表示
                使用一个短横杠加一个空格
                json
                    讲师:[牛犇,王凯,丁丁,静静,xx]
                YAML
                   讲师:
                     - 牛犇
                     - 王凯
                     - 丁丁
                     - 静静
                     - xx
                json
                    讲师:[
                        {姓名:牛犇,爱好:大锤},
                        {姓名:王凯,爱好:吃香蕉},
                        {姓名:丁丁,爱好:写书},
                        {姓名:静静,爱好:养猫},
                        {姓名:xx,爱好:开车}
                    ]

                YAML

                    讲师:
                      -
                        姓名: 牛犇    ## 每层级 俩个空格分隔,"key:"与"value"间是有一个空格
                        爱好: 大锤
                      -
                        姓名: 王凯
                        爱好: 吃香蕉
                      -
                        姓名: 丁丁
                        爱好: 写书
                      -
                        姓名: 静静
                        爱好: 养猫
                      -
                        姓名: xx
                        爱好: 开车

    二.PLAYBOOK
        什么是PLAYBOOK
            ansible用于配置,部署和管理托管主机剧本,通过PLAYBOOK的详细描述,执行一系列的tasks

        为什么要使用PLAYBOOK
            执行简单的任务,使用ad-hoc

        PLAYBOOK语法基础
            PLAYBOOK语法
                PLAYBOOK由YAML编写,
                #表示注释
                同一列表元素保持缩进
                PLAYBOOK由一个/多个play组成
                play中hosts,variables,roles,tasks等对象表示方法都是键值以':' 分隔
                YAML开始 为---
            playbook构成
                hosts: 定义要执行PLAYBOOK的远程主机组
                vars: 定义PLAYBOOK运行时需要使用的变量
                tasks: 定义将要在远程主机上执行的任务列表
                handlers: 定义tasks执行完成后需要调用的任务
            PLAYBOOK执行结果
                绿色 成功
                红色 失败
                黄色 系统状态改变


            eg1:
                ---
                - hosts: all
                  remote_user: root
                  tasks:
                    - ping:
        变量
            eg2:
                ---
                - hosts: web
                  remote_user: root
                  vars:
                    username: l4
                  tasks:
                    - name: "create user "
                      user:
                        name: "{{username}}"
                        group: users
                    - name: "change_passwd"
                      shell: echo 123 |passwd --stdin "{{username}}"

            eg3:  变量过滤器
                ---
                - hosts: web
                  remote_user: root
                  vars:
                    username: w5
                  tasks:
                    - name: "create user "
                      user:
                        name: "{{username}}"
                        group: users
                        password: "{{'123'|password_hash('sha512')}}"


            eg4:参数传递
                ansible-playbook user.01.yml -e "{username:  ll}"
                ---
                - hosts: web
                  remote_user: root
                  tasks:
                    - name: "create user "
                      user:
                        name: "{{username}}"
                        group: users
                        password: "{{'123'|password_hash('sha512')}}"

            eg5: 参数传递
                ansible-playbook user.01.yml -e @username.yml

                vim username.yml
                ---
                username:
                  nb
                vim  user.01.yml
                ---
                - hosts: web
                  remote_user: root
                  tasks:
                    - name: "create user "
                      user:
                        name: "{{username}}"
                        group: users
                        password: "{{'123'|password_hash('sha512')}}"
            eg6: 安装httpd

                ---
                - hosts: web
                  remote_user: root
                  tasks:
                    - yum:
                        name: httpd
                        state: latest
                    - lineinfile:
                        path: /etc/httpd/conf/httpd.conf
                        regexp: '^Listen 80'
                        insertafter: '^#Listen '
                        line: 'Listen 8080'
                    - copy:
                        src: /root/index.html
                        dest: /var/www/html/index.html
                        owner: apache
                        group: apache
                        mode: 0644
                    - service:
                        name: httpd
                        state: started
                        enabled: yes

                vim index.html
                    html1
        erro模块
            忽略错误继续执行,
            ignore_errors: True
                True  忽略错误继续执行
                False 默认 遇到错误停止

                eg: 在web 上的创建 /tmp/cache 并 修改配置文件重新加载配置文件
                    ---
                    - hosts: web
                      remote_user: root
                      tasks:
                        - shell: mkdir /tmp/cache
                          ignore_errors: True
                        - copy:
                            src: /root/httpd.conf
                            dest: /etc/httpd/conf/httpd.conf
                            owner: apache
                            group: apache
                            mode: 0644
                        - service:
                            name: httpd
                            state: restarted
                            enabled: yes

        tag 标识
            调用 : ansible-playbook apacher_tags.yml -t syncconf
                ---
                - hosts: web
                  remote_user: root
                  tasks:
                #    - shell: mkdir /tmp/cache
                #      ignore_errors: True
                    - copy:
                        src: /root/httpd.conf
                        dest: /etc/httpd/conf/httpd.conf
                        owner: apache
                        group: apache
                        mode: 0644
                      tags: syncconf  ##与copy同级
                    - name: restart apache
                      service:
                        name: httpd
                        state: restarted
                        enabled: yes
        when 条件触发
            需要满足特定条件后出发某一项操作,或在特定条件下执行某个行为

            tasks:
              - name: somecommand
                  command: somecommand
                  when: expr
            eg:
                ---
                - name: Install vim
                  hosts: all
                  tasks:
                    - name: Install vim yum
                      yum: name=vim-enhanced state=installed
                      when: ansible_os_family == "RedHat"
                    - name: Install vim apt
                      apt: name=vim state=installed
                      when: ansible_os_family == "Debian"
        with_itmes  playbooks 标准循环

            通过{{item}} 获取每次迭代的值
            创建多个用户
            ---
            - hosts: web
              remote_user: root
              tasks:
                - name: add users
                  user: group=wheel |password="{{'123456'|password_hash('sha512')}} name={{item}}"
                  with_items:["nd","dd","plj","lx"]

            with_items 进阶

                为不同用户定义不同组
                ---
                - hosts: db


                  tasks:
                    - name: adduser "{{item.name}}"
                      user:
                        name: "{{item.name}}"
                        group: "{{item.group}}"
                        password: "{{item.pwd|password_hash('sha512')}}"
                      with_items:
                        -
                          name: nb
                          group: nobody
                          pwd: "123456"
                        -
                          name: wk
                          group: users
                          pwd: "654321"
                        -
                          name: dd
                          group: adm
                          pwd: nginx
                        -
                          name: jj
                          group: mysql
                          pwd: 123qqq...A
                        -
                          name: xx
                          group: root
                          pwd: a




        register  保存前一个命令的结果
            有时候我们需要前面一个命令的执行结果来判断是否要执行后面的操作,
            我们需要register模块来保存前一个命令的返回状态,在后面调用
            - command: test command
              register: result
            - command: run command
              when: result
            变量注册进阶
                -针对运行命令结果返回值做判定
                -当系统负载超过一定值做特殊处理

                vim dubug.yml
                ---
                - hosts: web
                  remote_user: root
                  tasks:
                    - shell: uptime | awk '{printf("%.2f",$(NF-2))}'
                      register: result      ####显示详细的错误信息
                    - service:
                        name: httpd
                        state: stopped
                      when: result.stdout|float > 0.7   #####result.stdout|float将输出字符转化为数值, 经过测试不转化为float也可以执行成功









        handlers 触发器
            ansible-playbook apacher_change.yml
                ---
                - hosts: web
                  remote_user: root
                  tasks:
                #    - shell: mkdir /tmp/cache
                #      ignore_errors: True
                    - copy:
                        src: /root/httpd.conf
                        dest: /etc/httpd/conf/httpd.conf
                        owner: apache
                        group: apache
                        mode: 0644
                #      tags: syncconf
                      notify:
                        - restart apache
                  handlers:  ###与tasks 同级 ,执行玩tasks后 才会执行 handler
                    - name: restart apache
                      service:
                        name: httpd
                        state: restarted
                        enabled: yes







        include and roles 调用PLAYBOOK文件
            include
                由于PLAYBOOK文件会随着项目越来越大,PLAYBOOK也会越来越复杂,我们可以把
                play tasks handler 分别写入不同文件 通过include指令包含进来

                tasks:
                  -include: tasks/setup.yml
                  -include: tasks/user.yml user=plj  ##user.yml 中可以通过{{user}} 来使用变量

                handler:
                  -include: handler/handlers.yml
            roles
                roles 像是一个加强版的include,他可以引入一个项目的文件和目录
                一般所需的目录层级有
                 -vars: 变量
                 -tasks: 任务层
                 -handler:出发层
                 -file: 文件
                 -template: 模板
                 -default: 默认,优先级最低

                 eg: 加入有一个play包含一个叫x的role则
                    ---
                    -hosts: host_group
                    roles:
                      -x
                 -x/tasks/main.yml
                 -x/vars/main.yml
                 -x/handler/main.yml
                 -x/.../main.yml
                 这些都会添加到这和play







        debug 模块
            检测语法 ansible-playbook --syntax-check test.yml

            运行测试 ansible-playbook -C test.yml
                由于测试和检测语法的问题可能 会检测失败,但是写的PLAYBOOK 并没有问题
            调用:ansible-playbook dubug.yml
            当一分钟之内的cpu使用率达到70%后就停止httpd服务,在报错时显示详细信息
            vim dubug.yml
            ---
            - hosts: web
              remote_user: root
              tasks:
                - shell: uptime | awk '{printf("%.2f",$(NF-2))}'
                  register: result      ####显示详细的错误信息
                - service:
                    name: httpd
                    state: stopped
                  when: result.stdout|float > 0.7   #####result.stdout|float将输出字符转化为数值, 经过测试不转化为float也可以执行成功
                - name: show debug info
                  debug: var=result

DAY03
    一.elk
        1.1 elk是什么
            elk 是一套日志监控解决方案 ,三个软件产品字母缩写,均为开源软件
            elasticsearch  日志检测和存储   (数据库)
            logstash       日志的收集和分析处理  (Java程序,处理log可视化-类似于php)
            kibana         负责日志的可视化  (web软件--类似nginx)

            虽然elk可zabbix都是监控软件但是,这是两个不同的监控方向,zabbix 监控服务,机器状态,elk监控日志


        1.2 elk能做什么
            -分布式日柱数据集中式查询和管理
            -系统监控,包含系统硬件和应用的各个组件的监控
            -故障排查
            -安全信息的事件管理
            --报表功能
        1.3 elasticsearch 特点
            非关系型数据库
            实时性
            分布式实时文档存储
            所有对象全是文档
            高可用 易于扩展 支持集群 分片和复制
            接口友好 支持json

            es 没有什么
            典型意义的事物
            面向文档的数据库
            没有授权和认证
        1.4 elasticsearch 相关概念

            node 装有es服务器节点
            cluster 有多个node组成集群
            document 一个可被搜索的基础信息单元     row       行
            index 拥有相似特征的文档集合           database
            type 一个索引中可定义一种/多种类型      table     表
            filed es最小单位 相当于数据的某一列     column    列
            shards 索引的分片,每一个分片就是shard
            replicas 索引的拷贝
            关系行
            DB--->database-->table-->rows-->columns

                                      行        列
            ES-->index-->types--> documents-->fields
                 索引     类型      文档          预


        1.5 安装

            准备 7台虚拟机
            配置 elk的 yum源
             createrepo .

            单台
            yum -y install  java-1.8.0-openjdk
            yum -y install elasticsearch
            vim +54 /etc/elasticsearch/elasticsearch.yml
            network.host: 0.0.0.0  //修改如下
            systemctl restart elasticsearch.service
            ss -antulp |grep 9200
            ss -antulp |grep 9300
            systemctl enable elasticsearch.service
            集群
            安装软件相同 只是修改的配置文件 有所不同
            vim /etc/hosts  //每台机器都要做
            192.168.1.51 es1
            192.168.1.52 es2
            192.168.1.53 es3
            192.168.1.54 es4
            192.168.1.55 es5
            192.168.1.56 kibana
            192.168.1.57 logstash
            192.168.1.58 Apache
            vim /etc/elasticsearch/elasticsearch.yml
            17 cluster.name: myelk        //配置集群名字
            23 node.name: es1        //当前主机名称,集群中node.name必须不同,与主机名一致,之前修改了hosts
            54 network.host: 0.0.0.0     // 0.0.0.0（监听所有地址）
            68 discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]
            //声明集群里的主机成员有谁，不需要全部写进去,但是在启动其他机器的服务时这三台机器的服务必须先启动.
            集群中的其他机器在启动后会寻址这三台机器来确认集群
            systemctl restart elasticsearch.service
            启动没有 报错信息 若检查9300/9200 无端口 请检查 systemctl status elasticsearch.service 和  vim /var/log/messages 拍错
            访问测试 查看node 数量
            firefox http://192.168.1.51:9200/_cluster/health?pretty
                "number_of_nodes" : 5,

        安装插件
               27  cd /usr/share/elasticsearch/bin/
               28  ./plugin install ftp://192.168.1.254/elk/bigdesk-master.zip
               31  ./plugin install ftp://192.168.1.254/elk/elasticsearch-head-master.zip
               32  ./plugin list
               33  ./plugin install ftp://192.168.1.254/elk/elasticsearch-kopf-master.zip
               34  ./plugin list

        访问测试

            http://192.168.1.51:9200/_plugin/bigdesk/#nodes
            http://192.168.1.51:9200/_plugin/kopf/#!/cluster
            http://192.168.1.51:9200/_plugin/head/
                索引名称     nsd
                分片数        5    将数据分成5份存在服务器
                副本数量       2   每台服务器报错一份源数据两份备份

                最多坏两台服务器



        补充知识
            http协议

                GET POST HEAD

                OPTIONS PUT DELETE TRACE和CONNECT
            ES 常用
                PUT         增
                DELETE      删
                POST        改
                GET         查

            命令 curl
                   -A 修改请求 agent -A
                   -X 设置请求方法
                   -i 显示返回头信息

                   curl -A Safari/537.36 -Li http://www.jkgaoyuan.ml |head -10

                      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                                     Dload  Upload   Total   Spent    Left  Speed
                    100  1104    0  1104    0     0   1319      0 --:--:-- --:--:-- --:--:--  1318HTTP/1.1 200 OK
                    Server: nginx/1.16.0
                    Date: Tue, 16 Jul 2019 06:45:32 GMT
                    Content-Type: text/html; charset=UTF-8
                    Transfer-Encoding: chunked
                    Connection: keep-alive
                    X-Powered-By: PHP/5.6.40

                   curl -A Safari/537.36 -Li -X PUT http://118.144.89.240/info.php

                    HTTP/1.1 200 OK
                    Server: nginx
                    Date: Tue, 16 Jul 2019 06:48:24 GMT
                    Content-Type: text/html; charset=UTF-8
                    Transfer-Encoding: chunked
                    Connection: keep-alive
                    Set-Cookie: PHPSESSID=f13e90b9a3b7fcc167e5e69c88445099; path=/
                    Expires: Thu, 19 Nov 1981 08:52:00 GMT
                    Cache-Control: no-store, no-cache, must-revalidate
                    Pragma: no-cache

                    <pre>
                    [ REQUEST_METHOD] ==> PUT
                    [    REMOTE_ADDR] ==> 223.255.15.21
                    [HTTP_USER_AGENT] ==> Safari/537.36
                    [   HTTP_REFERER] ==>



            RESTful API 调用
                显示 所有cat目录
                curl  http://192.168.1.51:9200/_cat/
                curl  http://192.168.1.51:9200/_cat/recovery
                显示 详细的 health信息 ?v 表示显示详细信息
                curl  http://192.168.1.51:9200/_cat/health?v

                通过命令行创建索引
                [root@es1 ~]# curl -XPUT http://192.168.1.51:9200/tedu -d '{
                >   "settings":{
                >     "index" :{
                >       number_of_shards: 5,
                >       number_of_replicas: 1
                >               }
                >               }
                > }'
                这里的ip地址填写从服务器/主服务器都可以
                插入数据
                                                    索引   类型   域
                curl -XPUT http://192.168.1.51:9200/tedu/teacher/3 -d '{
                "name": "ff",
                "age": "35",
                "gender": "man",
                "like": "chaoyan"
                }'

                修改数据
                    -XPOST  _update  {"doc": {}} 为固定格式
                curl -XPOST http://192.168.1.51:9200/tedu/teacher/4/_update -d '{"doc": {"gender":"wmen"}}'

                查询
                curl -XGET http://192.168.1.51:9200/tedu/teacher/6?putty
                删除
                    就是不能删除类型
                删除字段
                curl -XDELETE http://192.168.1.51:9200/tedu/teacher/5?putty
                删除索引
                curl -XDELETE http://192.168.1.51:9200/tedu/
                删库
                curl -XDELETE http://192.168.1.51:9200/*

                批量导入数据

                lftp 192.168.1.254
                cd elk/
                get logs.jsonl.gz
                logs.jsonl.gz
                shakespeare.json.gz

                gzip -d *.gz

                curl -XPOST 'http://192.168.1.51:9200/_bulk' --data-binary @logs.jsonl
                curl -XPOST 'http://192.168.1.51:9200/_bulk' --data-binary @shakespeare.json
                curl -XPOST 'http://192.168.1.51:9200/x/xx/_bulk/' --data-binary @accounts.json

                accounts.json
                {"index":{"_id":"1"}}^M
                shakespeare.json
                {"index":{"_index":"shakespeare","_type":"act","_id":0}}

                这是表中必须包含的,但是account.json 缺少了 索引和 类型,我们应在导入数据时补上索引和类型
                索引 类型 字段



        1.6kibana
            安装 配置yum,使用 es1 使用的 yum源

                yum -y install kibana
            修改配置文件
                 vim /opt/kibana/config/kibana.yml


                  2 server.port: 5601

                  5 server.host: "0.0.0.0"

                  15 elasticsearch.url: "http://es1:9200"


                  23 kibana.index: ".kibana"

                  26 kibana.defaultAppId: "discover"

                  53   elasticsearch.pingTimeout: 1500
                  57   elasticsearch.requestTimeout: 30000
                  64   elasticsearch.startupTimeout: 5000



                  systemctl restart kibana.service
                  http://192.168.1.56:5601/status

                  状态为green则为正常

                  http://192.168.1.56:5601/

                    discover

                    kibana选择日志

                      支持通配符
                      选择logstash-*
                      time-field  选择 @timestramp
                      create

                      选择时间范围  右上角
                      15.5.17  51.5.21
                      框选


                    visualiz

                    pie chart


        1.7 logstash

            安装
                 yum -y install java-1.8.0-openjdk
                 yum -y install logstash

                 安装位置
                    /opt/logstash/

                    默认没有 配置文件

                 编写配置文件
                    默认包含三部分
                        input{}  输入
                        filter{} 处理
                        output{} 输出
                    vim /opt/logstash/logstash.conf

                    input{
                            stdin{}
                    }
                    filter{ }
                    output{
                            stdout{}
                    }

                 启动
                    -f 指定配置文件位置
                    /opt/logstash/bin/logstash -f logstash.conf

                 查看logstash 已经安装的插件
                    /opt/logstash/bin/logstash-plugin list

                 由于logstash的插件众多,可以访问下面两个 网站寻找帮助
                    https://github.com/logstash-plugins
                    https://www.elastic.co/guide/en/logstash/current/index.html   // INPUT filter OUTPUT 这三项


                 配置file插件
                    vim /opt/logstash/logstash.conf


                    input{
                            file{
                            start_position => "beginning"    // 开始读取log文件的位置,这个参数受到下面文件的影响,并且优先级高于 position
                            sincedb_path => "/var/lib/logstash/sinedb-access"    // 将指针文件定向到 /var/lib/logstash/sinedb-access, 该文件决定了服务启动后,读取log文件位置,该文件默认在 /root .sincedb_xxx
                            path => ["/tmp/a.log","/var/tmp/b.log"]
                            type => "a.log"                  //标记

                            }
                    }

                    filter{ }
                    output{
                            stdout{ codec => "rubydebug"}
                    }


DAY04
    大型架构的配置及其技术

    一.大数据
        定义:无法再一定时间范围内用常规软件工具进行捕捉管理和处理的数据集合

            从海量数据中快速获取有价值的信息.

            大规模并行处理数据库,数据挖掘,分布式文件系统,/数据库,云计算和可扩展的存储系统

        特点: 数量(vloume)  速度(velocity) 种类(variety) 价值(value) 真实性(veracity)

    二.Hadoop
        定义: 分析和处理海量数据的软件平台
              开源 Java分支
              分布式基础架构

              高可靠 高扩展 高效性 高容错 低成本
        最先有Google 发布的三篇论文  产生 GFS MapReduce BigTable 组成
        之后有Yahoo 依照论文用Java 进行实现
            GFS------> HDFS
            MapReduce------>  MapReduce
            BigTable------> Hbase


        1.1Hadoop 常用组件

            HDFS      hadoop分布式文件系统  核心
            mapreduce 分布式计算框架        核心
            Yarn      集群资源管理系统      核心
            Zookeeper 分布式协作服务
            Hbase     分布式列存数据库
            Hive      基于hadoop的数据仓库
            Sqoop     数据同步工具

        1.2 hdfs 角色及概念
            1.2.1 hdfs结构
                client
                    切分文件
                    访问hdfs
                    与namenode交互,获取文件位置信息
                    与datanode交互,读取和写入数据
                datanode
                    数据存储节点,存储实际数据
                    汇报存储信息给namenode
                namenode
                    master节点,管理hdfs的名称空间和数据块映射信息(该文件名fsimage什么数据存储位置),
                    配置副本策略(数据存储几份),处理所有客户请求,(管理数据具体存储位置)

                secondary namenode
                    定期合并fsimage 和fsedits(数据变更日志,记录数据何处变化), 推送给namenode
                    紧急情况下,辅助恢复namenode
                    但是secondary namenode 非namenode 热备份
                block
                    每块128m大小
                    每块多副本

            1.2.2 mapreduce

                    由Java实现的分布式计算框架
                    jobtracker  分解任务
                        master节点
                        管理所有的作业/任务监控/错误处理
                        将任务分解,分配给tasktracker
                    tasktracker 执行任务

                    map task
                        分析记录每条数据记录,传递给用户编写的map()并执行,将输出结果写入本地磁盘
                        -若map-only 作业,直接写入 hdfs
                    reducer task
                        从map task的执行结果中,远程读取输入数据,对数据进行排序,将数据安装分组,传递给用户编写的reduce函数执行


            1.2.3 yarn
                    核心: 将jobtracker 和task tracker 分离,由下面几大组件构成
                         -resourcemanager 一个全局的资源管理节点
                         -nodemanager   每个节点(RM)代理
                         -applicationmaster 表示每个应用
                         -每一个applicationmaster有多个container在nodemanager上运行


                    resource manager
                       处理客户端请求
                       启动/监控applicationmaster
                       监控nodemanager
                       资源分配与调度

                    nodemanager

                        单个节点的资源管理
                        处理来自resourcemanager
                        处理来自applicationmaster
                    container

                        对任务运行环境的抽象,封装了cpu 内存
                        对维度资源及环境变量,启动命令的任务运行相关的信息
                    applicationmaster

                        数据切分
                        为应用申请资源,并分配内部任务
                        任务监控与容错
                    client
                        用户与yarn的交互客户端
                        提交应用程序,监控 应用程序状态,杀死应用

        1.3 hadoop
                 注意:所有的hadoop的文件必须一致
                模式

                    单机
                    伪分布式  测试开发
                    完全分布式  生产


                单机install Hadoop

                    安装Java环境
                    yum -y install java-1.8.0-openjdk-devel


                    tar -xf hadoop-2.7.7.tar.gz  -C

                    cp -r hadoop-2.7.7/ /usr/local/hadoop/

                    配置Java的运行环境
                    vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh

                    指定 Java安装路径
                    rpm -ql java-1.8.0-openjdk
                    export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre"
                    指定hadoop的配置文件 位置
                    export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"

                    cd /usr/local/hadoop
                    mkdir testinput

                    cp LICENSE.txt  NOTICE.txt README.txt testinput/
                    ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount testinput/ testoutput

                    wordcount 是内置的命令
                    wordcount 输入目录  输出目录(会自己创建)  我们只需要准备好输入目录中的内容就好

                    查看结果
                    cat testoutput/part-r-00000


                伪分布式

                    伪分布式的安装和完全分布式类似,区别在与所有的角色安装在一台机器上面,使用本地磁盘,一般生产环境都会使用完全分布式,伪分布式用来学习和测试

                    伪分布式配置和完全分布式配置类似

                hadoop配置文件及格式

                    xml文件配置格式
                    <property>
                        <name>关键词</name>
                        <value>变量</value>
                        <description>描述</description>
                    </property>


                搭建完全分布式

                    准备三台机器(两台node 一台namenode,算上上午的node1 一共四台) 配置/etc/hosts
                      ping通namenode
                      192.168.1.60 node1
                      192.168.1.61 node2
                      192.168.1.62 node3
                      192.168.1.63 namenode

                      java -version 验证java
                      jps 验证角色

                    确保 namenode 能面秘钥远程 node1-3 包括本机

                        vim /etc/ssh/ssh_config
                        Host *
                        GSSAPIAuthentication yes
                        StrictHostKeyChecking no
                                     加密长度 算法  不需要密码  文件名称
                        ssh-keygen -b 2048 -t rsa -N '' -f .ssh/id_rsa

                        ssh-copy-id root@node1
                        ssh-copy-id root@node2
                        ssh-copy-id root@node3
                        ssh-copy-id root@namenode


                    修改hdfsde配置文件

                        下面配置文件的参数可以在下面网站的左下角找到
                        http://hadoop.apache.org/docs/stable/

                        ##################
                        core-site.xml
                        #################
                        vim /usr/local/hadoop/etc/hadoop/core-site.xml

                                hadoop.tmp.dir  数据的根目录, 类似mysql /usr/lib/mysql
                            将<configuration>中添加如下

                            <configuration>

                                <property>
                                    <name>fs.defaultFS</name>
                                    <value>hdfs://namenode:9000</value>
                                </property>
                                <property>
                                    <name>hadoop.tmp.dir</name>
                                    <value>/var/hadoop</value>
                                </property>

                            </configuration>

                            ##############
                            hdfs-site.xml
                            #############

                            vim /usr/local/hadoop/etc/hadoop/core-site.xml
                                哪里启动 namenode
                                哪里启动 secondary namenode

                            <configuration>
                                <property>
                                    <name>dfs.namenode.http-address</name>
                                    <value>namenode:50070</value>
                                </property>
                                <property>
                                    <name>dfs.namenode.secondary.http-address</name>
                                    <value>namenode:50090</value>
                                </property>
                                <property>
                                    <name>dfs.replication</name>
                                    <value>3</value>
                                </property>
                            </configuration>

                            ###########
                            slaves
                            ###########
                                声明谁是datanode
                            vim /usr/local/hadoop/etc/hadoop/slaves

                            node1
                            node2
                            node3

                        同步配置

                          注意:所有的hadoop的文件必须一致

                          scp -r /usr/local/hadoop/ root@node2:/usr/local/hadoop/
                          scp -r /usr/local/hadoop/ root@node3:/usr/local/hadoop/
                          scp -r /usr/local/hadoop/ root@namenode3:/usr/local/hadoop/

                        在namenode上初始化并启动服务,检查集群状态

                            cd /usr/local/hadoop
                            ./bin/hdfs namenode -format    //初始化
                            ./sbin/start-dfs.sh            //启动服务
                            ./bin/hdfs dfsadmin -report    // 检查集群状态

                            ./sbin/stop-dfs.sh              //停止服务
                            ./sbin/stop-all.sh              //停止所有服务

                            jps 验证

                                23985 Jps
                                23747 SecondaryNameNode
                                23564 NameNode

                        日志位置
                            每台机器都有
                            /usr/local/hadoop/logs/

                        rsync -avXSH



                    修改 mapreduce 和 Yarn 的配置文件

                        官方手册

                        下面配置文件的参数可以在下面网站的左下角找到
                        http://hadoop.apache.org/docs/stable/

                        ssh root@namenode
                        cd /usr/local/hadoop/etc/hadoop/
                        mv mapred-site.xml.template mapred-site.xml

                        mapreduce

                        vim mapred-site.xml

                        <configuration>
                        <property>
                            <name>mapreduce.framework.name</name>
                            <value>yarn</value>
                        </property>

                        </configuration>

                        Yarn

                        vim yarn-site.xml

                        <configuration>

                        <!-- Site specific YARN configuration properties -->

                        <property>
                                <name>yarn.resourcemanager.hostname</name>
                                <value>namenode</value>
                        </property>
                        <property>
                                <name>yarn.nodemanager.aux-services</name>  //计算框架
                                <value>mapreduce_shuffle</value>            //由开发决定
                        </property>
                        </configuration>

                    同步配置文件

                        for i in node{1..3};do rsync -avXSH /usr/local/hadoop/etc ${i}:/usr/local/hadoop/; done

                    启动
                        因为上面已经启动了dfs ,这里只需要启动yarn
                        /usr/local/hadoop/sbin/start-yarn.sh


                    检查
                        namenode
                        jps     // java ps

                            23747 SecondaryNameNode
                            24248 ResourceManager
                            24507 Jps
                            23564 NameNode

                            namenode 主机
                            注意这里ip需要与你设定的ip一致
                        http://192.168.1.63:50070/        //--namenode web页面
                        http://192.168.1.63:50090/        //--secondory namenode web页面
                        http://192.168.1.60:50075/        //--datanode web页面（node1,node2,node3）
                        http://192.168.1.63:8088/        //--resourcemanager web页面
                        http://192.168.1.60:8042/        //--nodemanager web页面（node1,node2,node3）








                        在namenode中创建文件夹

                            /usr/local/hadoop/bin/hadoop fs -mkdir /test   // -后面写 shell命令
                            /usr/local/hadoop/bin/hadoop fs -ls /
                            /usr/local/hadoop/bin/hadoop fs -put id_rsa id_rsa.pub /test  //上传本地文件到 hadoop, 前面本机文件,后面hadoop文件
                            /usr/local/hadoop/bin/hadoop fs -get /test /test1   //下载 签名写hadoop的 后面写本机,若没有目录会自己创建

                            但是只支持部分命令,touch在 hadoop 中 应该写为 touchz
                            我们将需要统计的文件上传到 hadoop文件系统中进行分析
                            cd /usr/local/hadoop  不去 目录 会table 不出 xxx.jar                          统计方式    hadoop文件非本机文件
                            ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /test/ /out

                    扩展节点

                        hdfs 增加节点 (修复节点操作与增加相同)

                            启动新服务器,做namenode ,免密登录
                            修改所有的/etc/hosts,增加新节点
                            安装openjdk 和 openstack-devel

                            修改namenode的 slave 文件添加node4
                            同步/usr/local/hadoop 到node4 /usr/local下
                            在该节点启动datanode
                                cd /usr/local/hadoop
                                ./sbin/hadoop-daemon.sh start datanode
                            查看
                             node4 jps
                             namenode  /usr/local/hadoop/bin/hdfs dfsadmin -report
                                4个

                            同步数据
                                    ./bin/hdfs dfsadmin --help
                                    ./bin/hdfs dfsadmin -setBalancerBandwidth 5000000  //设置同步带宽
                                    ./sbin/start-balancer.sh    //同步
                                    ./bin/hdfs dfsadmin -report

                        Yarn 节点
                            列出 yarn节点
                            ./bin/yarn node -list

                             ./sbin/yarn-daemon.sh start nodemanager  //启动
                             ./bin/yarn node -list              //查看节点
                             ./sbin/yarn-daemon.sh stop nodemanager
                             停止节点后 并不会立刻消失而是,等待一段时间后消失

                        删除节点
                            hdfs 节点

                            配置
                            cd /usr/local/hadoop/
                            vim ./etc/hadoop/hdfs-site.xml
                                在<configuration> </configuration>
                            添加
                                    <property>
                                        <name>dfs.hosts.exclude</name>
                                        <value>/usr/local/hadoop/etc/hadoop/exclude</value>
                                    </property>


                            vim ./etc/hadoop/exclude

                                node4
                             ./bin/hdfs dfsadmin -report   //查看node节点

                            Decommission Status : Normal   //正常状体

                            ./bin/hdfs dfsadmin -refreshNodes  //刷新node节点

                            Decommission Status : Decommissioned   //删除成功





                NFS 网关

                    准备 两台 1.65(nfsgw) 1.66(client) 服务器
                    NFS
                        特性:
                            HDFS 超级用户是与namenode 进行本身具有相同表示的用户,
                            超级可以执行任何操作,因为权限检查永远不会认为超级用户失败
                        注意事项:
                            安全NFS  Kerberos keytab 中的用户是代理用户
                            非安全NFS 运行网关进程的用户是代理用户

                        下面用序号表示 操作的顺序 代理用户和 配置namenode 中序号相同 表示同时进行
                    配置代理用户
                        在nfsgw上
                            1.添加用户 (同时在namenode上进行)
                                groupadd -g 800 nfsuser
                                seradd -u 800 -g 800 -r -d /var/hadoop nfsuser
                            1.查找 rpcbin/ nfs-utlis
                                rpm -qa |grep -i rpcbind
                                rpm -qa |grep -i nfs-utils
                                有了就卸载,卸载后重启
                            3.拷贝hadoop文件到 nfsgw (在namenode上操作,请参考)

                                需要等待namenode修改完core-site.xml成后 进行

                                rsync -avXSH /usr/local/hadoop/ root@192.168.1.65:/usr/local/ ###这个命令会将hadoop下的所有文件 拷贝到 /usr/local/ 下
                                rsync -avXSH /usr/local/hadoop root@192.168.1.65:/usr/local/   ###这个命令会将hadoop文件夹 拷贝到 /usr/local/ 下

                            4.修改  hdfs-site
                                    (只在nfsgw)
                                vim ./etc/hadoop/hdfs-site.xml
                                添加 如下
                                <property>
                                    <name>nfs.dump.dir</name>
                                    <value>/var/nfstmp</value>
                                </property>
                                <property>
                                    <name>nfs.exports.allowed.hosts</name>
                                    <value>* rw</value>
                                </property>


                            4.创建 临时文件夹
                               设定权限
                                mkdir /var/nfstmp

                                chown 800.800 /var/nfstmp

                                setfacl -m user:nfsuser:rwx logs/
                                getfacl logs/

                            5.启动
                                portmap
                                    必须先启动
                                ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap

                                nfs3
                                    在portmap后启动
                                [root@nfsgw hadoop]# sudo -u nfsuser \
                                > ./sbin/hadoop-daemon.sh --script ./bin/hdfs start nfs3
                                停止服务时相反,先停止 nfs3 , 依赖关系


                        配置namenode
                            1.添加用户 (同时在nfsgw上进行)
                                groupadd -g 800 nfsuser
                                useradd -u 800 -g 800 -r -d /var/hadoop nfsuser

                            1.停止所有的hadoop服务
                                cd  /usr/local/hadoop/
                                ./sbin/stop-all.sh

                            2.修改 core-site.xml 文件

                                vim ./etc/hadoop/core-site.xml

                                <property>
                                    <name>hadoop.proxyuser.nfsuser.groups</name>
                                    <value>*</value>
                                 </property>

                                <property>
                                    <name>hadoop.proxyuser.nfsuser.hosts</name>
                                    <value>*</value>
                                 </property>

                            2.删除 不必要的文件
                                vim ./etc/hadoop/slaves
                                    删除 node4
                                vim ./etc/hadoop/exclude
                                    删除内容
                            2.同步文件
                                同步配置文件给node节点
                                 rsync -avXSH /usr/local/hadoop/etc/ root@node1:/usr/local/hadoop/etc/
                                 rsync -avXSH /usr/local/hadoop/etc/ root@node2:/usr/local/hadoop/etc/
                                 rsync -avXSH /usr/local/hadoop/etc/ root@node3:/usr/local/hadoop/etc/

                            2.启动hdfs服务

                                namenode

                                cd  /usr/local/hadoop/
                                 ./sbin/start-dfs.sh

                            3.同步 /usr/local/hadoop 给 nfsgw

                                 拷贝hadoop

                                #只是测试,用下面 :rsync -avXSH /usr/local/hadoop/ root@192.168.1.65:/usr/local/  ###这个命令会将hadoop下的所有文件 拷贝到 /usr/local/ 下
                                rsync -avXSH /usr/local/hadoop  root@192.168.1.65:/usr/local/   ###这个命令会将hadoop文件夹 拷贝到 /usr/local/ 下

                            配置client

                                挂载hdfs 文件系统  (需要等待,nfsgw 启动完成后才能挂载)
                                安装
                                    yum -y install nfs-utils
                                挂载
                                    mount -t nfs -o \
                                    vers=3,proto=tcp,nolock,noatime,sync,noacl 192.168.1.64:/  /mnt/  //挂载

                                由于hdfs只支持的nfs3协议,所以这里 也要使用 nfs3 协议
                                vers=3
                                使用tcp作为传输协议
                                proto=tcp
                                不支持NLM  内核?
                                nolock
                                禁用access time
                                noatime
                                禁用acl权限
                                noacl
















    补充 :
       linux 文件的三个时间信息 atime mtime ctime

            stat  [选项] 文件
                stat可以显示inode(索引节点)信息也可以查看一个文件的某些信息 部分Metadata(元数据)
            access time  最后一次访问时间,没有改动
            modify time   最后一次修改文件的时间
            change time  最后一次对文件属性改变的时间,包括权限 大小 属性

            inodb(index node) 索引节点
                作用:用于储存文件和目录基本信息

                block

                    磁盘在格式化的时候会被分为两部分 数据区和inodb区,并且在这个时候会设定iNode区域的大小
                    比如我们一般ntfs文件系统 每个块(block)大小为4k,由8个扇区(sector)组成block,每个sector存储512字节,一共4k,
                    所以具有文件系统的磁盘也被叫做块设备
                    当然block的大小的设定一般是和我们存储文件的大小有关,文件越大相应的block应该设置越大,反之,需要注意的是block越小磁盘寻道的时间就会变长,

                iNode节点的大小

                    每个文件必须要有一个iNode,所以可能会有iNode节点被用完但是磁盘还是有空间的情况,这是我们无法继续创建文件
                    每个inode节点的大小，一般是128字节或256字节。
                    查询iNode大小
                        sudo dumpe2fs -h /dev/sda2 | grep "Inode size"   ## /dev/sda2被查询的分区
                    查询硬盘分区iNode的总数和
                        df -i
                    关于如何修改iNode的大小可以参考,我先鸽了
                        https://blog.csdn.net/lemontree1945/article/details/80744009

                inodb区主要用来存放 文件的元数据
                    元数据包含如下信息
                       文件的字节数
                　　    文件拥有者的User ID
                　　    文件的Group ID
                　　    文件的读、写、执行权限
                　　    文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。
                　　    链接数，即有多少文件名指向这个inode
                　　    文件数据block的位置
DAY05
    一.zookeeper
        是什么

            开源的分布式应用程序协调服务
        能干嘛

            保证数据在集群之间的事物一致性

        zookeeper 运用场景

            集群分布式锁
            集群统一命名服务
            分布式协调服务

        角色与特性

        角色与选举
            zookeeper角色与选举
                -服务在启动是的时候是没有角色的(LOOKING)
                -角色是通过选举产生的
                -选举产生一个Leader,剩下的时Follower
            选举leader原则
                -集群中超过半数的机器投票选择Leader
                -加入集群中有Nn台服务器,那么leader必须得到n/2+1 台服务器的投票
                -Leader死亡,重新选举Leader
                -若down机超过一般,则集群挂掉
                -若无法得到足够的投票数量,就重新发起投票,若参与投票机器不足n/2+1
                集群停止工作
                -Observer不计算在投票总设备数量里面


        安装
            准备 host 安装java-1.8.0.openjdk
                /etc/hosts  每台
                192.168.1.60 node1
                192.168.1.61 node2
                192.168.1.62 node3
                192.168.1.63 namenode


                tar -xf zookeeper-3.4.13.tar.gz

                mv zookeeper-3.4.13 zookeeper

                mv zookeeper /usr/local/


                修改配置文件

                    cd /usr/local/zookeeper/conf/


                    mv zoo_sample.cfg zoo.cfg


                    chown root:root zoo.cfg
                    vim zoo.cfg
                    添加如下
                    server.1=node1:2888:3888
                    server.2=node2:2888:3888
                    server.3=node3:2888:3888
                    server.4=namenode:2888:3888:observer

                    mkdir /tmp/zookeeper
                    ssh node1 mkdir /tmp/zookeeper
                    ssh node2 mkdir /tmp/zookeeper
                    ssh node3 mkdir /tmp/zookeeper

               编辑 myid 文件 与 /usr/local/zookeeper/conf/zoo.cfg 中的 service 对应

                    ssh node4 'echo 4 > /tmp/zookeeper/myid'
                    ssh node3 'echo 3 > /tmp/zookeeper/myid'
                    ssh node2 'echo 2 > /tmp/zookeeper/myid'
                    ssh node1 'echo 1 > /tmp/zookeeper/myid'
                每一台服务器都要手动启动,全部启动后才能查看状态
                /usr/local/zookeeper/bin/zkServer.sh start   //启动
                /usr/local/zookeeper/bin/zkServer.sh status  //查看状态

               老师给的 脚本  在namenode下运行, 使用zookeeper 提供的api借口查询
                    ./zkstats node{1..3}

                    #!/bin/bash
                    function getzkstat(){
                        exec 2>/dev/null
                        exec 8<>/dev/tcp/$1/2181
                        echo stat >&8
                        Msg=$(cat <&8 |grep -P "^Mode:")
                        echo -e "$1\t${Msg:-Mode: \x1b[31mNULL\x1b[0m}"
                        exec 8<&-
                    }

                    if (( $# == 0 ));then
                        echo "${0##*/} zk1 zk2 zk3 ... ..."
                    else
                        for i in $@;do
                            getzkstat ${i}
                        done
                    fi

    二.kafka
        是什么

            分布式消息系统
            kafka是使用Scala编写
            一种消息中间件
        为什么使用

            解耦 冗余 提高扩展性 缓冲
            保证顺序 灵活 削峰填谷
            异步通信
        kafka 角色

            角色/集群结构
                producer 生产者,负责发布消息
                consumer 消费者,负责读取处理消息
                topic   消息的类别
                parition 每个topic集群包含一个/多个partition
                broker kafka集群包含一个或多个服务器

            kafka 通过 zookeeper 选举Leader


        安装

            安装在node1-3上

                 tar -xf kafka_2.12-2.1.0.tgz
                 mv kafka_2.12-2.1.0 /usr/local/kafka
                 修改配置文件 /usr/local/kafka/config/server.properties
                     cd /usr/local/kafka/config
                     vim server.properties
                         broker.id=22
                         zookeeper.connect=node1:2181,node2:2181,node3:2181




                 拷贝 kafka 到其他主机，并修改 broker.id ,不能重复
                   for i in 63 64; do rsync -aSH --delete /usr/local/kafka 192.168.1.$i:/usr/local/; done



                    vim /usr/local/kafka/config/server.properties
                        broker.id=23

                    vim /usr/local/kafka/config/server.properties
                        broker.id=24
        启动

            启动 kafka 集群（node1，node2，node3启动）
               /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
               jps查看

            验证配置，创建一个 topic
                /usr/local/kafka/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --zookeeper localhost:2181 --topic aa

            模拟生产者，发布消息
                /usr/local/kafka/bin/kafka-console-producer.sh \
                --broker-list localhost:9092 --topic aa
                123
            模拟消费者，接收消息
                /usr/local/kafka/bin/kafka-console-consumer.sh \
                --bootstrap-server node1:9092 --topic aa
                123


    三.hadoop 高可用

        原因

        -由于namenode是hdfs的核心配置,而]HDFS又是hadoop核心组件,namenode在hadoop集群中至关重要
        -namenode宕机,将导致集群不可用,若namenode数据丢失将导致整个集群的数据丢失,
            而namenode的数据更新有比较频繁,实现namenode的高可用是必须的

        解决方案
                HDFS with nfs
                HDFS with qjm

            方案对比
                HA方案对比
                    -都可以实现热备份
                    -都是一个active nn 和一个 standby nn
                    -都是用zookeeper和ZKFC来实现自动失效恢复
                    -失效切换都是用Fencin配置的方法来active nn
                    -nfs 数据共享变更方案把数据存储在共享存储里,我们还需要考虑nfs的高可用
                    -qjm 不需要共享存储,但是需要让每一个dn都知道两个nn的位置,
                     并把块信息和心跳包发送个active 和standby两个nn

                使用原因 (qjm)
                    -解决了namenode 单点故障
                    -hadoop 给出了HDFS的高可用ha方案:HDFS通常由两个namenode组成,
                     一个active nn 和一个 standby nn. active namenode对外提供服务,
                     比如处理来自客户端的RPC请求,而standby nn 则不能对外提供服务,仅仅同步active nn的状态,
                     以便在它失败时进行切换

                nn 高可用架构
                    -为了让standby node 和active node 同步,这两个node 都与一组jns的互相独立的进程保持通信(journal nodes)
                     当active node更新namespace,它将记录修改日志发送给jns的多数排.standby node将会从 jns 中读取edits
                     ,并持续关注它们对于日志的变更
                    -standby node 将日志变更应用在自己的namespace中,当failover[故障切换] 发生standby将会在提升自己为active之前,
                     确保能够从jns中读取所有的edits,即在 failover 故障切换 发生前standby持有 namespace与active保持完全同步
                    -由于namenode更新频繁,为了确保主备数据一致性,为了支持快速 failover ,standby node 持有集群中blocks的最新位置是
                     非常必要的. 为了这一目的datanode需要同时配置两个namenode地址,同时和它建立心跳链接,并把blocks位置发送给nn

                    -需要注意的是任何时刻只能有一个active namenode,负责会导致集群状态混乱,两个nn将会有两种不同的数据状态,将导致数据丢失或者状态异常,这种情况称为
                     split-brain (脑列,三节点通信阻塞,集群中不同的datanode,看到不同的 active name node)
                    -对于jns来说 任何时候只能有一个nn 作为writer ; 在fillover 故障切换期间,原来的standby node 将会接管active的所有职能,并负责向jns写入日志记录,
                     这种机制阻止了其他的namenode处于active状态的问题


        配置

            主要修改
                  如下文件并将修改好的文件传递给node1-3 nn和 nnbackup
                  vim core-site.xml
                  vim hdfs-site.xml
                  vim yarn-site.xml

            同步配置文件
                for i in {node1,node2,node3,nn,nnbackup}; do  rsync -avXSH /usr/local/hadoop/etc/ root@192.168.1.$i:/usr/local/hadoop/etc/; done
            初始化集群
                删除 使用过的存储初始化目录
                    rm -rf /var/hadoop/*
                    ssh node1 rm -rf /var/hadoop/*
                    ssh node2 rm -rf /var/hadoop/*
                    ssh node3 rm -rf /var/hadoop/*

                初始化 nn1 zk集群
                    /usr/local/hadoop/bin/hdfs zkfc -formatZK
                node 1-3 启动journalnode服务
                    /usr/local/hadoop/sbin/hadoop-daemon.sh start journalnodejps
                格式化 nn1

                    /usr/local/hadoop//bin/hdfs namenode -format
                nn2 数据同步到本地 /var/hadoop/dfs
                    rsync -aSH namenode:/var/hadoop/dfs/ /var/hadoop/

                初始化 nn1 jns
                    /usr/local/hadoop/bin/hdfs namenode -initalizeSharedEdits

                node 1-3 停止journalnode 服务
                    /usr/local/hadoop//sbin/hadoop-daemon.sh stop journalnode
            启动集群
                nn1
                    /usr/local/hadoop/sbin/start-dfs.sh
                    /usr/local/hadoop/sbin/start-yarn.sh
                nnbackup  启动热备
                     /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager
            检查集群状态
                resource manager 状态
                /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm2
                nn状态
                /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1

            查看节点
                /usr/local/hadoop/bin/hdfs dfsadmin -report
                /usr/local/hadoop/bin/yarn node -list
            访问集群文件
                 /usr/local/hadoop/bin/hadoop fs -mkdir /input/
                 /usr/local/hadoop/bin/hadoop fs -ls hdfs://nsd1903/
                 ### 集群中定义的 /usr/local/hadoop/etc/hadoop/core-site.xml

            主从切换
                /usr/local/hadoop/sbin/hadoop-daemon.sh stop namenode


               test
