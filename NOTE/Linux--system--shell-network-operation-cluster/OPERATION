运维 (7天)
        主机ip 以及ip地址
        clients eth0 192.168.4.10/24
        proxy   eth0 192.168.4.5/24
                eth1 192.168.2.5/24
        web1    eth1 192.168.2.100/24
        web2    eth2 192.168.2.200/24
    web服务器对比
        unix 与 linux 平台下
        -apache  Nginx  Tengine Lighttp
        -Tomcat   IBM websphere Jboss
        windows
        -IIS Internet information server
        Nginx(相比于Apache,开源) 更快 更小  效率更高 并发量更高
        Tengine 为Nginx的改编
    LNMP(PHP) NGNIX+PHP  只能这么配合
    TOMCATE+JAVA


    Nginx 介绍
        轻量级的http服务器
        高性能http和反向代理服务器同时 也是 imap /pops smtp 代理服务器
        现代化软件都是模块化设计

    如何查看 网页使用的服务器
        F12 进入 network 选项 ,选择其中一项, 查看 response head  找关键词    server
    安全策略都是针对用户和用户组的
        谁启动服务,该服务就具有谁的权限.
    安装 NGINX
        root ------>httpd(nginx)--------客户访问
        useradd nginx
        root执行命令---->以nginx普通用户启动服务 systemctl restart
        ##########################################################
        由于软件可能存在 漏洞 ,在yum 安装程序的时候 会自动创建一个用户名称和软件相同的用户,启动服务的时候以该用户启动.
        若该程序被攻击,那么攻击者也只能获取该用户的权限 与 / 无关. 而在使用源码安装的的时候 这些需要 手动 操作.
        ##########################################################
        1)安装依赖包
            若不知道依赖包 则 查看 官方文档 或者直接安装 包什么错误 装什么包
        2)./configure 安装
            [root@proxy ~]# yum -y install gcc pcre-devel openssl-devel        //安装依赖包
            [root@proxy ~]# useradd -s /sbin/nologin nginx
            [root@proxy ~]# tar  -xf   nginx-1.10.3.tar.gz
            [root@proxy ~]# cd  nginx-1.10.3
            [root@proxy nginx-1.10.3]# ./configure   \
            > --prefix=/usr/local/nginx   \                //指定安装路径
            > --user=nginx   \                            //指定用户
            > --group=nginx  \                            //指定组
            > --with-http_ssl_module                        //开启SSL加密功能 ###with XXX 安装附加模块 without XXX不添加 附加模块
            [root@proxy nginx-1.10.3]# make && make install    //编译并安装

        3)启动服务
            [root@proxy ~]# /usr/local/nginx/sbin/nginx                    //启动服务
            [root@proxy ~]# /usr/local/nginx/sbin/nginx -s stop            //关闭服务
            [root@proxy ~]# /usr/local/nginx/sbin/nginx -s reload        //重新加载配置文件 ### (不关闭服务)使配置文件立刻生效
            [root@proxy ~]# /usr/local/nginx/sbin/nginx -V                //查看软件信息
            [root@proxy ~]# ln -s /usr/local/nginx/sbin/nginx /sbin/        //方便后期使用
            当创建完成 软连接后 就可以使用
                nginx 来代替 /usr/local/nginx/sbin/nginx
                nginx -s stop


        查看服务端口
            netstat命令可以查看系统中启动的端口信息，该命令常用选项如下：
                -a显示所有端口的信息
                -n以数字格式显示端口号
                -t显示TCP连接的端口
                -u显示UDP连接的端口
                -l显示服务正在监听的端口信息，如httpd启动后，会一直监听80端口
                -p显示监听端口的服务名称是什么（也就是程序名称）
                netstat -untlp
                        -anptu
                netstat -anptul | grep 80 查找80 端口谁在使用


        版本升级/功能安装
            功能安装没有办法动态安装只能重新安装(升级安装)
            1.版本老旧,需要升级
            2.添加模块

            生成的objs 是从src(源码)中提取需要安装的文件,且没有编译,使用make(而不是make install)编译生成安装文件.
            make install 是重新安装将会覆盖/usr/local/nginx/html or conf or logs or /sbin/nginx ,也就是这些文件夹下所有的文件将会删除.

        1）编译新版本nginx软件
            [root@proxy ~]# tar  -zxvf   nginx-1.12.2.tar.gz
            [root@proxy ~]# cd nginx-1.12.2
            ############请在nginx安装包目录下执行否则会出错####################################################
            [root@proxy nginx-1.12.8]# cat auto/options | grep YES  ##查看默认安装和可以安装的nginx模块
            #######################################################################################
            [root@proxy nginx-1.12.2]# ./configure   \
            > --prefix=/usr/local/nginx   \
            > --user=nginx   \
            > --group=nginx  \
            > --with-http_ssl_module
            ####################### 注意这里是make 且 不适用make install(重新安装)
            [root@proxy nginx-1.12.2]# make

        2)备份老的nginx主程序，并使用编译好的新版本nginx替换老版本
            [root@proxy nginx-1.12.2]# mv /usr/local/nginx/sbin/nginx  \
            >/usr/local/nginx/sbin/nginxold
            [root@proxy nginx-1.12.2]# cp objs/nginx  /usr/local/nginx/sbin/         //拷贝新版本

            ###########################################
            [root@proxy nginx-1.12.2]# make upgrade                            //升级
            ############################################333
            #或者使用killall nginx杀死进程后再启动nginx。
            /usr/local/nginx/sbin/nginx -t
            nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok
            nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful
            kill -USR2 `cat /usr/local/nginx/logs/nginx.pid`
            sleep 1
            test -f /usr/local/nginx/logs/nginx.pid.oldbin
            kill -QUIT `cat /usr/local/nginx/logs/nginx.pid.oldbin`
            [root@proxy ~]# /usr/local/nginx/sbin/nginx –v                //查看版本

    访问测试
        1)分别使用浏览器和命令行工具curl测试服务器页面
            [root@client ~]# firefox http://192.168.4.5
            [root@client ~]# curl http://192.168.4.5
           #####################################
            curl -u tom:123 192.168.4.6   ### -u 添加用户和密码


    LNMP的配置
    nginx 和 httpd 的配置
        httpd 配置
            <virtualhost *:80>
            documentroot /var/www/html
            servername www.example.com
            </virtualhost>
        nginx 配置
          vim /usr/local/nginx/conf/nginx.conf
            server {
                ###监听端口
                listen 80;
                ###域名
                server_name www.dc.com;
                ####网页根路径
                root html;###默认写相对路径,也可以是用绝对路径
                ######root "/usr/local/nginx/html";
                ####默认首页,第二个是当第一个网页失效的时候备份
                index index.html index.htm;
                    }
          配置格式
            server {                     #### server 代表的是     网站

                location / {             #### location 代表的是  网站下的某个网页
                }
                location / {
                }
            }

    实验目的: 给网页添加用户访问认证,输入正确的账户密码才能访问
        1)修改配置文件
            vim /usr/local/nginx/conf/nginx.con
            server {
            listen       80;
            server_name  localhost;

            #charset koi8-r;

            #access_log  logs/host.access.log  main;
            需要手动添加的配置
            ###############################################
            auth_basic "Input Password:";                 ###提示信息
            auth_basic_user_file "/usr/local/nginx/pass";ZZtom ng ###保存用户名密码的文件
            ###############################################
            location / {
                root   html;
                index  index.html index.htm;
            }
        2)生成密码文件
            ####################
            ###不要使用 vim 创建##
            ####################
            安装httpd-Tools,创建 密码文件
            yum -y install httpd-tools
            #############################################################
            -c 新建文件,当添加第二个用户时应当 不写 -c
            [root@proxy lnmp_soft]# htpasswd -c /usr/local/nginx/pass tom
            New password:
            Re-type new password:
            Adding password for user tom
            [root@proxy lnmp_soft]# htpasswd /usr/local/nginx/pass dc
            New password:
            Re-type new password:
            Adding password for user dc

        3)重启服务
            软连接
            nginx -s reload
        4)排错

            cd /usr/local/nginx/logs

            tailf error.log
            进入文件后 一路回车 让页面 什么都不显示 用浏览器访问 , 之后出现的错误错误信息 可作为排错的依据

    虚拟主机服务
        基于域名(最常用),基于IP,基于端口

        1)修改配置文件
                 每一个server 就是一个 网站 并且放在 html中
            [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                server {
                        listen       80;                                      //端口
                        server_name  www.a.com;                                //域名
                        auth_basic "Input Password:";                        //认证提示符
                        auth_basic_user_file "/usr/local/nginx/pass";        //认证密码文件
                location / {
                        root   html;                                    //指定网站根路径
                        index  index.html index.htm;
                               }

                }
                … …
                    server {
                        listen  80;                                        //端口
                        server_name  www.b.com;                                //域名
                    location / {
                        root   www;                                 //指定网站根路径
                        index  index.html index.htm;
                        }
                }

            批量修改配置文件参考 ADMIN 349 行
            vim 末行模式 中 :88,90s/#//
        2) 重启服务
            /usr/local/nginx/sbin/nginx -s reload(重新加载nginx配置文件)


    https加密服务
         对称加密
            AES DES [安全]
            单机加密(安全)
            网络加密(不安全)
         非对称加密
            RSA DSA 私钥 公钥
         信息摘要
            MD5 SHA256 SHA


         1)生成私钥与公钥
            [root@proxy ~]# cd /usr/local/nginx/conf
            [root@proxy ~]# openssl genrsa > cert.key                            //生成私钥
                                              格式
            [root@proxy ~]# openssl req -new -x509 -key cert.key > cert.pem      //生成证书

         2)修改配置文件
            [root@proxy ~]# vim  /usr/local/nginx/conf/nginx.conf
            … …
            server {
                    listen       443 ssl;
                    server_name            www.c.com;
                    ssl_certificate      cert.pem;         #这里是证书文件
                    ssl_certificate_key  cert.key;         #这里是私钥文件
                    ssl_session_cache    shared:SSL:1m;
                    ssl_session_timeout  5m;
                    ssl_ciphers  HIGH:!aNULL:!MD5;
                    ssl_prefer_server_ciphers  on;
                    location / {
                        root   html;
                        index  index.html index.htm;
                    }
                }
         3)重启服务
            [root@proxy ~]# /usr/local/nginx/sbin/nginx -s reload
            #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
            #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)

         4)验证
            [root@client ~]# vim /etc/hosts  ####添加hosts 由于没有 dns服务器.
            192.168.4.5    www.c.com  www.a.com   www.b.com
            firefox https://www.c.com
            curl  https://www.c.com
            需要输入密码
            curl  -u tom:123 https://www.c.com
    动态页面和静态页面的实现
         区别
            动态(Java,php,py,sh...)代码在服务器执行
            静态(jpg,MP4...)
            #########添加php-fpm后用户访问网站流程##############################
             www.example.com        |
                   |
                Nginx        |
                   |路由到www.example.com/index.php        |
                   |加载nginx的fast-cgi模块        |
                   |fast-cgi监听127.0.0.1:9000地址        |
                   |www.example.com/index.php请求到达127.0.0.1:9000
                   |
                   |php-fpm 监听127.0.0.1:9000
                   |
                   |php-fpm 接收到请求，启用worker进.程处理请求        |
                   |php-fpm 处理完请求，返回给nginx        |
                   |nginx将结果通过http返回给浏览器
            ############  nginx 是不可以 处理 web页面的 只能交给php处理,而php-fpm则加快了php处理的速度########
            cgi是一个传输协议
            php-fpm 提供fastcgi的进程管理功能,也就是说实现了fastercgi.
            注意，FastCGI的内存消耗问题，一个PHP-FPM解释器将消耗约25M的内存。
         1) 安装 nginx mariadb(客户端软件)  mariadb-server(服务器端软件) mariadb-devel(mariadb依赖软件) php php-ftm(PHP服务) php-mysql(链接mysql数据库模块)
            [root@proxy ~]# yum -y install   mariadb   mariadb-server   mariadb-devel
            [root@proxy ~]# yum -y  install  php   php-mysql
            [root@proxy ~]# yum -y  install php-fpm


         2)启动服务
            这里需要注意的是，如果服务器上已经启动了其他监听80端口的服务软件（如httpd），则需要先关闭该服务，否则会出现冲突。

            1.启动Nginx服务
            [root@proxy ~]# systemctl stop httpd                //如果该服务存在则关闭该服务
            [root@proxy ~]# /usr/local/nginx/sbin/nginx             //启动Nginx服务
            [root@proxy ~]# netstat -utnlp | grep :80
            tcp    0    0 0.0.0.0:80        0.0.0.0:*        LISTEN        32428/nginx
            2.启动MySQL服务
            [root@proxy ~]# systemctl start  mariadb           //启动服务器
            [root@proxy ~]# systemctl status mariadb           //查看服务状态
            [root@proxy ~]# systemctl enable mariadb           //设置开机启动
            3.启动PHP-FPM服务
            [root@proxy ~]# systemctl start php-fpm           //启动服务
            [root@proxy ~]# systemctl status php-fpm          //查看服务状态
            [root@proxy ~]# systemctl enable php-fpm          //设置开机启动


            若网页会静态,nginx 直接返回文件
            若网页会动态,nginx 转发给9000(php)端口

            location 匹配用户的地址栏,从域名/ip往后 ; 且支持正则模糊匹配 添加 '~'; 不添加~ 表示模糊匹配
              location 一定要写在 server 中来
              server {
                location / {
                deny 1.1.1.1;
                allow all;
                }

                location /abc {
                allow all;
                }
                location /dachui {
                allow all;
                    }
                }

                #######匹配 动态网页 (以php 结尾的文件, '\'代表 转义 ,   ,'~' 表示使用正则 , '.' 在linux 中代表 匹配任意字符)
                修改nginx.conf 动静分离
                location ~ \.php$ {
                  root           html;
                ################# 将动态文件转发给9000 端口 交个 php-fpm 处理
                  fastcgi_pass   127.0.0.1:9000;
                  fastcgi_index  index.php;
                 #  fastcgi_param  SCRIPT_FILENAME  /s    cripts$fastcgi_script_name;
                 ########## 导入 nginx 变量
                  include        fastcgi.conf;
                 }


         3)添加 网页测试访问
                cd /root/lnmp_soft/php_scripts/
                firefox 192.168.4.5/mysql.php
         3)排错
            当重启nginx服务没有错误时,且页面加载失败那么,可能是 php 错误
            Nginx的默认访问日志文件为/usr/local/nginx/logs/access.log
            Nginx的默认错误日志文件为/usr/local/nginx/logs/error.log
            PHP默认错误日志文件为/var/log/php-fpm/www-error.log
##################################################################
    地址重写
             NGINX服务器地址重写
             用到的参数 : rewrite
             rewrite regex replacement flag
             用到的参数 : rewrite 旧地址(支持正则表达式) 新地址 参数
             #######################
             地址重写格式【总结】
             #######################
             rewrite 旧地址 新地址 [选项];
             last 不再读其他rewrite
             break 不再读其他语句，结束请求
             redirect 临时重定向
             permament 永久重定向
         1.访问a跳转到b
            1)修改nginx配置参数

                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                server {
                        listen       80;
                        server_name  localhost;
                charset utsf-8; ####支持中文显示
                rewrite /a.html  /b.html redirect; ####实现地址转换 ### redirect 访问a.html重定向到b.html（跳转地址栏）

                ###auth_basic "Input Password:"; #提示信息
                ###auth_basic_user_file "/usr/local/nginx/pass"; ###保存用户名密码的文件
                location / {
                    root   html;
                index  index.html index.htm;
                }
                }
            2) 测试访问
                重新加载配置文件
                /usr/local/nginx/sbin/nginx  -s  reload
                firefox  http://192.168.4.5/a.html 添加 redirect 后注意访问 地址栏的的变化

         2.访问192.168.4.5的请求重定向至www.tmooc.cn
            1)修改配置文件
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                server {
                        listen       80;
                        server_name  localhost;
         #####################################################
                rewrite ^/  http://www.tmooc.cn/;
         ####################################################
                location / {
                    root   html;
                index  index.html index.htm;
                # rewrite /a.html  /b.html  redirect;
                }
                }
            2)访问测试
                /usr/local/nginx/sbin/nginx  -s  reload
                firefox  http://192.168.4.5/a.html (真机测试 虚拟机 没有 网络)

         3.访问192.168.4.5/下面子页面，重定向至www.tmooc.cn/下相同的页面
            1)修改配置文件
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                server {
                        listen       80;
                        server_name  localhost;
                ############在正则中()表示复制 \1 表示粘贴第一个参数,
                ############但是 在 nginx中粘贴第一个复制参数  $1 表示;;; 注意这个不同
                rewrite ^/(.*)$  http://www.tmooc.cn/$1;
                location / {
                    root   html;
                index  index.html index.htm;
                }
                }
            2) 重新加载配置文件 ,访问测试
               /usr/local/nginx/sbin/nginx  -s  reload
               firefox 192.168.4.5/free

         4.修改配置文件(实现curl和火狐访问相同链接返回的页面不同)
            不同终端访问相同的网站 显示不同网页(移动端和桌面端网页的实现)
            1)修改配置文件
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                server {
                        listen       80;
                        server_name  localhost;
                location / {
                    root   html;
                index  index.html index.htm;
                }
                #这里，~符号代表模糊正则匹配，*符号代表不区分大小写,只要包含firefox 字符就执行
                if ($http_user_agent ~* firefox) {            //识别客户端firefox浏览器
                rewrite ^(.*)$  /firefox/$1;
                }
                }
            2) 添加网页
                [root@proxy ~]# echo "I am Normal page" > /usr/local/nginx/html/test.html
                [root@proxy ~]# mkdir  -p  /usr/local/nginx/html/firefox/
                [root@proxy ~]# echo "firefox page" > /usr/local/nginx/html/firefox/test.html
            3)重启服务 访问测试
                [root@proxy html]# nginx -s reload
                firefox 192.168.4.5/test.html
                Google-chrome 192.168.4.5/test.html


    NGINX反向代理服务器
         正向代理客户端 : 服务器不知道用户(翻墙),代理用户访问服务器
         反向代理服务器 : 用户不知道自己访问的是那一台服务器
         功能: 调度(负载均衡).健康检查(/)
             使用Nginx实现Web反向代理功能，实现如下功能：
                后端Web服务器两台，可以使用httpd实现
                Nginx采用轮询的方式调用后端Web服务器
                两台Web服务器的权重要求设置为不同的值
                最大失败次数为1，失败超时时间为30秒

         算法: 轮询算法(roundrobin,RR)
                                 |------->web1
                clients---->proxy-
                                 |------->web2
              IP_hash
         1.部署实施后端Web服务器
             1）部署后端Web1服务器
                [root@web1 ~]# yum  -y  install  httpd
                [root@web1 ~]# echo "192.168.2.100" > /var/www/html/index.html
                [root@web1 ~]# systemctl restart httpd
                [root@web1 ~]# firewall-cmd --set-default-zone=trusted
                [root@web1 ~]# setenforce 0
             2)部署后端Web2服务器
                [root@web2 ~]# yum  -y  install  httpd
                [root@web2 ~]# echo "192.168.2.200" > /var/www/html/index.html
                [root@web2 ~]# systemctl restart httpd
                [root@web2 ~]# firewall-cmd --set-default-zone=trusted
                [root@web2 ~]# setenforce 0
         2.配置Nginx服务器，添加服务器池，实现反向代理功能
             1）修改/usr/local/nginx/conf/nginx.conf配置文件
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                http {
                .. ..
                #使用upstream定义后端服务器集群，集群名称任意(如webserver)
                #使用server定义集群中的具体服务器和端口
                upstream webserver {
                                server 192.168.2.100:80;
                                server 192.168.2.200:80;
                        }
                .. ..
                server {
                        listen        80;
                        server_name  localhost;
                            location / {
                #通过proxy_pass将用户的请求转发给webserver集群
                            proxy_pass http://webserver;
                        }
                }
             2)重启nginx服务
                [root@proxy ~]# /usr/local/nginx/sbin/nginx -s reload
                #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
                #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)
             3）客户端使用浏览器访问代理服务器测试轮询效果
                [root@client ~]# curl http://192.168.4.5            //使用该命令多次访问查看效果
                [root@client ~]# curl http://192.168.4.5            //使用该命令多次访问查看效果

         3.配置upstream服务器集群池属性
             1）设置失败次数，超时时间，权重
                weight可以设置后台服务器的权重，max_fails可以设置后台服务器的失败次数，fail_timeout可以设置后台服务器的失败超时时间。
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                http {
                .. ..
                upstream webserver {
                           ####这里端口默认可以不写(80)         失败多少次后认为失败    超时(检测服务失败后重新开始检测的时间)
                                server 192.168.2.100 weight=1 max_fails=1 fail_timeout=30;
                                server 192.168.2.200 weight=2 max_fails=2 fail_timeout=30;
                                ####彻底关闭2.101的集群服务
                                server 192.168.2.101 down;
                                ip_hash; ###客户端访问网页后,之后的访问依然又第一次提供服务的server提供,除非该server挂了
                        }
                #weight设置服务器权重值，默认值为1
                #max_fails设置最大失败次数
                #fail_timeout设置失败超时时间，单位为秒
                #down标记服务器已关机，不参与集群调度
                .. ..
                server {
                        listen        80;
                        server_name  localhost;
                            location / {
                            proxy_pass http://webserver;
                        }
                }

             2）重启nginx服务
                [root@proxy ~]# /usr/local/nginx/sbin/nginx -s reload
                #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
                #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)

             3）关闭一台后端服务器（如web1）
                [root@web1 ~]# systemctl stop httpd

             4）客户端使用浏览器访问代理服务器测试轮询效果
                [root@client ~]# curl http://192.168.4.5            //使用该命令多次访问查看效果
             5)重启web1,再次进行测试
                [root@client ~]# curl http://192.168.4.5            //使用该命令多次访问查看效果

    Nginx的TCP/UDP调度器
         问题

            使用Nginx实现TCP/UDP调度器功能，实现如下功能：
            后端SSH服务器两台
            Nginx编译安装时需要使用--with-stream，开启ngx_stream_core_module模块
            Nginx采用轮询的方式调用后端SSH服务器
         方案

            使用4台RHEL7虚拟机，其中一台作为Nginx代理服务器，该服务器需要配置两块网卡，
            IP地址分别为192.168.4.5和192.168.2.5，两台SSH服务器IP地址分别为192.168.2.100和192.168.2.200。
            客户端测试主机IP地址为192.168.4.10。

         步骤
         1.部署支持4层TCP/UDP代理的Nginx服务器
           1)部署nginx服务器

                编译安装必须要使用--with-stream参数开启4层代理模块。
                [root@proxy ~]# yum -y install gcc pcre-devel openssl-devel        //安装依赖包
                [root@proxy ~]# tar  -xf   nginx-1.12.2.tar.gz
                [root@proxy ~]# cd  nginx-1.12.2
                [root@proxy nginx-1.12.2]# ./configure   \
                > --with-http_ssl_module                                //开启SSL加密功能
                > --with-stream                                       //开启4层反向代理功能
                [root@proxy nginx-1.12.2]# make && make install           //编译并安装

         2.配置Nginx服务器，添加服务器池，实现TCP/UDP反向代理功能
            1)修改/usr/local/nginx/conf/nginx.conf配置文件
            ########################################################################
                注意这段配置文件应该在http 之前 ;;; 若写在 http中则会使用 http协议来传输数据
            ##########################################################################
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                stream {
                            upstream backend {
                               server 192.168.2.100:22;            //后端SSH服务器的IP和端口
                               server 192.168.2.200:22;
                }
                            server {
                                listen 12345;                    //Nginx监听的端口
                                proxy_connect_timeout 1s;         //连接的超时时间，可选配置
                                proxy_timeout 3s;
                                 proxy_pass backend;
                             }
                }
                http {
                .. ..
                }
            2)重启nginx服务
                [root@proxy ~]# /usr/local/nginx/sbin/nginx -s reload
                #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
                #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)


            3）客户端使用访问代理服务器测试轮询效果
                [root@client ~]# ssh 192.168.4.5 -p 12345            //使用该命令多次访问查看效果

    Nginx常见问题处理
         问题

            本案例要求对Nginx服务器进行适当优化，解决如下问题，以提升服务器的处理性能：
            如何自定义返回给客户端的404错误页面
            如何查看服务器状态信息
            如果客户端访问服务器提示“Too many open files”如何解决
            如何解决客户端访问头部信息过长的问题
            如何让客户端浏览器缓存数据

            日志切割(重要)

            开启gzip压缩功能，提高数据传输效率
            开启文件缓存功能
            然后客户机访问此Web服务器验证效果：
            使用ab压力测试软件测试并发量
            编写测试脚本生成长头部信息的访问请求
            客户端访问不存在的页面，测试404错误页面是否重定向

         步骤
         1.自定义报错页面
            1)优化前，客户端使用浏览器访问不存在的页面，会提示404文件未找到
                [root@client ~]# firefox http://192.168.4.5/xxxxx        //访问一个不存在的页面
            2）修改Nginx配置文件，自定义报错页面
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                        charset utf-8;                    //仅需要中文时需要改选项，可选项
                error_page   404  /404.html;    //自定义错误页面
                .. ..
                [root@proxy ~]# vim /usr/local/nginx/html/404.html        //生成错误页面
                Oops,No NO no page …
                [root@proxy ~]# nginx -s reload
                #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
                #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)

            3）优化后，客户端使用浏览器访问不存在的页面，会提示自己定义的40x.html页面
                [root@client ~]# firefox http://192.168.4.5/xxxxx        //访问一个不存在的页面

          ###############################################################################

         2.如何查看服务器状态信息（非常重要的功能）
            服务器状态信息
                1.实时并发量
                2.等待的数量
                3.总链接数量
                4.pv量和uv量 需要自己写脚本
                  page view
                  user view
            1）编译安装时使用--with-http_stub_status_module开启状态页面模块
                [root@proxy ~]# tar  -zxvf   nginx-1.12.2.tar.gz
                [root@proxy ~]# cd  nginx-1.12.2
                [root@proxy nginx-1.12.2]# ./configure   \
                > --with-http_ssl_module                        //开启SSL加密功能
                > --with-stream                                //开启TCP/UDP代理模块
                > --with-http_stub_status_module                //开启status状态页面
                [root@proxy nginx-1.12.2]# make && make install    //编译并安装

            2）启用Nginx服务并查看监听端口状态
                ss命令可以查看系统中启动的端口信息，该命令常用选项如下：
                -a显示所有端口的信息
                -n以数字格式显示端口号
                -t显示TCP连接的端口
                -u显示UDP连接的端口
                -l显示服务正在监听的端口信息，如httpd启动后，会一直监听80端口
                -p显示监听端口的服务名称是什么（也就是程序名称）
                注意：在RHEL7系统中可以使用ss命令替代netstat命令，功能一样，选项一样。

                [root@proxy ~]# /usr/local/nginx/sbin/nginx
                [root@proxy ~]# netstat  -anptu  |  grep nginx
                tcp        0        0 0.0.0.0:80        0.0.0.0:*        LISTEN        10441/nginx
                [root@proxy ~]# ss  -anptu  |  grep nginx
            3）修改Nginx配置文件，定义状态页面
                [root@proxy ~]# cat /usr/local/nginx/conf/nginx.conf
                … …
                location /status {
                                stub_status on;
                                 #allow IP地址;
                                 #deny IP地址;
                        }
                … …
                [root@proxy ~]# nginx

            4）优化后，查看状态页面信息
                [root@proxy ~]# curl  http://192.168.4.5/status
                Active connections: 1
                server accepts handled requests
                 10 10 3
                Reading: 0 Writing: 1 Waiting: 0
            #######################################################
                Active connections：当前活动的连接数量。
                Accepts：已经接受客户端的连接总数量。
                Handled：已经处理客户端的连接总数量。
                （一般与accepts一致，除非服务器限制了连接数量）。
                Requests：客户端发送的请求数量。
                Reading：当前服务器正在读取客户端请求头的数量。
                Writing：当前服务器正在写响应信息的数量。
                Waiting：当前多少客户端在等待服务器的响应

         3.优化Nginx并发量

            操作系统的资源限制
            软件限制(samba,nginx)
            这两者来管理 软件所占用的资源
            ulimit -a
            查看系统 资源限制
                [root@proxy ~]# ulimit -a
                [root@proxy ~]# ulimit -a
                    core file size          (blocks, -c) 0
                    data seg size           (kbytes, -d) unlimited
                    scheduling priority             (-e) 0
                    file size               (blocks, -f) unlimited
                    pending signals                 (-i) 5569
                    max locked memory       (kbytes, -l) 64
                    max memory size         (kbytes, -m) unlimited
                    open files                      (-n) 1024
                    pipe size            (512 bytes, -p) 8
                    POSIX message queues     (bytes, -q) 819200
                    real-time priority              (-r) 0
                    stack size              (kbytes, -s) 8192
                    cpu time               (seconds, -t) unlimited
                    max user processes              (-u) 5569
                    virtual memory          (kbytes, -v) unlimited
                    file locks                      (-x) unlimited
                #########修改硬限制#################################### (决断值)
                [root@proxy ~]# ulimit -Hn 100000
                #########修改软限制#####################################(警告值)
                [root@proxy ~]# ulimit -Sn 100000
                [root@proxy ~]# ulimit -a
                    core file size          (blocks, -c) 0
                    data seg size           (kbytes, -d) unlimited
                    scheduling priority             (-e) 0
                    file size               (blocks, -f) unlimited
                    pending signals                 (-i) 5569
                    max locked memory       (kbytes, -l) 64
                    max memory size         (kbytes, -m) unlimited
                ########################################################
                    open files                      (-n) 100000
                    pipe size            (512 bytes, -p) 8
                ########################################################
                    POSIX message queues     (bytes, -q) 819200
                    real-time priority              (-r) 0
                    stack size              (kbytes, -s) 8192
                    cpu time               (seconds, -t) unlimited
                    max user processes              (-u) 5569
                    virtual memory          (kbytes, -v) unlimited
                    file locks                      (-x) unlimited

            1）优化前使用ab高并发测试
                [root@proxy ~]# ab -n 2000 -c 2000 http://192.168.4.5/
                Benchmarking 192.168.4.5 (be patient)
                socket: Too many open files (24)                //提示打开文件数量过多
            2）修改Nginx配置文件，增加并发量
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                .. ..
                worker_processes  2;                    //与CPU核心数量一致
                events {
                worker_connections 65535;        //每个worker最大并发连接数
                }
                .. ..
                [root@proxy ~]# nginx -s reload

            3）优化Linux内核参数（最大文件数量）
                [root@proxy ~]# ulimit -a                        //查看所有属性值
                [root@proxy ~]# ulimit -Hn 100000                //设置硬限制（临时规则）
                [root@proxy ~]# ulimit -Sn 100000                //设置软限制（临时规则）
                ################## 永久限制 ###############################
                [root@proxy ~]# vim /etc/security/limits.conf
                    .. ..
                *               soft    nofile            100000
                *               hard    nofile            100000
                #该配置文件分4列，分别如下：
                #用户或组    硬限制或软限制    需要限制的项目   限制的值
            4）优化后测试服务器并发量（因为客户端没调内核参数，所以在proxy测试）
                ab命令最多支持2w并发
                [root@proxy ~]# ab -n 2000 -c 2000 http://192.168.4.5/
         4.优化Nginx数据包头缓存
            1）优化前，使用脚本测试长头部请求是否能获得响应
                [root@proxy ~]# cat lnmp_soft/buffer.sh
                    #!/bin/bash
                    URL=http://192.168.4.5/index.html?
                    for i in {1..5000}
                    do
                        URL=${URL}v$i=$i
                    done
                    curl $URL                                //经过5000次循环后，生成一个长的URL地址栏
                    [root@proxy ~]# ./buffer.sh
                    .. ..
                    <center><h1>414 Request-URI Too Large</h1></center>        //提示头部信息过大

            2）修改Nginx配置文件，增加数据包头部缓存大小
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                    .. ..
                    http {
                    client_header_buffer_size    1k;        //默认请求包头信息的缓存
                    large_client_header_buffers  4 4k;        //大请求包头部信息的缓存个数与容量
                    .. ..
                    }
                    [root@proxy ~]# nginx -s reload
                    3）优化后，使用脚本测试长头部请求是否能获得响应
                    [root@proxy ~]#cat cat buffer.sh
                    #!/bin/bash
                    URL=http://192.168.4.5/index.html?
                    for i in {1..5000}
                    do
                        URL=${URL}v$i=$i
                        done
                    curl $URL
                [root@proxy ~]# ./buffer.sh



         5.浏览器本地缓存静态数据
            1)浏览器具有缓存功能,但  缓存什么 缓存多久 则是由 服务器决定
                浏览器访问 查看缓存
                about:cache

            2）清空firefox本地缓存数据

            3）修改Nginx配置文件，定义对静态页面的缓存时间
                [root@proxy ~]# vim /usr/local/nginx/conf/nginx.conf
                server {
                        listen       80;
                        server_name  localhost;
                        location / {
                            root   html;
                            index  index.html index.htm;
                        }
                location ~* \.(jpg|jpeg|gif|png|css|js|ico|xml)$ {
                expires        30d;            //定义客户端缓存时间为30天
                }
                }
                [root@proxy ~]# cp /usr/share/backgrounds/day.jpg /usr/local/nginx/html
                [root@proxy ~]# nginx -s reload
                #请先确保nginx是启动状态，否则运行该命令会报错,报错信息如下：
                #[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)
            4）优化后，使用Firefox浏览器访问图片，再次查看缓存信息
                [root@client ~]# firefox http://192.168.4.5/day.jpg

                在firefox地址栏内输入about:cache，查看本地缓存数据，查看是否有图片以及过期时间是否正确。


         6.日志切割(重要)
            日志文件越来越大怎么办？单个文件10G? 如何切割？（非常常见的面试题）
            步骤：

            (1) 把旧的日志重命名
            (2) kill USR1 PID(nginx的进程PID号)   kill,(设计用来传递信息), 命令有许多功能 杀进程只是其中一个

            1）手动执行
                备注：/usr/local/nginx/logs/nginx.pid文件中存放的是nginx的进程PID号。
                [root@proxy ~]#  mv access.log access2.log
                [root@proxy ~]# kill -USR1 $(cat /usr/local/nginx/logs/nginx.pid)
            2）自动完成
                每周5的03点03分自动执行脚本完成日志切割工作。
                [root@proxy ~]# vim /usr/local/nginx/logbak.sh
                    #!/bin/bash
                    date=`date +%Y%m%d`
                    logpath=/usr/local/nginx/logs
                    mv $logpath/access.log $logpath/access-$date.log
                    mv $logpath/error.log $logpath/error-$date.log
                    kill -USR1 $(cat $logpath/nginx.pid)
                    [root@proxy ~]# crontab -e
                    03 03 * * 5  /usr/local/nginx/logbak.sh

         7.页面进行压缩处理
                所有主流浏览器支持gzip 压缩
                 小于 1000字节的文件 不压缩
                1）修改Nginx配置文件
                    [root@proxy ~]# cat /usr/local/nginx/conf/nginx.conf
                    http {
                    .. ..
                    gzip on;                            //开启压缩
                    gzip_min_length 1000;                //小文件不压缩
                    gzip_comp_level 4;                //压缩比率
                    gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;
                                                        //对特定文件压缩，类型参考mime.types
                    .. ..
                    }

         8.服务器内存缓存
                  加速用户读取 网页 ,默认不开启.
                1）如果需要处理大量静态文件，可以将文件缓存在(服务器)内存，下次访问会更快。
                http {
                open_file_cache          max=2000  inactive=20s;
                        open_file_cache_valid    60s;
                        open_file_cache_min_uses 5;
                        open_file_cache_errors   off;
                //设置服务器最大缓存2000个文件句柄，关闭20秒内无请求的文件句柄
                //文件句柄的有效时间是60秒，60秒后过期
                //只有访问次数超过5次会被缓存
                }

    Session与Cookie 、 部署memcached 、 Session共享
            通过Nginx调度器负载后端两台Web服务器，实现以下目标：
            部署Nginx为前台调度服务器
            调度算法设置为轮询
            后端为两台LNMP服务器
            部署测试页面，查看PHP本地的Session信息

         1.PHP的本地Session信息
            注册                           写数据
           clients ---------------------> server
            登录                          vim uuid.txt(session)
                                          tom,logined:true/f
            cookie <----------------------
           session: 储存在服务器端,保护用户名登录信息
           cookies: 储存在本地用户,由服务器下发给客户端,保存在客户端的一个文件中
           session和cookie是为了解决 用户登录状态
         搭建集群后 用户的登录会出现问题
            1)部署后端LNMP服务器相关软件(这里跳过)
            2)构建memcached服务
               安装memcached软件，并启动服务
               使用telnet测试memcached服务
               对memcached进行增、删、改、查等操

                验证时需要客户端主机安装telnet，远程memcached来验证服务器的功能：
                add name 0 180 10	//变量不存在则添加
                set name 0 180 10	//添加或替换变量
                replace name 0 180 10	//替换
                get name	//读取变量
                append name 0 180 10	//向变量中追加数据
                delete name	//删除变量
                flush_all	//清空所有
                提示：0表示不压缩，180为数据缓存时间，10为需要存储的数据字节数量。

            构建memcached服务

                (1）使用yum安装软件包memcached
                    [root@proxy ~]# yum -y  install   memcached
                    [root@proxy ~]# rpm -qa memcached
                    memcached-1.4.15-10.el7_3.1.x86_64
                (2) memcached配置文件（查看即可，不需要修改）
                    [root@proxy ~]# vim /usr/lib/systemd/system/memcached.service
                    ExecStart=/usr/bin/memcached -u $USER -p $PORT -m $CACHESIZE -c $MAXCONN $OPTIONS
                    [root@proxy ~]# vim /etc/sysconfig/memcached
                    PORT="11211"
                    USER="memcached"
                    MAXCONN="1024"
                    CACHESIZE="64"
                    OPTIONS=""
                (3）启动服务并查看网络连接状态验证是否开启成功：
                    netstat命令可以查看系统中启动的端口信息，该命令常用选项如下：
                    -a显示所有端口的信息
                    -n以数字格式显示端口号
                    -t显示TCP连接的端口
                    -u显示UDP连接的端口
                    -l显示服务正在监听的端口信息，如httpd启动后，会一直监听80端口
                    -p显示监听端口的服务名称是什么（也就是程序名称）
                    注意：在RHEL7系统中，使用ss命令可以替代netstat，功能与选项一样。
                    [root@proxy ~]# systemctl  start  memcached
                    [root@proxy ~]# systemctl  status  memcached
                    [root@proxy ~]# netstat  -anptu  |  grep memcached
                    tcp    0    0 0.0.0.0:11211        0.0.0.0:*        LISTEN        2839/memcached
                    tcp    0    0 :::11211            :::*                LISTEN        2839/memcached
                    udp    0    0 0.0.0.0:11211        0.0.0.0:*                    2839/memcached
                    udp    0    0 :::11211            :::*                            2839/memcached
                    [root@proxy ~]# setenforce 0
                    [root@proxy ~]# firewall-cmd --set-default-zone=trusted
               使用telnet访问memcached服务器

                (4）使用yum安装telnet
                    [root@proxy ~]# yum -y install telnet
                (5)使用telnet连接服务器测试memcached服务器功能，包括增、删、改、查等操作。
                    [root@proxy ~]# telnet  192.168.4.5  11211
                    Trying 192.168.4.5...
                    ……
                    ##提示：0表示不压缩，180为数据缓存时间，3为需要存储的数据字节数量。
                    set name 0 180 3                //定义变量，变量名称为name
                    plj                            //输入变量的值，值为plj
                    STORED
                    get name                        //获取变量的值
                    VALUE name 0 3                 //输出结果
                    plj
                    END
                    ##提示：0表示不压缩，180为数据缓存时间，3为需要存储的数据字节数量。
                    add myname 0 180 10            //新建，myname不存在则添加，存在则报错
                    set myname 0 180 10            //添加或替换变量
                    replace myname 0 180 10        //替换，如果myname不存在则报错
                    get myname                    //读取变量
                    append myname 0 180 10        //向变量中追加数据
                    delete myname                    //删除变量
                    flush_all                        //清空所有
                    quit                            //退出登录

                                                             web1 + php-pecl-memcache  链接 memcache
                client-------------proxy+memcached-----------
                                                             wbe2 + php-pecl-memcache

         LNMP(php动态网站)+memcached

                沿用已经部署的 LNMP+memcached 网站平台,通过PHP页面实现对memcached服务器的数据操作，
                实现以下目标：

            为PHP安装memcache扩展
                创建PHP页面，并编写PHP代码，实现对memcached的数据操作
            1.方案

                如果希望使用PHP来操作memcached，注意必须要为PHP安装memcache扩展（php-pecl-memcache），否则PHP无法解析连接memcached的指令。客户端测试时需要提前安装telnet远程工具。
            2. 步骤

                实现此案例需要按照如下步骤进行。
                步骤一：创建PHP页面，使用PHP语言测试memcached服务

               1）部署测试页面
                    创建PHP首页文档/usr/local/nginx/html/index.php，测试页面可以参考lnmp_soft/php_scripts/mem.php。
                    注意：192.168.2.5是memcached数据库。
                     [root@web1 ~]# vim /usr/local/nginx/html/mem.php
                    <?php
                    $memcache=new Memcache;                //创建memcache对象
                    $memcache->connect('192.168.2.5',11211) or die ('could not connect!!');
                    $memcache->set('key','test');             //定义变量
                    $get_values=$memcache->get('key');        //获取变量值
                    echo $get_values;
                    ?>
               2）客户端测试（结果会失败）
                    客户端使用浏览器访问服务器PHP首页文档，检验对memcached的操作是否成功：
                    [root@web1 ~]# firefox http://192.168.2.100/mem.php
                    注意：这里因为没有给PHP安装扩展包，默认PHP无法连接memcached数据库，需要给PHP安装扩展模块才可以连接memcached数据库。
               3）为PHP添加memcache扩展
                    [root@web1 ~]# yum -y install  php-pecl-memcache
                    [root@web1 ~]# systemctl restart php-fpm
               4）客户端再次测试（结果会成功显示数据结果）
                    [root@web1 ~]# firefox http://192.168.2.100/mem.php
                     显示test
         PHP实现session共享
            目标: 实现web1 和 web2 将session 写入 proxy  中  并 必要时在 proxy读取session
                  Nginx服务器除了承担调度器外，还需要担任memcached数据库的角色，
                  并在两台后端LNMP服务器上实现PHP的session会话共享。

            1)为PHP添加memcache扩展
                注意，因为后端两台web服务器(web1,web2)都需要连接memcached数据库，所以两台主机都需要安装PHP扩展模块(下面也web1为例)。
                [root@web1 ~]# yum -y install  php-pecl-memcache
                注意：这里因为没有给PHP安装扩展包，默认PHP无法连接memcached数据库，需要给PHP安装扩展模块才可以连接memcached数据库。
            2）客户端测试（结果会失败）
                客户端使用浏览器访问服务器PHP首页文档，检验对memcached的操作是否成功：
                [root@web1 ~]# firefox http://192.168.2.100/test.php
            3）为PHP添加memcache扩展
                [root@web1 ~]# yum -y install  php-pecl-memcache
                [root@web1 ~]# systemctl restart php-fpm
            4）客户端再次测试（结果会成功显示数据结果）
                [root@web1 ~]# firefox http://192.168.2.100/test.php

          ###############################################################
         TOMCATE+JAVA
            jdk(jre是jdk的阉割版)开放技术
                安装部署JDK基础环境
                安装部署Tomcat服务器
                创建JSP测试页面，文件名为test.jsp，显示服务器当前时间
                访问Tomcat服务器的8080端口，浏览默认首页
            常见的servlet容器


            1.安装部署Tomcat服务器
                1)使用RPM安装JDK环境
                    [root@web1 ~]# yum -y install  java-1.8.0-openjdk                //安装JDK
                    [root@web1 ~]# yum -y install java-1.8.0-openjdk-headless        //安装JDK
                    [root@web1 ~]# java -version                                    //查看JAVA版本
                2）安装Tomcat（apache-tomcat-8.0.30.tar.gz软件包，在lnmp_soft中有提供）
                    [root@web1 ~]# tar -xf  apache-tomcat-8.0.30.tar.gz
                    [root@web1 ~]# mv apache-tomcat-8.0.30  /usr/local/tomcat
                    [root@web1 ~]# ls /usr/local/tomcat
                    bin/                                            //主程序目录
                    lib/                                            //库文件目录
                    logs/                                          //日志目录
                    temp/                                         //临时目录
                    work/                                        //自动编译目录jsp代码转换servlet
                    conf/                                        //配置文件目录
                    webapps/                                        //页面目录
                3）启动服务
                    [root@web1 ~]# /usr/local/tomcat/bin/startup.sh

                4）服务器验证端口信息
                    [root@web1 ~]# netstat -nutlp |grep java        //查看java监听的端口
                    tcp        0      0 :::8080              :::*                LISTEN      2778/java
                    tcp        0      0 ::ffff:127.0.0.1:8005     :::*         LISTEN       2778/java

                    TOMCATE每次启动都需要读取足够多的随机数据 所以会造成8005 端口的服务起不来
                    提示：如果检查端口时，8005端口启动非常慢，可用使用下面的命令用urandom替换random（非必须操作）。
                    [root@web1 ~]# mv /dev/random  /dev/random.bak
                    [root@web1 ~]# ln -s /dev/urandom  /dev/random
                5）客户端浏览测试页面(proxy作为客户端)
                    [root@proxy ~]# firefox http://192.168.2.100:8080


            (2).修改Tomcat配置文件
                1）创建测试JSP页面
                    [root@web1 ~]# vim  /usr/local/tomcat/webapps/ROOT/test.jsp
                    <html>
                    <body>
                    <center>
                    Now time is: <%=new java.util.Date()%>            //显示服务器当前时间
                    </center>
                    </body>
                    </html>
                2）重启服务
                    [root@web1 ~]# /usr/local/tomcat/bin/shutdown.sh
                    [root@web1 ~]# /usr/local/tomcat/bin/startup.sh


            (3)验证测试
                1) 测试
                    [root@proxy ~]# firefox http://192.168.2.100:8080
                    [root@proxy ~]# firefox 使用Tomcat部署虚拟主机http://192.168.2.100:8080/test.jsp



            2.使用Tomcat部署虚拟主机(基于域名的虚拟主机)
                目标:
                    实现两个基于域名的虚拟主机，域名分别为：www.a.com和 www.b.com
                    使用www.a.com域名访问的页面根路径为/usr/local/tomcat/a/ROOT
                    使用www.b.com域名访问的页面根路径为/usr/local/tomcat/b/base
                    访问www.a.com/test时，页面自动跳转到/var/www/html目录下的页面
                    访问页面时支持SSL加密通讯
                    私钥、证书存储路径为/usr/local/tomcat/conf/cert
                    每个虚拟主机都拥有独立的访问日志文件
                    配置tomcat集群环境
                TOMCATE 修改 conf的 server.conf
                    中参数

                        <Host name="www.a.com"  appBase="a"
                        unpackWARs="true" autoDeploy="true">
                        </Host>

                        # cat /usr/local/tomcat/conf/server.xml
                        <Server>
                           <Service>
                             <Connector port=8080 />
                             <Connector port=8009 />
                             <Engine name="Catalina" defaultHost="localhost">
                        <Host name="www.a.com" appBase="a" unpackWARS="true" autoDeploy="true">
                        </Host>
                        <Host name="www.b.com" appBase="b" unpackWARS="true" autoDeploy="true">
                        </Host>

                (1)配置服务器虚拟主机
                    1）修改server.xml配置文件，创建虚拟主机
                        [root@web1 ~]# vim /usr/local/tomcat/conf/server.xml
                        … …
                        <Host name="www.a.com" appBase="a" unpackWARS="true" autoDeploy="true">
                        </Host>
                        <Host name="www.b.com" appBase="b" unpackWARS="true" autoDeploy="true">
                        </Host>
                    2）创建虚拟主机对应的页面根路径
                        [root@web1 ~]# mkdir -p  /usr/local/tomcat/{a,b}/ROOT
                        [root@web1 ~]# echo "AAA"   > /usr/local/tomcat/a/ROOT/index.html
                        [root@web1 ~]# echo "BBB" > /usr/local/tomcat/b/ROOT/index.html

                    3）重启Tomcat服务器
                        [root@web1 ~]# /usr/local/tomcat/bin/shutdown.sh
                        [root@web1 ~]# /usr/local/tomcat/bin/startup.sh
                    4)客户端设置host文件，并浏览测试页面进行测试(proxy充当客户端角色)
                        [root@proxy ~]# vim /etc/hosts
                        … …
                        192.168.2.100      www.a.com  www.b.com
                        [root@proxy ~]# firefox http://www.a.com:8080/        //注意访问的端口为8080
                        [root@proxy ~]# firefox http://www.b.com:8080/
                (2)修改www.b.com网站的首页目录为base

                    1）使用docBase参数可以修改默认网站首页路径
                        [root@web1 ~]# vim /usr/local/tomcat/conf/server.xml
                        … …
                        <Host name="www.a.com" appBase="a" unpackWARS="true" autoDeploy="true">
                        #########################################################3
                        curl www.a.com/test/ 显示 /var/www/html的 index.html页面
                        #######################################################
                        <Context path="/test" docBase="/var/www/html"/>
                        ################
                        </Host>
                        <Host name="www.b.com" appBase="b" unpackWARS="true" autoDeploy="true">
                        #############################
                        curl www.b.com 显示 base
                        <Context path="" docBase="base"/>
                        </Host>
                        … …
                        [root@web1 ~]# mkdir  /usr/local/tomcat/b/base
                        [root@web1 ~]# echo "BASE" > /usr/local/tomcat/b/base/index.html
                        [root@web1 ~]# /usr/local/tomcat/bin/shutdown.sh
                        [root@web1 ~]# /usr/local/tomcat/bin/startup.sh
                    2）测试查看页面是否正确(proxy充当客户端角色)
                        [root@proxy ~]# firefox http://www.b.com:8080/        //结果为base目录下的页面内容


                        TOMCATE的 https 默认做一次 就会给 全部的 虚拟主机启用加密
                        也就说 TOMCATE 不会将端口绑定到虚拟主机(端口只负责转发),

                (3)HTTPS网站加密
                    1)创建加密用的私钥和证书文件
                        [root@web1 ~]# keytool -genkeypair -alias tomcat -keyalg RSA -keystore /usr/local/tomcat/keystore                //提示输入密码为:123456
                        //-genkeypair     生成密钥对
                        //-alias tomcat     密钥别名
                        //-keyalg RSA     定义密钥算法为RSA算法
                        //-keystore         定义密钥文件存储在:/usr/local/tomcat/keystore
                    2)再次修改server.xml配置文件，创建支持加密连接的Connector
                        [root@web1 ~]# vim /usr/local/tomcat/conf/server.xml
                        … …
                        84 行左右
                        <Connector port="8443" protocol="org.apache.coyote.http11.Http11NioProtocol"
                        maxThreads="150" SSLEnabled="true" scheme="https" secure="true"
                    ######添加秘钥文件信息 和密码信息 默认没有需要手写
                        keystoreFile="/usr/local/tomcat/keystore" keystorePass="123456" clientAuth="false" sslProtocol="TLS" />
                        //备注，默认这段Connector被注释掉了，打开注释，添加密钥信息即可
                    3）重启Tomcat服务器
                        [root@web1 ~]# /usr/local/tomcat/bin/shutdown.sh
                        [root@web1 ~]# /usr/local/tomcat/bin/startup.sh
                    4）客户端设置host文件，并浏览测试页面进行测试(proxy充当客户端角色)
                        [root@proxy ~]# vim /etc/hosts
                        … …
                        192.168.2.100      www.a.com  www.b.com
                        [root@proxy ~]# firefox https://www.a.com:8443/
                        [root@proxy ~]# firefox https://www.b.com:8443/
                        [root@proxy ~]# firefox https://192.168.2.100:8443/

                (4)配置Tomcat日志

                    1)为每个虚拟主机设置不同的日志文件
                        [root@web1 ~]# vim /usr/local/tomcat/conf/server.xml
                        .. ..
                        <Host name="www.a.com" appBase="a" unpackWARS="true" autoDeploy="true">
                        <Context path="/test" docBase="/var/www/html/" />
                        #从默认localhost虚拟主机中把Valve这段复制过来，适当修改下即可
                        <Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
                                       prefix=" a_access" suffix=".txt"
                                       pattern="%h %l %u %t &quot;%r&quot; %s %b" />
                        </Host>
                        <Host name="www.b.com" appBase="b" unpackWARS="true" autoDeploy="true">
                        <Context path="" docBase="base" />
                        <Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
                                       prefix=" b_access" suffix=".txt"
                                       pattern="%h %l %u %t &quot;%r&quot; %s %b" />
                        </Host>
                        .. ..
                    2）重启Tomcat服务器
                        [root@web1 ~]# /usr/local/tomcat/bin/shutdown.sh
                        [root@web1 ~]# /usr/local/tomcat/bin/startup.sh
                    3）查看服务器日志文件
                        [root@web1 ~]# ls /usr/local/tomcat/logs/
            3.配置Tomcat集群

                    1) 在192.168.4.5主机上配置Nginx调度器（具体安装步骤参考前面的章节）
                        [root@proxy ~]# vim  /usr/local/nginx/conf/nginx.conf
                        http{
                            upstream toms {
                                server 192.168.2.100:8080;
                                server 192.168.2.200:8080;
                            }
                            server  {
                                listen 80;
                                server_name localhost;
                                location / {
                                    proxy_pass  http://toms;
                                }
                            }
                        }
                    2) 在192.168.2.100和192.168.2.200主机上配置Tomcat调度器
                        以下以Web1为例：
                        [root@web1 ~]# yum -y install  java-1.8.0-openjdk                //安装JDK
                        [root@web1 ~]# yum -y install java-1.8.0-openjdk-headless        //安装JDK
                        [root@web1 ~]# tar -xzf  apache-tomcat-8.0.30.tar.gz
                        [root@web1 ~]# mv apache-tomcat-8.0.30  /usr/local/tomcat
                    3）启动服务
                        [root@web1 ~]# /usr/local/tomcat/bin/startup.sh
                    4) 客户端验证
                        为了防止有数据缓存，可以使用真实主机的google-chrome访问代理服务器，输入Ctrl+F5刷新页面。

            4.使用Varnish加速Web
                    目标

                    通过配置Varnish缓存服务器，实现如下目标：
                    使用Varnish加速后端Web服务
                    代理服务器可以将远程的Web服务器页面缓存在本地
                    远程Web服务器对客户端用户是透明的
                    利用缓存机制提高网站的响应速度
                    使用varnishadm命令管理缓存页面
                    使用varnishstat命令查看Varnish状态
                    3.2 方案

                    通过源码编译安装Varnish缓存服务器
                    编译安装Varnish软件
                    修改配置文件，缓存代理源Web服务器，实现Web加速功能
                    使用3台RHEL7虚拟机，其中一台作为Web服务器（192.168.2.100）、一台作为Varnish代理服务器（192.168.4.5,192.168.2.5)，
                    另外一台作为测试用的Linux客户机（192.168.4.10）


                    对于Web服务器的部署，此实验中仅需要安装nginx或者httpd软件、启动服务，并生成测试首页文件即可，
                    默认httpd网站根路径为/var/www/html，首页文档名称为index.html，
                    默认nginx网站根路径为/usr/local/nginx/html，默认首页为index.html。下面的实验我们以httpd为例作为Web服务器。



                (1)构建Web服务器

                    1）使用yum安装web软件包
                        [root@web1 ~]# yum  -y  install  httpd
                    2）启用httpd服务（注意需要关闭nginx，否则端口冲突）
                        [root@web1 ~]# systemctl start httpd
                        httpd服务默认通过TCP 80端口监听客户端请求：
                        [root@web1 ~]# netstat  -anptu  |  grep httpd
                        tcp        0        0        :::80        :::*        LISTEN        2813/httpd
                    3）为Web访问建立测试文件
                        在网站根目录/var/www/html下创建一个名为index.html的首页文件：
                        [root@web1 ~]# cat /var/www/html/index.html
                        192.168.2.100
                    4）测试页面是否正常（代理服务器测试后台web）
                        [root@proxy ~]# firefox http://192.168.2.100
                (2)部署Varnish缓存服务器(192.168.4.5)

                    1）编译安装软件
                        [root@proxy ~]# yum -y install gcc readline-devel    //安装软件依赖包
                        [root@proxy ~]# yum -y install ncurses-devel         //安装软件依赖包
                        [root@proxy ~]# yum -y install pcre-devel            //安装软件依赖包
                        [root@proxy ~]# yum -y install python-docutils         //安装软件依赖包
                         [root@proxy ~]# useradd -s /sbin/nologin varnish                //创建账户
                        [root@proxy ~]# tar -xf varnish-5.2.1.tar.gz
                        [root@proxy ~]# cd varnish-5.2.1
                        [root@proxy varnish-5.2.1]# ./configure
                        [root@proxy varnish-5.2.1]# make && make install
                    2）复制启动脚本及配置文件
                        [root@proxy varnish-5.2.1]# cp  etc/example.vcl   /usr/local/etc/default.vcl
                    3）修改代理配置文件
                        [root@proxy ~]# vim  /usr/local/etc/default.vcl
                        backend default {
                             .host = "192.168.2.100";
                             .port = "80";
                         }
                    4）启动服务
                         [root@proxy ~]# varnishd  -f /usr/local/etc/default.vcl
                        //varnishd命令的其他选项说明如下：
                        //varnishd -s malloc,128M        定义varnish使用内存作为缓存，空间为128M
                        //varnishd -s file,/var/lib/varnish_storage.bin,1G 定义varnish使用文件作为缓存
                (3)客户端测试

                    1）客户端开启浏览器访问
                        [root@client ~]# curl http://192.168.4.5
                (4)其他操作

                    1）查看varnish日志
                        [root@proxy ~]# varnishlog                        //varnish日志
                        [root@proxy ~]# varnishncsa                    //访问日志
                    2）更新缓存数据，在后台web服务器更新页面内容后，用户访问代理服务器看到的还是之前的数据，说明缓存中的数据过期了需要更新（默认也会自动更新，但非实时更新）。
                        [root@proxy ~]# varnishadm
                        varnish> ban req.url ~ .*
                        //清空缓存数据，支持正则表达式
    Git

         Git基本操作
            1.配置git服务器
                目标
                安装Git软件
                创建版本库
                客户端克隆版本仓库到本地
                本地工作目录修改数据
                提交本地修改到服务器

                方案
                1）YUM安装Git软件。
                    [root@web1 ~]# yum -y install git
                    [root@web1 ~]# git --version
                2) 初始化一个空仓库。
                    [root@web1 ~]# mkdir /var/git
                    [root@web1 ~]# git init /var/git/project --bare
                    [root@web1 ~]# ls /var/git/
                    config  description  HEAD  hooks  info  objects  refs

            2. 客户端测试 192.168.2.200
                 常用指令
                    clone  将远程服务器的仓库克隆到本地
                    config 修改git配置
                    add    添加修改到暂存区
                    commit 提交修改到本地仓库
                    push   提交的远程服务器
                    checkout 切换分支
                1) clone克隆服务器仓库到本地。
                    [root@web2 ~]# yum -y install git
                    [root@web2 ~]# git clone root@192.168.2.100 :/var/git/project
                    [root@web2 ~]# cd project
                    [root@web2 ~]# ls
                2) 修改git配置。
                    [root@web2 project]# git config --global user.email "you@example.com"
                    [root@web2 project]# git config --global user.name "Your Name"
                    [root@web2 project]# cat ~/.gitconfig
                    [user]
                        email = you@example.com
                        name = Your Name
                3） 本地工作区对数据进行增删改查(必须要先进入仓库再操作数据)。
                    [root@web2 project]# echo "init date" > init.txt
                    [root@web2 project]# mkdir demo
                    [root@web2 project]# cp /etc/hosts demo
                4） 查看仓库中数据的状态。
                    [root@web2 project]# git status
                5） 将工作区的修改提交到暂存区。
                    [root@web2 project]# git add .
                6) 将暂存区修改提交到本地仓库。
                    [root@web2 project]# git commit  -m  "注释，可以为任意字符"
                    [root@web2 project]# git status
                7） 将本地仓库中的数据推送到远程服务器(web2将数据推送到web1)。
                    [root@web2 project]# git config --global push.default simple
                    [root@web2 project]# git push
                    root@192.168.2.100's password:  输入服务器root密码
                    [root@web2 project]# git status
                8) 将服务器上的数据更新到本地（web1的数据更新到web2）。
                    备注：可能其他人也在修改数据并提交服务器，就会导致自己的本地数据为旧数据，使用pull就可以将服务器上新的数据更新到本地。
                    [root@web2 project]# git pull
                9) 查看版本日志。
                    [root@web2 project]# git log
                    [root@web2 project]# git log --pretty=oneline
                    [root@web2 project]# git log --oneline
                    [root@web2 project]# git reflog
                    备注：客户端也可以使用图形程序访问服务器。
            3.HEAD指针操作
                目标
                    查看Git版本信息
                    移动指针
                    通过移动HEAD指针恢复数据
                    合并版本
                方案
                    HEAD指针是一个可以在任何分支和版本移动的指针，
                    通过移动指针我们可以将数据还原至任何版本。
                    没做一次提交操作都会导致git更新一个版本，
                    HEAD指针也跟着自动移动
                HEAD指针基本操作

                1）准备工作（多对数据仓库进行修改、提交操作，以产生多个版本）。
                    [root@web2 project]# echo "new file" > new.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "add new.txt"
                    [root@web2 project]# echo "first" >> new.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "new.txt:first line"
                    [root@web2 project]# echo "second" >> new.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "new.txt:second"
                    [root@web2 project]# echo "third" >> new.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "new.txt:third"
                    [root@web2 project]# git push
                    [root@web2 project]# echo "123" > num.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "num.txt:123"
                    [root@web2 project]# echo "456" > num.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "num.txt:456"
                    [root@web2 project]# echo "789" > num.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "num.txt:789"
                    [root@web2 project]# git push
                2) 查看Git版本信息。
                    [root@web2 project]# git reflog
                    [root@web2 project]# git log --oneline
                    04ddc0f num.txt:789
                    7bba57b num.txt:456
                    301c090 num.txt:123
                    b427164 new.txt:third
                    0584949 new.txt:second
                    ece2dfd new.txt:first line
                    e1112ac add new.txt
                    1a0d908 初始化
                3）移动HEAD指针，将数据还原到任意版本。
                    提示：当前HEAD指针为HEAD@{0}。
                    [root@web2 project]# git reset --hard 301c0
                    [root@web2 project]# git reflog
                    301c090 HEAD@{0}: reset: moving to 301c0
                    04ddc0f HEAD@{1}: commit: num.txt:789
                    7bba57b HEAD@{2}: commit: num.txt:456
                    301c090 HEAD@{3}: commit: num.txt:123
                    b427164 HEAD@{5}: commit: new.txt:third
                    0584949 HEAD@{6}: commit: new.txt:second
                    ece2dfd HEAD@{7}: commit: new.txt:first line
                    e1112ac HEAD@{8}: commit: add new.txt
                    1a0d908 HEAD@{9}: commit (initial): 初始化
                    [root@web2 project]# cat num.txt                #查看文件是否为123
                    123
                    [root@web2 project]# git reset --hard 7bba57b
                    [root@web2 project]# cat num.txt                #查看文件是否为123，456
                    123
                    456
                    [root@web2 project]# git reflog                #查看指针移动历史
                    7bba57b HEAD@{0}: reset: moving to 7bba57b
                    301c090 HEAD@{1}: reset: moving to 301c0
                    … …
                    [root@web2 project]# git reset --hard 04ddc0f    #恢复num.txt的所有数据
                4)模拟误删后的数据还原操作。
                    [root@web2 project]# git rm init.txt                    #删除文件
                    rm 'init.txt'
                    [root@web2 project]# git commit -m "delete init.txt"    #提交本地仓库
                    [root@web2 project]# git reflog                        #查看版本历史
                    0dc2b76 HEAD@{0}: commit: delete init.txt
                    7bba57b HEAD@{0}: reset: moving to 7bba57b
                    301c090 HEAD@{1}: reset: moving to 301c0
                    … …
                    [root@web2 project]# git reset --hard 04ddc0f            #恢复数据
                    [root@web2 project]# ls
                    demo  init.txt  new.txt  num.txt

            4.Git分支操作
                沿用上个练习，学习操作Git分支，具体要求如下：
                    查看分支
                    创建分支
                    切换分支
                    合并分支
                    解决分支的冲突
                Git支持按功能模块、时间、版本等标准创建分支，分支可以让开发分多条主线同时进行，每条主线互不影响

                常见的分支规范如下：
                    MASTER分支（MASTER是主分支，是代码的核心）。
                    DEVELOP分支（DEVELOP最新开发成果的分支）。
                    RELEASE分支（为发布新产品设置的分支）。
                    HOTFIX分支（为了修复软件BUG缺陷的分支）。
                    FEATURE分支（为开发新功能设置的分支）。

                (1)查看并创建分支

                1）查看当前分支。
                    [root@web2 project]# git status
                    # On branch master
                    nothing to commit, working directory clean
                    [root@web2 project]# git branch -v
                    * master 0dc2b76 delete init.txt
                2）创建分支。
                    [root@web2 project]# git branch hotfix
                    [root@web2 project]# git branch feature
                    [root@web2 project]# git branch -v
                      feature 0dc2b76 delete init.txt
                      hotfix  0dc2b76 delete init.txt
                    * master  0dc2b76 delete init.txt
                (2)切换与合并分支

                1）切换分支。
                    [root@web2 project]# git checkout hotfix ###切换分支
                    [root@web2 project]# git branch -v
                      feature 0dc2b76 delete init.txt
                    * hotfix  0dc2b76 delete init.txt
                    master  0dc2b76 delete init.txt
                2）在新的分支上可以继续进行数据操作（增、删、改、查）。
                    [root@web2 project]# echo "fix a bug" >> new.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "fix a bug"
                3）将hotfix修改的数据合并到master分支。
                    注意，合并前必须要先切换到master分支，然后再执行merge命令。
                    [root@web2 project]# git checkout master
                    [root@web2 project]# cat new.txt        #默认master分支中没有hotfix分支中的数据
                    [root@web2 project]# git merge hotfix
                    Updating 0dc2b76..5b4a755
                    Fast-forward
                     new.txt | 1 ++
                     1 file changed, 1 insertions(+)
                4）将所有本地修改提交远程服务器。
                    [root@web2 project]# git push

                (3)解决版本分支的冲突问题

                1）在不同分支中修改相同文件的相同行数据，模拟数据冲突。
                    [root@web2 project]# git checkout hotfix
                    [root@web2 project]# echo "AAA" > a.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "add a.txt by hotfix"
                    [root@web2 project]# echo "BBB" > a.txt
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "add a.txt by master"
                    自动合并 a.txt
                    冲突（添加/添加）：合并冲突于 a.txt
                    自动合并失败，修正冲突然后提交修正的结果。
                2）查看有冲突的文件内容，修改文件为最终版本的数据，解决冲突。
                    [root@web2 project]# cat a.txt                #该文件中包含有冲突的内容
                    <<<<<<< HEAD
                    BBB
                    =======
                    AAA
                    >>>>>>> hotfix
                    [root@web2 project]# vim a.txt              #修改该文件，为最终需要的数据，解决冲突
                    BBB
                    [root@web2 project]# git add .
                    [root@web2 project]# git commit -m "resolved"
                    总结：分支指针与HEAD指针的关系。
                    创建分支的本质是在当前提交上创建一个可以移动的指针
                    如何判断当前分支呢？答案是根据HEAD这个特殊指针



            Git服务器
                问题

                沿用练习三，学习Git不同的服务器形式，具体要求如下：
                创建SSH协议服务器
                创建Git协议服务器
                创建HTTP协议服务器
                方案

                Git支持很多服务器协议形式，不同协议的Git服务器，客户端就可以使用不同的形式访问服务器。
                创建的服务器协议有SSH协议、Git协议、HTTP协议。

            (1)：SSH协议服务器（支持读写操作）

                1）创建基于密码验证的SSH协议服务器（web1主机操作）。
                    [root@web1 ~]# git init --bare /var/git/base_ssh
                    Initialized empty Git repository in /var/git/base_ssh/
                2)客户端访问的方式（web2主机操作）。
                    [root@web2 ~]# git clone root@192.168.2.100:/var/git/base_ssh
                    [root@web2 ~]# rm -rf base_ssh
                3）客户端生成SSH密钥，实现免密码登陆git服务器（web2主机操作）。

                   将客户端生成的公钥 传递给 需要登录的机器, 本机保留私钥, 远程时通过 私钥解密.

                    -f 指定 id_rsa 文件位置
                    [root@web2 ~]# ssh-keygen -f /root/.ssh/id_rsa -N ''
                    [root@web2 ~]# ssh-copy-id  192.168.2.100
                    [root@web2 ~]# git clone root@192.168.2.100:/var/git
                    [root@web2 ~]# git push
                (2)Git协议服务器（只读操作的服务器）

                1）安装git-daemon软件包（web1主机操作）。
                    [root@web1 ~]# yum -y install git-daemon
                2）创建版本库（web1主机操作）。
                    [root@web1 ~]# git init --bare /var/git/base_git
                    Initialized empty Git repository in /var/git/base_git/
                3）修改配置文件，启动git服务（web1主机操作）。
                    [root@web1 ~]# vim /usr/lib/systemd/system/git@.service
                    修改前内容如下：
                    ExecStart=-/usr/libexec/git-core/git-daemon --base-path=/var/lib/git
                    --export-all --user-path=public_git --syslog --inetd –verbose
                    修改后内容如下：
                    ExecStart=-/usr/libexec/git-core/git-daemon --base-path=/var/git
                    --export-all --user-path=public_git --syslog --inetd –verbose
                    [root@web1 ~]# systemctl  start  git.socket
                4）客户端访问方式（web2主机操作）
                    [root@web2 ~]# git clone git://192.168.2.100/base_git
                (3)HTTP协议服务器（只读操作的服务器）

                    1）安装gitweb、httpd软件包（web1主机操作）。
                        [root@web1 ~]# yum -y install httpd gitweb
                    2）修改配置文件，设置仓库根目录（web1主机操作）。
                        [root@web1 ~]# vim +11 /etc/gitweb.conf  ###打开文件 后进入第 11 行
                        $projectroot = "/var/git";                        #添加一行
                    3) 创建版本仓库（web1主机操作）
                        [root@web1 ~]# git init --bare /var/git/base_http
                    4）启动httpd服务器
                        [root@web1 ~]# systemctl start httpd
                    5）客户端访问方式（web2主机操作）
                        注意：调用虚拟机中的firefox浏览器，需要在远程时使用ssh -X 服务器IP，并且确保真实主机的firefox已经关闭。
                        [root@web2 ~]# firefox http://192.168.2.100/git/
         制作nginx源码的RPM包(将源码包制作成rpm包)
            rpm就是一个压缩包
            方法
                安装 rpm-build
                1)yum -y install rpm-build
                生成 rpmbuild 目录及其文件
                2)rpmbuild -ba zone.spec
                    错误：stat /root/zone.spec 失败：没有那个文件或目录
                  ls rpmbuild/
                    BUILD  BUILDROOT  RPMS  SOURCES  SPECS  SRPMS
                想要谁的 rpm包就将 谁的源码放到/rpmbuild/source 目录下,修改配置文件(重要) 确认将谁 做成rpm包
                    cp /nginx-1.12.2.tar.gz .
                3)修改配置文件
                    vim /root/rpmbuild/SPECS/nginx.spec
                        Name:nginx                                        #源码包软件名称
                        Version:1.12.2                                    #源码包软件的版本号
                        Release:    10                                        #制作的RPM包版本号
                        Summary: Nginx is a web server software.            #RPM软件的概述
                        License:GPL                                        #软件的协议
                        URL:    www.test.com                                    #网址
                        Source0:nginx-1.12.2.tar.gz                        #源码包文件的全称
                        #BuildRequires:                                    #制作RPM时的依赖关系,并不能解决依赖只能提示
                        #Requires:                                        #安装RPM时的依赖关系
                        %description
                        nginx [engine x] is an HTTP and reverse proxy server.    #软件的详细描述
                        %post
                        useradd nginx                               #非必需操作：安装后脚本(创建账户,删根等危险操作 也可以执行,exe 同要的道理)
                        %prep
                        %setup -q                                #自动解压源码包，并cd进入目录
                        %build
                        ./configure
                        make %{?_smp_mflags}
                        %install
                        make install DESTDIR=%{buildroot}
                        %files
                        %doc
                        /usr/local/nginx/*                    #对哪些文件与目录打包
                        %changelog
                 打包 安装
                4)rpmbuild -ba /root/rpmbuild/SPECS/nginx.spec

                  yum -y install  gcc  pcre-devel openssl-devel
                5) 安装
                  rpm -ivh RPMS/x86_64/nginx-1.12.2-10.x86_64.rpm
    VPN服务器
                vpn 虚拟专用网
                    在公网上建立专用的私有网络,进行加班通信,多用于集团公司的各地子公司建立链接
                    企业中广泛用于
                    可以用于 穿墙
                    主流软件  GRE  PPTP   L2TP+IPSec,SSL
                           不加密 部分加密  全部加密
         配置GRE VPN
            linux(默认支持) window(不支持) 网络设备默认支持
            要求
                启用内核模块ip_gre
                创建一个虚拟VPN隧道(10.10.10.0/24)
                实现两台主机点到点的隧道通讯
            方法
                使用lsmod查看当前计算机已经加载的模块，使用modprobe加载Linux内核模块，
                使用modinfo可以查看内核模块的信息。
                ip 设置
                    client  eth3 201.1.2.10/24
                    proxy  eth3 201.1.2.10/24
                    proxy  eth0 192.168.4.5/24
            1.启用GRE模块（client和proxy都需要操作）

                1）查看计算机当前加载的模块
                    [root@client ~]# lsmod                            //显示模块列表
                    [root@client ~]# lsmod  | grep ip_gre            //确定是否加载了gre模块
                2)加载模块ip_gre
                    [root@client ~]# modprobe  ip_gre
                3）查看模块信息
                    [root@client ~]# modinfo ip_gre
                    filename:       /lib/modules/3.10.0-693.el7.x86_64/kernel/net/ipv4/ip_gre.ko.xz
                    alias:          netdev-gretap0
                    alias:          netdev-gre0
                    alias:          rtnl-link-gretap
                    alias:          rtnl-link-gre
                    license:        GPL
                    rhelversion:    7.4
                    srcversion:     F37A2BF90692F86E3A8BD15
                    depends:        ip_tunnel,gre
                    intree:         Y
                    vermagic:       3.10.0-693.el7.x86_64 SMP mod_unload modversions
                    signer:         CentOS Linux kernel signing key
                    sig_key:        DA:18:7D:CA:7D:BE:53:AB:05:BD:13:BD:0C:4E:21:F4:22:B6:A4:9C
                    sig_hashalgo:   sha256
                    parm:           log_ecn_error:Log packets received with corrupted ECN (bool)

            2.Client主机创建VPN隧道

                1）创建隧道
                    [root@client ~]# ip tunnel add tun0  mode gre \
                    >  remote 201.1.2.5 local 201.1.2.10
                    //ip tunnel add创建隧道（隧道名称为tun0），ip tunnel help可以查看帮助
                    //mode设置隧道使用gre模式
                    //local后面跟本机的IP地址，remote后面是与其他主机建立隧道的对方IP地址
                2）启用该隧道（类似与设置网卡up）
                    [root@client ~]# ip link show
                    [root@client ~]# ip link set tun0 up         //设置UP
                    [root@client ~]# ip link show
                3）为VPN配置隧道IP地址
                    [root@client ~]# ip addr add 10.10.10.10/24 peer 10.10.10.5/24 \
                    >  dev tun0
                    //为隧道tun0设置本地IP地址（10.10.10.10.10/24）
                    //隧道对面的主机IP的隧道IP为10.10.10.5/24
                    [root@client ~]# ip a s                      //查看IP地址
                                     ip add show     etho       //查看网卡 0  的 IP信息
                4）关闭防火墙
                    [root@client ~]# firewall-cmd --set-default-zone=trusted

            3.Proxy主机创建VPN隧道

                1）查看计算机当前加载的模块
                    [root@client ~]# lsmod                            //显示模块列表
                    [root@client ~]# lsmod  | grep ip_gre            //确定是否加载了gre模块
                2)加载模块ip_gre
                    [root@client ~]# modprobe  ip_gre
                3）创建隧道
                    [root@proxy ~]# ~]# ip tunnel add tun0  mode gre \
                    >  remote 201.1.2.10 local 201.1.2.5
                    //ip tunnel add创建隧道（隧道名称为tun0），ip tunnel help可以查看帮助
                    //mode设置隧道使用gre模式
                    //local后面跟本机的IP地址，remote后面是与其他主机建立隧道的对方IP地址
                4）启用该隧道（类似与设置网卡up）
                    [root@proxy ~]# ip link show
                    [root@proxy ~]# ip link set tun0 up         //设置UP
                    [root@proxy ~]# ip link show
                5）为VPN配置隧道IP地址
                    [root@proxy ~]# ip addr add 10.10.10.5/24 peer 10.10.10.10/24 \
                    >  dev tun0
                    //为隧道tun0设置本地IP地址（10.10.10.10.5/24）
                    //隧道对面的主机IP的隧道IP为10.10.10.10/24
                    [root@proxy ~]# ip a s                      //查看IP地址
                6）开启路由转发、关闭防火墙
                    [root@proxy ~]# echo "1" > /proc/sys/net/ipv4/ip_forward
                    [root@proxy ~]# firewall-cmd --set-default-zone=trusted
                7)测试连通性
                    [root@client ~]#  ping 10.10.10.5
                    [root@proxy ~]#   ping 10.10.10.10


         创建PPTP VPN (W和L都支持)
                要求
                    本案例要求搭建一个PPTP VPN环境，并测试该VPN网络是否能够正常通讯，要求如下:
                    使用PPTP协议创建一个支持身份验证的隧道连接
                    使用MPPE对数据进行加密
                    为客户端分配192.168.3.0/24的地址池
                    客户端连接的用户名为jacob，密码为123456
            1.部署VPN服务器

                1）安装软件包（软件包参考lnmp_soft）
                    [root@proxy ~]# yum localinstall pptpd-1.4.0-2.el7.x86_64.rpm
                    [root@proxy ~]# rpm -qc pptpd
                    /etc/ppp/options.pptpd
                    /etc/pptpd.conf
                    /etc/sysconfig/pptpd
                2)修改配置文件
                    [root@proxy ~]# vim /etc/pptpd.conf
                    .. ..
                    localip 201.1.2.5                                    //服务器本地IP
                    remoteip 192.168.3.1-50                            //分配给客户端的IP池
                    [root@proxy ~]# vim /etc/ppp/options.pptpd
                    require-mppe-128                                    //使用MPPE加密数据
                    ms-dns 8.8.8.8                                    //DNS服务器
                    [root@proxy ~]# vim /etc/ppp/chap-secrets            //修改账户配置文件
                    jacob           *               123456      *
                    //用户名    服务器标记    密码    客户端
                    [root@proxy ~]# echo "1" > /proc/sys/net/ipv4/ip_forward    //开启路由转发
                3）启动服务
                    [root@proxy ~]# systemctl start pptpd
                    [root@proxy ~]# systemctl enable pptpd
                    [root@proxy ~]# firewall-cmd --set-default-zone=trusted
                4）翻墙设置（非必需操作）
                    [root@proxy ~]# iptables -t nat -A POSTROUTING -s 192.168.3.0/24 \
                    >  -j SNAT --to-source 201.1.2.5

                5)在Windows 上 配置 vpn 链接

         创建L2TP+IPSec VPN(W和L都支持)
                本案例要求搭建一个L2TP+IPSec VPN环境，并测试该VPN网络是否能够正常通讯，具体要求如下：
                使用L2TP协议创建一个支持身份验证与加密的隧道连接
                使用IPSec对数据进行加密
                为客户端分配192.168.3.0/24的地址池
                客户端连接的用户名为：jacob，密码为：123456
                预共享密钥为：randpass
            1.部署IPSec服务

                1）安装软件包
                    [root@client ~]# yum -y install libreswan
                2)新建IPSec密钥验证配置文件
                    [root@client ~]# cat /etc/ipsec.conf                //仅查看一下该主配置文件
                    .. ..
                    include /etc/ipsec.d/*.conf                    //加载该目录下的所有配置文件
                    [root@client ~]# vim /etc/ipsec.d/myipsec.conf
                    //新建该文件，参考lnmp_soft/vpn/myipsec.conf
                    conn IDC-PSK-NAT
                        rightsubnet=vhost:%priv                        //允许建立的VPN虚拟网络
                        also=IDC-PSK-noNAT
                    conn IDC-PSK-noNAT
                        authby=secret                                    //加密认证
                            ike=3des-sha1;modp1024                        //算法
                            phase2alg=aes256-sha1;modp2048                //算法
                        pfs=no
                        auto=add
                        keyingtries=3
                        rekey=no
                        ikelifetime=8h
                        keylife=3h
                        type=transport
                        left=201.1.2.10                                //重要，服务器本机的外网IP
                        leftprotoport=17/1701
                        right=%any                                    //允许任何客户端连接
                        rightprotoport=17/%any
                3)创建IPSec预定义共享密钥
                    [root@client ~]# cat /etc/ipsec.secrets                 //仅查看，不要修改该文件
                    include /etc/ipsec.d/*.secrets
                    [root@client ~]# vim /etc/ipsec.d/mypass.secrets        //新建该文件
                    201.1.2.10   %any:    PSK    "randpass"             //randpass为预共享密钥
                                                                    //201.1.2.10是VPN服务器的IP
                4)启动IPSec服务
                    [root@client ~]# systemctl start ipsec
                    [root@client ~]# netstat -ntulp |grep pluto
                    udp        0      0 127.0.0.1:4500          0.0.0.0:*           3148/pluto
                    udp        0      0 192.168.4.10:4500      0.0.0.0:*           3148/pluto
                    udp        0      0 201.1.2.10:4500         0.0.0.0:*           3148/pluto
                    udp        0      0 127.0.0.1:500           0.0.0.0:*           3148/pluto
                    udp        0      0 192.168.4.10:500       0.0.0.0:*           3148/pluto
                    udp        0      0 201.1.2.10:500          0.0.0.0:*           3148/pluto
                    udp6       0      0 ::1:500                 :::*                 3148/pluto

            2.部署XL2TP服务

                1）安装软件包（软件包参考lnmp_soft）
                    [root@client ~]# yum localinstall xl2tpd-1.3.8-2.el7.x86_64.rpm
                2) 修改xl2tp配置文件（修改3个配置文件的内容）
                    [root@client ~]#  vim  /etc/xl2tpd/xl2tpd.conf                //修改主配置文件
                    [global]
                    .. ..
                    [lns default]
                    .. ..
                    ip range = 192.168.3.128-192.168.3.254                    //分配给客户端的IP池
                    local ip = 201.1.2.10                                    //VPN服务器的IP地址
                    [root@client ~]# vim /etc/ppp/options.xl2tpd            //认证配置
                    require-mschap-v2                                         //添加一行，强制要求认证
                    #crtscts                                                //注释或删除该行
                    #lock                                                //注释或删除该行
                    root@client ~]# vim /etc/ppp/chap-secrets                    //修改密码文件
                    jacob   *       123456  *                //账户名称   服务器标记   密码   客户端IP
                3）启动服务
                    [root@client ~]# systemctl start xl2tpd
                    [root@client ~]# netstat  -ntulp |grep xl2tpd
                    udp     0      0 0.0.0.0:1701      0.0.0.0:*          3580/xl2tpd
                4）设置路由转发，防火墙
                    [root@client ~]# echo "1" > /proc/sys/net/ipv4/ip_forward
                    [root@client ~]# firewall-cmd --set-default-zone=trusted
                5）翻墙设置（非必需操作）
                    [root@client ~]# iptables -t nat -A POSTROUTING -s 192.168.3.0/24 \
                    >  -j SNAT --to-source 201.1.2.10
            3.客户端设置

                启动一台Windows虚拟机，将虚拟机网卡桥接到public2，配置IP地址为201.1.2.20。
                1) 新建网络连接（参考案例2），输入VPN服务器账户与密码（参考案例2）。
                    属性---选择----安全---xl2 ---- randpass
                    若是 win7 以上则不需要修改注册表
                2)设置Windows注册表（不修改注册表，连接VPN默认会报789错误），具体操作如下：
                    单击"开始"，单击"运行"，键入"regedit"，然后单击"确定"
                    找到下面的注册表子项，然后单击它：
                    HKEY_LOCAL_MACHINE\ System\CurrentControlSet\Services\Rasman\Parameters
                    在"编辑"菜单上，单击"新建"->"DWORD值"
                    在"名称"框中，键入"ProhibitIpSec"
                    在"数值数据"框中，键入"1"，然后单击"确定"
                    退出注册表编辑器，然后重新启动计算机

         linux 内核模块
                modprob  模块名称
                lsmod    查看安装模块
                rmmod    删除模块
                modinfo
                iptunnel add 创建隧道tun0
                ip addr add  网络地址
         pptpd
                3个配置文件
                vpn的私有网络
                localipremoteip
                ms-dns
                用户名和密码
         xl2tp和ipsec
                ipsec
                主配置文件
                /etc/ipsec.d/xxx.conf  加密算法
                /etc/ipsec.d//xxxx/secrets PSK 域共享秘钥
                xl2tp
                主配置文件
                3个配置文件
                /etc/xl2tpd/xl2tpd.conf
                /etc/ppp/options.xl2tpd
                /etc/ppp/chap-secrets
    NTP时间同步
        要求
            搭建一个NTP服务器，为整个网络环境中的所有主机提供时间校准服务，具体要求如下：
            部署一台NTP时间服务器
            设置时间服务器上层与0.centos.pool.ntp.org同步
            设置本地服务器层级数量为10
            允许192.168.4.0/24网络的主机同步时间
            客户端验证时间是否同步
        方案
            clients 作为客户端  192.168.4.5
            proxy 作为服务端  ---4.10

            软件 chrondy默认安装 最小化安装也会有
            Network Time Protocol（网络时间协议）采用的是分层设计
            Stratum层的总数限制在15以内（包括15）。(不超过16跳)
        1.部署NTP服务

            1）安装软件包
                [root@proxy ~]# yum -y install chrony
                [root@proxy ~]# rpm -qc chrony                        //查看配置文件列表
                /etc/chrony.conf
                /etc/chrony.keys
                .. ..
            2)修改配置文件
                [root@proxy ~]# cat /etc/chrony.conf
                .. ..
                server 0.centos.pool.ntp.org iburst         //server用户客户端指向上层NTP服务器
                allow 192.168.4.0/24                        //允许那个IP或网络访问NTP
                #deny  192.168.4.1                        //拒绝那个IP或网络访问NTP
                local stratum 10                            //设置NTP服务器的层数量 不超过15层
                .. ..
            3) 将客户端时间修改为错误的时间
                [root@client ~]#  date  -s  "hour:minute"         //调整时间（小时：分钟）
                [root@client ~]#  date                            //查看修改后的时间
            4) 重启chrony与服务器同步时间
                [root@client ~]# systemctl  restart  chronyd
            5) 确认时间是否已经同步
                [root@client ~]# date                            //多执行几次查看结果




    CLUSTER 集群,簇

            NTP(闰秒) 随机的多了一秒 60 无法预测提前准备
    集群以及LVS
        集群分类:
                高性能计算集群HPC 解决科学问题
                高可靠集群(HA)   互联网多用
                高负载均衡集群(LB)
        需要注意的是lvs 与 nginx 代理 原理不同
            clients 访问nginx(代理clients访问web1)   nginx 访问 web1
            clients 访问lvs(相当于路由器)    转发给    web1 / web2

        lvs(nat)==路由器(ip forward = 1 )
        web1 web2网关 必须陪

        优点:
            降低成本 提高成本 增加可靠性 高扩展

        LVS术语:
                Director Server  调度服务器
                real server      真实服务器
                VIP              虚拟IP
                RIP              真实IP
                DIP              调度器链接节点服务器的ip
        LVS 工作模式:
            NAT 模式 调度和真实服务器都在 局域网中,之间直接链接
                和nginx的调度服务器相同(瓶颈也是调度服务器,尤其是数据回传给用户的) 用户与调度服务器链接,由调度服务器 回传数据
            DR  模式 调度和真实服务器都在 局域网中,使用 交换机链接 用户 直接与路由器 链接,直接由 互联网 传递数据,不通过调度服务器
              TUN 模式 (用的少) 所有的服务器都在互联网 中 调度服务器与真实服务器间 用 vpn 链接 ,数据回传有互联网 传输
            -g (DR)
            -i (tun)
            -m (NAT)
        ipvsadm
            常用的命令
                ipvsadm-A               添加虚拟服务器
                ipvsadm-E               修改虚拟服务器
                ipvsadm-D               删除虚拟服务器
                ipvsadm-C               删除所有
                ipvsadm-e               修改真实服务器
                ipvsadm-a               添加真实服务器
                ipvsadm-d               删除真实服务器
                ipvsadm-L               查看LVS 规则表
                -s[rr|wrr|lc|wlc]       指定集群算法
                rr 论循算法
                wrr 加权轮循算法
                sh ip哈希
                lc 最小链接
                wlc 加权最少链接
            方案
                1.ipvsadm命令用法
                    1)创建lvs集群 算法为wrr
                        [root@proxy ~]# yum -y install ipvsadm
                        [root@proxy ~]# ipvsadm -A -t 192.168.4.5:80 -s wrr
                        [root@proxy ~]# ipvsadm -Ln
                        IP Virtual Server version 1.2.1 (size=4096)
                        Prot LocalAddress:Port Scheduler Flags
                          -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
                        TCP  192.168.4.5:80 wrr
                    2)向集群添加rs
                        [root@proxy ~]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.100 -m -w 1
                        ############# -m  指定工作模式  -w 指定权重  ############
                        [root@proxy ~]# ipvsadm -Ln
                        IP Virtual Server version 1.2.1 (size=4096)
                        Prot LocalAddress:Port Scheduler Flags
                          -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
                        TCP  192.168.4.5:80 wrr
                          -> 192.168.2.100:80             Masq    1      0          0
                        [root@proxy ~]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.200 -m -w 2
                        [root@proxy ~]# ipvsadm –a -t 192.168.4.5:80 -r 192.168.2.201 -m -w 3
                        [root@proxy ~]# ipvsadm –a -t 192.168.4.5:80 -r 192.168.2.202 -m -w 4
                    3）修改集群服务器设置(-E 修改调度器算法，将加权轮询修改为轮询)
                        [root@proxy ~]# ipvsadm -E -t 192.168.4.5:80 -s rr
                        [root@proxy ~]# ipvsadm -Ln
                        IP Virtual Server version 1.2.1 (size=4096)
                        Prot LocalAddress:Port Scheduler Flags
                          -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
                        TCP  192.168.4.5:80 rr
                          -> 192.168.2.100:80             Masq    1      0          0
                          -> 192.168.2.200:80             Masq    2      0          0
                          -> 192.168.2.201:80             Masq    2      0          0
                          -> 192.168.2.202:80             Masq    1      0          0
                    4）修改read server（-e  使用-g选项，将模式改为DR模式）
                        [root@proxy ~]# ipvsadm -e -t 192.168.4.5:80 -r 192.168.2.202 -g

                    5）查看LVS状态
                        [root@proxy ~]# ipvsadm -Ln
                        6）创建另一个集群（算法为最少连接算法；使用-m选项，设置工作模式为NAT模式）
                        [root@proxy ~]# ipvsadm -A -t 192.168.4.5:3306 -s lc
                        [root@proxy ~]# ipvsadm -a -t 192.168.4.5:3306 -r 192.168.2.100 -m
                        [root@proxy ~]# ipvsadm -a -t 192.168.4.5:3306 -r 192.168.2.200 -m
                        6）永久保存所有规则
                        [root@proxy ~]# ipvsadm-save -n > /etc/sysconfig/ipvsadm
                        7）清空所有规则
                        [root@proxy ~]# ipvsadm -C

                2.部署LVS-NAT集群
                    要求
                        使用LVS实现NAT模式的集群调度服务器，为用户提供Web服务：
                        集群对外公网IP地址为192.168.4.5
                        调度器内网IP地址为192.168.2.5
                        真实Web服务器地址分别为192.168.2.100、192.168.2.200
                        使用加权轮询调度算法，真实服务器权重分别为1和2


                        将web1和 web2 的网关配置192.168.2.5
                        只保留一个网卡启用,不然会出错

                    (1)配置基础环境
                        1）设置Web服务器（以web1为例）
                            [root@web1 ~]# yum -y install httpd
                            [root@web1 ~]# echo "192.168.2.100" > /var/www/html/index.html
                        2）启动Web服务器软件
                            [root@web1 ~]# systemctl restart httpd
                        3)关闭防火墙与SELinux
                            [root@web1 ~]# systmctl stop firewalld
                            [root@web1 ~]# setenforce 0
                    (2)部署LVS-NAT模式调度器

                        1)确认调度器的路由转发功能(如果已经开启，可以忽略)
                            [root@proxy ~]# echo 1 > /proc/sys/net/ipv4/ip_forward
                            [root@proxy ~]# cat /proc/sys/net/ipv4/ip_forward
                            1
                            [root@proxy ~]# echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
                        #修改配置文件，设置永久规则
                        2）创建集群服务器
                            [root@proxy ~]# yum -y install ipvsadm
                            [root@proxy ~]# ipvsadm -A -t 192.168.4.5:80 -s wrr
                        2）添加真实服务器
                            [root@proxy ~]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.100 -w 1 -m
                            [root@proxy ~]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.200 -w 1 -m
                        3）查看规则列表，并保存规则
                            [root@proxy ~]# ipvsadm -Ln
                            [root@proxy ~]# ipvsadm-save -n > /etc/sysconfig/ipvsadm

                3.部署LVS-DR集群
                    要求
                        使用LVS实现DR模式的集群调度服务器，为用户提供Web服务：
                        客户端IP地址为192.168.4.10
                        LVS调度器VIP地址为192.168.4.15
                        LVS调度器DIP地址设置为192.168.4.5
                        真实Web服务器地址分别为192.168.4.100、192.168.4.200
                        使用加权轮询调度算法，web1的权重为1，web2的权重为2
                    补充
                        CIP是客户端的IP地址；
                        VIP是对客户端提供服务的IP地址；
                        RIP是后端服务器的真实IP地址；
                        DIP是调度器与后端服务器通信的IP地址（VIP必须配置在虚拟接口）\

                                       _____web1 eth0 4.100
                                       |    lo:0 4.15/32()
                                       |
                    clients -------->lvs1 lvs2
                                       |     web1 eth0 4.200
                                       |_____lo:0 4.15/32(添加一个虚拟ip,也叫主机网络只有一个主机位,且只响应物理网卡ip)
                        keeplived (VRRP协议:虚拟路由热备协议) 提高可靠性

                    (1)配置实验网络环境

                        1）设置Proxy代理服务器的VIP和DIP

                            注意：为了防止冲突，VIP必须要配置在网卡的虚拟接口！！！

                             [root@proxy ~]# cd /etc/sysconfig/network-scripts/
                            [root@proxy ~]# cp ifcfg-eth0{,:0}
                            [root@proxy ~]# vim ifcfg-eth0
                            TYPE=Ethernet
                            BOOTPROTO=none
                            NAME=eth0
                            DEVICE=eth0
                            ONBOOT=yes
                            IPADDR=192.168.4.5
                            PREFIX=24
                            [root@proxy ~]# vim ifcfg-eth0:0
                            TYPE=Ethernet
                            BOOTPROTO=none
                            DEFROUTE=yes
                            NAME=eth0:0
                            DEVICE=eth0:0
                            ONBOOT=yes
                            IPADDR=192.168.4.15
                            PREFIX=24
                            [root@proxy ~]# systemctl restart network
                        2）设置Web1服务器网络参数
                            [root@web1 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.100/24 connection.autoconnect yes
                            [root@web1 ~]# nmcli connection up eth0
                            接下来给web1配置VIP地址。
                            注意：这里的子网掩码必须是32（也就是全255），网络地址与IP地址一样，广播地址与IP地址也一样。
                            [root@web1 ~]# cd /etc/sysconfig/network-scripts/
                            [root@web1 ~]# cp ifcfg-lo{,:0}    ####将 ifcfg-lo 复制并重命名为 ifcfg-lo:0
                            [root@web1 ~]# vim ifcfg-lo:0
                            DEVICE=lo:0
                            IPADDR=192.168.4.15
                            NETMASK=255.255.255.255
                            NETWORK=192.168.4.15
                            BROADCAST=192.168.4.15
                            ONBOOT=yes
                            NAME=lo:0
                            防止地址冲突的问题：
                            这里因为web1也配置与代理一样的VIP地址，默认肯定会出现地址冲突；
                            sysctl.conf文件写入这下面四行的主要目的就是访问192.168.4.15的数据包，只有调度器会响应，其他主机都不做任何响应，这样防止地址冲突的问题。
                            [root@web1 ~]# vim /etc/sysctl.conf
                            #手动写入如下4行内容
                            net.ipv4.conf.all.arp_ignore = 1
                            net.ipv4.conf.lo.arp_ignore = 1
                            net.ipv4.conf.lo.arp_announce = 2
                            net.ipv4.conf.all.arp_announce = 2
                            #当有arp广播问谁是192.168.4.15时，本机忽略该ARP广播，不做任何回应
                            #本机不要向外宣告自己的lo回环地址是192.168.4.15
                            [root@web1 ~]# sysctl -p
                            重启网络服务，设置防火墙与SELinux
                            [root@web1 ~]# systemctl restart network
                            [root@web1 ~]# ifconfig
                            [root@web1 ~]# systemctl stop firewalld
                            [root@web1 ~]# setenforce 0
                        3）设置Web2服务器网络参数
                            [root@web2 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.200/24 connection.autoconnect yes
                            [root@web2 ~]# nmcli connection up eth0
                            接下来给web2配置VIP地址
                            注意：这里的子网掩码必须是32（也就是全255），网络地址与IP地址一样，广播地址与IP地址也一样。
                            [root@web2 ~]# cd /etc/sysconfig/network-scripts/
                            [root@web2 ~]# cp ifcfg-lo{,:0}
                            [root@web2 ~]# vim ifcfg-lo:0
                            DEVICE=lo:0
                            IPADDR=192.168.4.15
                            NETMASK=255.255.255.255
                            NETWORK=192.168.4.15
                            BROADCAST=192.168.4.15
                            ONBOOT=yes
                            NAME=lo:0
                            防止地址冲突的问题：
                            这里因为web1也配置与代理一样的VIP地址，默认肯定会出现地址冲突；
                            sysctl.conf文件写入这下面四行的主要目的就是访问192.168.4.15的数据包，只有调度器会响应，其他主机都不做任何响应，这样防止地址冲突的问题。
                            [root@web2 ~]# vim /etc/sysctl.conf
                            #手动写入如下4行内容
                            net.ipv4.conf.all.arp_ignore = 1
                            net.ipv4.conf.lo.arp_ignore = 1
                            net.ipv4.conf.lo.arp_announce = 2
                            net.ipv4.conf.all.arp_announce = 2
                            #当有arp广播问谁是192.168.4.15时，本机忽略该ARP广播，不做任何回应
                            #本机不要向外宣告自己的lo回环地址是192.168.4.15
                            [root@web2 ~]# sysctl -p
                            重启网络服务，设置防火墙与SELinux
                            [root@web2 ~]# systemctl restart network
                            [root@web2 ~]# ifconfig
                            [root@web2 ~]# systemctl stop firewalld
                            [root@web2 ~]# setenforce 0
                    (2)配置后端Web服务器

                        1）自定义Web页面
                            [root@web1 ~]# yum -y install httpd
                            [root@web1 ~]# echo "192.168.4.100" > /var/www/html/index.html
                            [root@web2 ~]# yum -y install httpd
                            [root@web2 ~]# echo "192.168.4.200" > /var/www/html/index.html
                        2）启动Web服务器软件
                            [root@web1 ~]# systemctl restart httpd
                            [root@web2 ~]# systemctl restart httpd
                    (3)proxy调度器安装软件并部署LVS-DR模式调度器

                        1）安装软件（如果已经安装，此步骤可以忽略）
                            [root@proxy ~]# yum -y install ipvsadm
                        2）清理之前实验的规则，创建新的集群服务器规则
                            [root@proxy ~]# ipvsadm -C                                #清空所有规则
                            [root@proxy ~]# ipvsadm -A -t 192.168.4.15:80 -s wrr
                        3）添加真实服务器(-g参数设置LVS工作模式为DR模式，-w设置权重)
                            [root@proxy ~]# ipvsadm -a -t 192.168.4.15:80 -r 192.168.4.100 -g -w 1
                            [root@proxy ~]# ipvsadm -a -t 192.168.4.15:80 -r 192.168.4.200 -g -w 1
                        4）查看规则列表，并保存规则
                            [root@proxy ~]# ipvsadm -Ln
                            TCP  192.168.4.15:80 wrr
                              -> 192.168.4.100:80             Route   1      0          0
                              -> 192.168.4.200:80             Route   2      0          0
                            [root@proxy ~]# ipvsadm-save -n > /etc/sysconfig/ipvsadm
                        5)访问测试
                            [root@client ~]# curl 192.168.4.15
                                <h1><marquee> I AM web1
                            [root@client ~]# curl 192.168.4.15
                                <h1>I AM WEB2


        keeplived高可用
                1.配置网络环境（如果在前面课程已经完成该配置，可以忽略此步骤）

                        1）设置Web1服务器网络参数、配置Web服务
                            [root@web1 ~]# nmcli connection modify eth0 ipv4.method manual ipv4.addresses 192.168.4.100/24 connection.autoconnect yes
                            [root@web1 ~]# nmcli connection up eth0
                            [root@web1 ~]# yum -y install httpd
                            [root@web1 ~]# echo "192.168.4.100" > /var/www/html/index.html
                            [root@web1 ~]# systemctl restart httpd
                        2）设置Web2服务器网络参数、配置Web服务
                            [root@web2 ~]# nmcli connection modify eth0 ipv4.method manual ipv4.addresses 192.168.4.200/24 connection.autoconnect yes
                            [root@web2 ~]# nmcli connection up eth0
                            [root@web2 ~]# yum -y install httpd
                            [root@web2 ~]# echo "192.168.4.200" > /var/www/html/index.html
                            [root@web2 ~]# systemctl restart httpd
                        3）配置proxy主机的网络参数（如果已经设置，可以忽略此步骤）
                            [root@proxy ~]# nmcli connection modify eth0 ipv4.method manual ipv4.addresses 192.168.4.5/24 connection.autoconnect yes
                            [root@proxy ~]# nmcli connection up eth0
                2.安装Keepalived软件

                        注意：两台Web服务器做相同的操作。
                            [root@web1 ~]# yum install -y keepalived
                            [root@web2 ~]# yum install -y keepalived
                3.部署Keepalived服务
                            1.实现vrrpvip
                            2.自动配置lvs,健康检查
                        1）修改web1服务器Keepalived配置文件
                            [root@web1 ~]# vim /etc/keepalived/keepalived.conf
                            global_defs {
                              notification_email {
                                admin@tarena.com.cn                //设置报警收件人邮箱
                              }
                              notification_email_from ka@localhost    //设置发件人
                              smtp_server 127.0.0.1                //定义邮件服务器
                              smtp_connect_timeout 30
                              router_id  web1                        //设置路由ID号（实验需要修改）
                            }
                            vrrp_instance VI_1 {
                              state MASTER                         //主服务器为MASTER（备服务器需要修改为BACKUP）, state 是作为 软件开启是临时状态,最终状态会由 priority 的数值决定,当priority 也不能比较 出结果的时候,对 ip 和 mac 进行对比.
                              interface eth0                    //定义网络接口
                              virtual_router_id 51                //主备服务器VRID号必须一致
                              priority 100                     //服务器优先级,优先级高优先获取VIP（实验需要修改）,也是最终决定 主副的 方式
                              advert_int 1
                              authentication {
                                auth_type pass
                                auth_pass 1111                       //主备服务器密码必须一致
                              }
                              virtual_ipaddress {                   //谁是主服务器谁获得该VIP（实验需要修改）
                                192.168.4.80
                              }
                              }
                        2）修改web2服务器Keepalived配置文件
                            [root@web2 ~]# vim /etc/keepalived/keepalived.conf
                            global_defs {
                              notification_email {
                                admin@tarena.com.cn                //设置报警收件人邮箱
                              }
                              notification_email_from ka@localhost    //设置发件人
                              smtp_server 127.0.0.1                //定义邮件服务器
                              smtp_connect_timeout 30
                              router_id  web2                        //设置路由ID号（实验需要修改）
                            }
                            vrrp_instance VI_1 {
                              state BACKUP                             //备服务器为BACKUP（实验需要修改）
                              interface eth0                        //定义网络接口
                              virtual_router_id 51                    //主辅VRID号必须一致
                              priority 50                             //服务器优先级（实验需要修改）
                              advert_int 1
                              authentication {
                                 auth_type pass
                                 auth_pass 1111                       //主辅服务器密码必须一致
                              }
                              virtual_ipaddress {                 //谁是主服务器谁配置VIP（实验需要修改）
                              192.168.4.80
                             }
                            }
                        3）启动服务
                            [root@web1 ~]# systemctl start keepalived
                            [root@web2 ~]# systemctl start keepalived
                        4）配置防火墙和SELinux
                            启动keepalived会自动添加一个drop的防火墙规则，需要清空！
                            [root@web1 ~]# iptables -F   ####关闭 IPtables防火墙
                            [root@web1 ~]# setenforce 0
                            [root@web2 ~]# iptables -F
                            [root@web1 ~]# setenforce 0


        Keepalived+LVS服务器
                要求
                        Keepalived为LVS调度器提供高可用功能，防止调度器单点故障，为用户提供Web服务：
                        LVS1调度器真实IP地址为192.168.4.5
                        LVS2调度器真实IP地址为192.168.4.6
                        服务器VIP地址设置为192.168.4.15
                        真实Web服务器地址分别为192.168.4.100、192.168.4.200
                        使用加权轮询调度算法，真实web服务器权重不同

                1.配置网络环境

                        1）设置Web1服务器的网络参数
                        [root@web1 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.100/24 connection.autoconnect yes
                            [root@web1 ~]# nmcli connection up eth0
                            接下来给web1配置VIP地址
                            注意：这里的子网掩码必须是32（也就是全255），网络地址与IP地址一样，广播地址与IP地址也一样。
                            [root@web1 ~]# cd /etc/sysconfig/network-scripts/
                            [root@web1 ~]# cp ifcfg-lo{,:0}
                            [root@web1 ~]# vim ifcfg-lo:0
                            DEVICE=lo:0
                            IPADDR=192.168.4.15
                            NETMASK=255.255.255.255
                            NETWORK=192.168.4.15
                            BROADCAST=192.168.4.15
                            ONBOOT=yes
                            NAME=lo:0
                            注意：这里因为web1也配置与调度器一样的VIP地址，默认肯定会出现地址冲突。
                            写入这四行的主要目的就是访问192.168.4.15的数据包，只有调度器会响应，其他主机都不做任何响应。
                            [root@web1 ~]# vim /etc/sysctl.conf
                            #手动写入如下4行内容
                            net.ipv4.conf.all.arp_ignore = 1
                            net.ipv4.conf.lo.arp_ignore = 1
                            net.ipv4.conf.lo.arp_announce = 2
                            net.ipv4.conf.all.arp_announce = 2
                            #当有arp广播问谁是192.168.4.15时，本机忽略该ARP广播，不做任何回应
                            #本机不要向外宣告自己的lo回环地址是192.168.4.15
                            重启网络服务，设置防火墙与SELinux
                            [root@web1 ~]# systemctl restart network
                            [root@web1 ~]# ifconfig
                            [root@web1 ~]# systemctl stop firewalld
                            [root@web1 ~]# setenforce 0
                        2）设置Web2服务器的网络参数
                            [root@web2 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.200/24 connection.autoconnect yes
                            [root@web2 ~]# nmcli connection up eth0
                            接下来给web2配置VIP地址
                            注意：这里的子网掩码必须是32（也就是全255），网络地址与IP地址一样，广播地址与IP地址也一样。
                            [root@web2 ~]# cd /etc/sysconfig/network-scripts/
                            [root@web2 ~]# cp ifcfg-lo{,:0}
                            [root@web2 ~]# vim ifcfg-lo:0
                            DEVICE=lo:0
                            IPADDR=192.168.4.15
                            NETMASK=255.255.255.255
                            NETWORK=192.168.4.15
                            BROADCAST=192.168.4.15
                            ONBOOT=yes
                            NAME=lo:0
                            注意：这里因为web2也配置与代理一样的VIP地址，默认肯定会出现地址冲突。
                            写入这四行的主要目的就是访问192.168.4.15的数据包，只有调度器会响应，其他主机都不做任何响应。
                            [root@web2 ~]# vim /etc/sysctl.conf
                            #手动写入如下4行内容
                            net.ipv4.conf.all.arp_ignore = 1
                            net.ipv4.conf.lo.arp_ignore = 1
                            net.ipv4.conf.lo.arp_announce = 2
                            net.ipv4.conf.all.arp_announce = 2
                            #当有arp广播问谁是192.168.4.15时，本机忽略该ARP广播，不做任何回应
                            #本机不要向外宣告自己的lo回环地址是192.168.4.15
                            重启网络服务，设置防火墙与SELinux
                            [root@web2 ~]# systemctl restart network
                            [root@web2 ~]# ifconfig
                            [root@web2 ~]# systemctl stop firewalld
                            [root@web2 ~]# setenforce 0
                        3）配置proxy1主机的网络参数(不配置VIP，由keepalvied自动配置)
                            [root@proxy1 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.5/24 connection.autoconnect yes
                            [root@proxy1 ~]# nmcli connection up eth0
                        4）配置proxy2主机的网络参数(不配置VIP，由keepalvied自动配置)
                            注意：按照前面的课程环境，默认没有该虚拟机，需要重新建一台虚拟机proxy2。
                            [root@proxy2 ~]# nmcli connection modify eth0 ipv4.method manual \
                            ipv4.addresses 192.168.4.6/24 connection.autoconnect yes
                            [root@proxy2 ~]# nmcli connection up eth0
                2.配置后台web服务

                        1）安装软件，自定义Web页面（web1和web2主机）
                            [root@web1 ~]# yum -y install httpd
                            [root@web1 ~]# echo "192.168.4.100" > /var/www/html/index.html
                            [root@web2 ~]# yum -y install httpd
                            [root@web2 ~]# echo "192.168.4.200" > /var/www/html/index.html
                        2）启动Web服务器软件(web1和web2主机)
                            [root@web1 ~]# systemctl start httpd ; systemctl enable httpd
                            [root@web2 ~]# systemctl start httpd ; systemctl enable httpd
                3.调度器安装Keepalived与ipvsadm软件

                        注意：两台LVS调度器执行相同的操作（如何已经安装软件，可用忽略此步骤）。
                        安装软件
                            [root@proxy1 ~]# yum install -y keepalived
                            [root@proxy1 ~]# systemctl enable keepalived
                            [root@proxy1 ~]# yum install -y ipvsadm
                            [root@proxy1 ~]# ipvsadm -C
                            [root@proxy2 ~]# yum install -y keepalived
                            [root@proxy2 ~]# systemctl enable keepalived
                            [root@proxy2 ~]# yum install -y ipvsadm
                            [root@proxy2 ~]# ipvsadm -C
                4.部署Keepalived实现LVS-DR模式调度器的高可用

                        1）LVS1调度器设置Keepalived，并启动服务
                            [root@proxy1 ~]# vim /etc/keepalived/keepalived.conf
                            global_defs {
                              notification_email {
                                admin@tarena.com.cn                //设置报警收件人邮箱
                              }
                              notification_email_from ka@localhost    //设置发件人
                              smtp_server 127.0.0.1                //定义邮件服务器
                              smtp_connect_timeout 30
                              router_id  lvs1                        //设置路由ID号(实验需要修改)
                            }
                            vrrp_instance VI_1 {
                              state MASTER                             //主服务器为MASTER
                              interface eth0                        //定义网络接口
                              virtual_router_id 50                    //主辅VRID号必须一致
                              priority 100                         //服务器优先级
                              advert_int 1
                              authentication {
                                auth_type pass
                                auth_pass 1111                       //主辅服务器密码必须一致
                              }
                              virtual_ipaddress {                   //配置VIP（实验需要修改）
                            192.168.4.15
                             }
                            }
                            virtual_server 192.168.4.15 80 {           //设置ipvsadm的VIP规则（实验需要修改）
                              delay_loop 6
                              lb_algo wrr                          //设置LVS调度算法为WRR
                              lb_kind DR                               //设置LVS的模式为DR
                              #persistence_timeout 50
                            #注意这样的作用是保持连接，开启后，客户端在一定时间内始终访问相同服务器
                              protocol TCP
                              real_server 192.168.4.100 80 {         //设置后端web服务器真实IP（实验需要修改）
                                weight 1                             //设置权重为1
                                TCP_CHECK {                            //对后台real_server做健康检查
                                connect_timeout 3
                                nb_get_retry 3
                                delay_before_retry 3
                                }
                              }
                            real_server 192.168.4.200 80 {       //设置后端web服务器真实IP（实验需要修改）
                                weight 2                          //设置权重为2
                                TCP_CHECK {
                                connect_timeout 3
                                nb_get_retry 3
                                delay_before_retry 3
                                }
                              }
                            }
                            [root@proxy1 ~]# systemctl start keepalived
                            [root@proxy1 ~]# ipvsadm -Ln                     #查看LVS规则
                            [root@proxy1 ~]# ip a  s                          #查看VIP配置
                        2）LVS2调度器设置Keepalived
                            [root@proxy2 ~]# vim /etc/keepalived/keepalived.conf
                            global_defs {
                              notification_email {
                                admin@tarena.com.cn                //设置报警收件人邮箱
                              }
                              notification_email_from ka@localhost    //设置发件人
                              smtp_server 127.0.0.1                //定义邮件服务器
                              smtp_connect_timeout 30
                              router_id  lvs2                        //设置路由ID号（实验需要修改）
                            }
                            vrrp_instance VI_1 {
                              state BACKUP                             //从服务器为BACKUP（实验需要修改）
                              interface eth0                        //定义网络接口
                              virtual_router_id 50                    //主辅VRID号必须一致
                              priority 50                             //服务器优先级（实验需要修改）
                              advert_int 1
                              authentication {
                                auth_type pass
                                auth_pass 1111                       //主辅服务器密码必须一致
                              }
                              virtual_ipaddress {                   //设置VIP（实验需要修改）
                            192.168.4.15
                            }
                            }
                            #################这里是lvs (ipvsadm)规则 实现集群###############
                            virtual_server 192.168.4.15 80 {          //自动设置LVS规则（实验需要修改）
                              delay_loop 6
                              lb_algo wrr                          //设置LVS调度算法为WRR
                              lb_kind DR                               //设置LVS的模式为DR
                              # persistence_timeout 50
                              #注意这样的作用是保持连接，开启后，客户端在一定时间内始终访问相同服务器
                              protocol TCP
                              real_server 192.168.4.100 80 {        //设置后端web服务器的真实IP（实验需要修改）
                                weight 1                              //设置权重为1
                                TCP_CHECK {                         //对后台real_server做健康检查
                                connect_timeout 3
                                nb_get_retry 3
                                delay_before_retry 3
                                }
                              }

                            real_server 192.168.4.200 80 {         //设置后端web服务器的真实IP（实验需要修改）
                                weight 2                              //设置权重为2
                                TCP_CHECK {
                                connect_timeout 3
                                nb_get_retry 3
                                delay_before_retry 3
                                }
                              }
                            [root@proxy2 ~]# systemctl start keepalived
                            [root@proxy2 ~]# ipvsadm -Ln                 #查看LVS规则
                            [root@proxy2 ~]# ip  a   s                    #查看VIP设置


        配置HAProxy负载平衡集群
                    优点: 支持 session cookie
                          可以通过 url 进行健康检查
                          效率 负载均衡速度 高于 nginx 低于 lvs
                          HAProxy 支出tcp udp 可以对mysql 进行负载均衡
                          调度算法丰富
                    缺点:正则弱于nginx
                        日志不如 nginx

                    注意事项：
                    将前面实验VIP、LVS等实验的内容清理干净！！！！！！
                    删除所有设备的VIP，清空所有LVS设置，关闭keepalived！！！
                    web1关闭多余的网卡与VIP，配置本地真实IP地址。

                    [root@web1 ~]# ifdown eth0
                    [root@web1 ~]# ifdown lo:0
                    [root@web1 ~]# nmcli connection modify eth1 ipv4.method manual \
                    ipv4.addresses 192.168.2.100/24 connection.autoconnect yes
                    [root@web1 ~]# nmcli connection up eth1
                    Web2关闭多余的网卡与VIP，配置本地真实IP地址。
                    [root@web2 ~]# ifdown eth0
                    [root@web2 ~]# ifdown lo:0
                    [root@web2 ~]# nmcli connection modify eth1 ipv4.method manual \
                    ipv4.addresses 192.168.2.200/24 connection.autoconnect yes
                    [root@web2 ~]# nmcli connection up eth1
                    proxy关闭keepalived服务，清理LVS规则。
                    [root@proxy ~]# systemctl stop keepalived
                    [root@proxy ~]# systemctl disable keepalived
                    [root@proxy ~]# ipvsadm -C
                    [root@proxy ~]# nmcli connection modify eth0 ipv4.method manual \
                    ipv4.addresses 192.168.4.5/24 connection.autoconnect yes
                    [root@proxy ~]# nmcli connection up eth0
                    [root@proxy ~]# nmcli connection modify eth1 ipv4.method manual \
                    ipv4.addresses 192.168.2.5/24 connection.autoconnect yes
                    [root@proxy ~]# nmcli connection up eth1



                    配置文件  vim/etc/haproxy/haproxy.cfg

                    global 全局设置
                        maxconn 软件最大并发量
                    default
                        maxconn 每个集群最大并发量 , 每个集群 并发量 相加超过   全局的maxconn 是不可以的

                    写法1 :listen
                            集群
                           listen
                            集群
                    写法2:
                         frontend main *:80
                            usr_backend static
                         backend static
                            balance     roundrobin
                            server      192.168.1.22
                    这两个写法都可以正常启动服务 推荐写法1,用的时候只保留一个 一下的全部删除
                    ##############################
                    # main frontend which proxys to the backends
                    #---------------------------------------------------------------------
                    frontend  main *:5000
                        acl url_static       path_beg       -i /static /images /javascript /stylesheets
                        acl url_static       path_end       -i .jpg .gif .png .css .js

                        use_backend static          if url_static
                        default_backend             app

                    #---------------------------------------------------------------------
                    # static backend for serving up images, stylesheets and such
                    #---------------------------------------------------------------------
                    backend static
                        balance     roundrobin
                        server      static 127.0.0.1:4331 check

                    #---------------------------------------------------------------------
                    # round robin balancing between the various backends
                    #---------------------------------------------------------------------
                    backend app
                        balance     roundrobin
                        server  app1 127.0.0.1:5001 check
                        server  app2 127.0.0.1:5002 check
                        server  app3 127.0.0.1:5003 check
                        server  app4 127.0.0.1:5004 check

                    ######################################################################



             1.配置后端Web服务器
                    设置两台后端Web服务（如果已经配置完成，可用忽略此步骤）
                        [root@web1 ~]# yum -y install httpd
                        [root@web1 ~]# systemctl start httpd
                        [root@web1 ~]# echo "192.168.2.100" > /var/www/html/index.html
                        [root@web2 ~]# yum -y install httpd
                        [root@web2 ~]# systemctl start httpd
                        [root@web2 ~]# echo "192.168.2.200" > /var/www/html/index.html
             2.部署HAProxy服务器
                    1）配置网络，安装软件
                        [root@haproxy ~]# yum -y install haproxy
                    2）修改配置文件
                        [root@haproxy ~]# vim /etc/haproxy/haproxy.cfg
                        global
                         log 127.0.0.1 local2   ###[err warning info debug]
                         chroot /usr/local/haproxy
                         pidfile /var/run/haproxy.pid ###haproxy的pid存放路径
                         maxconn 4000     ###最大连接数，默认4000
                         user haproxy
                         group haproxy
                         daemon       ###创建1个进程进入deamon模式运行
                        defaults
                         mode http    ###默认的模式mode { tcp|http|health } log global   ###采用全局定义的日志
                         option dontlognull  ###不记录健康检查的日志信息
                         option httpclose  ###每次请求完毕后主动关闭http通道
                         option httplog   ###日志类别http日志格式
                         option forwardfor  ###后端服务器可以从Http Header中获得客户端ip
                         option redispatch  ###serverid服务器挂掉后强制定向到其他健康服务器
                         timeout con nect 10000 #如果backend没有指定，默认为10s
                         timeout client 300000 ###客户端连接超时
                         timeout server 300000 ###服务器连接超时
                         maxconn  60000  ###最大连接数
                         retries  3   ###3次连接失败就认为服务不可用，也可以通过后面设置
                        listen stats 0.0.0.0:1080   #监听端口
                            stats refresh 30s   #统计页面自动刷新时间
                            stats uri /stats   #统计页面url
                            stats realm Haproxy Manager #进入管理解面查看状态信息
                            stats auth admin:admin  #统计页面用户名和密码设置
                          #stats hide-version   #隐藏统计页面上HAProxy的版本信息
                        listen  websrv-rewrite 0.0.0.0:80
                           balance roundrobin
                           server  web1 192.168.2.100:80 check inter 2000 rise 2 fall 5
                           server  web2 192.168.2.200:80 check inter 2000 rise 2 fall 5
                    3）启动服务器并设置开机启动
                        [root@haproxy ~]# systemctl start haproxy
                        [root@haproxy ~]# systemctl enable haproxy




             keeplive (vrrp)
                    浮动vip (高可用)
                    自动配置lvs规则 ,健康检查(tcp_check , http_get  , ssl_get)





    ceph储存
        常见的存储类型(面试常考)
            DAS  直连存储(direct attach storage)  SATA  SAS(服务器)  IDE
            NAS  网络附加存储(network attach storage)  nfs  http samba  ftp  共享文件系统
            SAN  存储区域网络(iscsia):共享块设备(需要格式化后挂载使用)
        分布式存储(distributed filesystem 物理节点不一定要直连在本地)
            几乎可以扩展无线大的空间
            常见的分布式文件系统
                lustre
                Hadoop (大数据用的多)
                FastDFS
                Ceph  (目前比较多,且成分布式文件系统的标准)
                GlusterFS

        ceph 特点:
                高扩展 高可用 高性能
                提供对象存储 块存储 文件系统存储
                提供EB 级别 储存空间
                软件定义存储 已经成为行业趋势SDS(soft define storage) 不绑定厂商
        组成:
            osd     提供储存设备
            monitor 监控/管理  负责调度(取余算法,将数据平均分配到每个osd,至少三台负责 高可用)
            ceph    默认三副本, 大数据自动切割为小文件(要求至少三个osd)
                    若只装前两个 只能块共享
            mds     文件系统
            rgw     提供对象存储 (类似于 百度云盘)
            clients 客户端
            journal(英文 日志) 缓存盘(生产环境需要ssd) 当用户储存文件时 ceph 会先存入缓存盘中,当ceph系统空闲的时候,转入后端 ceph储存盘

        要求
                准备四台KVM虚拟机，其三台作为存储集群节点，一台安装为客户端，实现如下功能：
                创建1台客户端虚拟机
                创建3台存储集群虚拟机
                配置主机名、IP地址、YUM源
                修改所有主机的主机名
                配置无密码SSH连接
                配置NTP时间同步
                创建虚拟机磁盘

            准备1 clients 3 node
            ip   4.10 4.11. 4.12 4.13

            1.安装准备
                1）物理机为所有节点配置yum源服务器。
                    提示：ceph10.iso在/linux-soft/02目录。
                    [root@room9pc01 ~]# mkdir  /var/ftp/ceph
                    [root@room9pc01 ~]# mount ceph10.iso /var/ftp/ceph/
                2）配置无密码连接(包括自己远程自己也不需要密码)，在node1操作。
                    [root@node1 ~]# ssh-keygen   -f /root/.ssh/id_rsa    -N ''
                    [root@node1 ~]# for i in 10  11  12  13
                     do
                         ssh-copy-id  192.168.4.$i
                     done
                3）修改/etc/hosts并同步到所有主机。
                    警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
                     [root@node1 ~]# cat /etc/hosts
                    ... ...
                    192.168.4.10  client
                    192.168.4.11     node1
                    192.168.4.12     node2
                    192.168.4.13     node3
                    警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
                    [root@node1 ~]# for i in 10  11  12  13
                    do
                    scp  /etc/hosts  192.168.4.$i:/etc/
                    done
                4）修改所有节点都需要配置YUM源，并同步到所有主机。
                    [root@node1 ~]# cat /etc/yum.repos.d/ceph.repo
                    [mon]
                    name=mon
                    baseurl=ftp://192.168.4.254/ceph/MON
                    gpgcheck=0
                    [osd]
                    name=osd
                    baseurl=ftp://192.168.4.254/ceph/OSD
                    gpgcheck=0
                    [tools]
                    name=tools
                    baseurl=ftp://192.168.4.254/ceph/Tools
                    gpgcheck=0
                    [root@node1 ~]# yum repolist                #验证YUM源软件数量
                    源标识            源名称                    状态
                    Dvd                redhat                    9,911
                    Mon                mon                        41
                    Osd                osd                        28
                    Tools            tools                    33
                    repolist: 10,013
                    [root@node1 ~]# for i in  10  11  12  13
                    do
                    scp  /etc/yum.repos.d/ceph.repo  192.168.4.$i:/etc/yum.repos.d/
                    done
                    5）所有节点主机与真实主机的NTP服务器同步时间。
                    提示：默认真实物理机已经配置为NTP服务器。
                    [root@node1 ~]# vim /etc/chrony.conf
                    … …
                    server 192.168.4.254   iburst
                    [root@node1 ~]# for i in 10 11 12 13
                    do
                         scp /etc/chrony.conf 192.168.4.$i:/etc/
                         ssh 192.168.4.$i "systemctl restart chronyd"
                     done
               5)准备存储磁盘

                    物理机上为每个虚拟机准备3块磁盘（可以使用命令，也可以使用图形直接添加）。
                     [root@room9pc01 ~]# virt-manager

            2.安装部署ceph集群

                要求
                    安装部署工具ceph-deploy
                    创建ceph集群
                    准备日志磁盘分区
                    创建OSD存储空间
                    查看ceph状态，验证
                ##################################
                思路
                    安装 ceph和插件
                        yum -y instal ceph-deploy
                        yum -y install ceph-mon ceph-osd ceph-mds ceph-radosgw
                        [monitor 是作为 监控主机的 osd 作为储存主机 ,扩容时 扩容osd 主机即可(将新的主机加入集群,修改配置文件;安装 ceph-osd.后准备磁盘 ,格式化 创建 osd 空间) ]
                    准备缓存盘(/dev/vdb)和磁盘(/dev/vdc d)
                        格式化磁盘
                        修改磁盘属主属组 为 ceph 确定其永久化设置
                    格式化osd磁盘(node1 node2 node3 只需要在node1 执行)
                        ceph-deploy disk  zap  node1:vdc   node1:vdd
                    创建OSD存储空间(node1 node2 node3 只需要在node1 执行)
                        ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2

                    检测状态
                        ceph -s
                    到这一步 块设备 就共享完成了
                    不考虑文件系统和对象存储
                ####################################################
                1）在node1安装部署工具，学习工具的语法格式。
                    [root@node1 ~]#  yum -y install ceph-deploy
                    #############################################
                    这个脚本需要无密码登录的支持
                    作用:远程链接 osd 启动服务 修改配置文件;;;当然没有也可以只不过需要手动配置 想想上千台
                    [root@node1 ~]#  ceph-deploy  --help    查询ceph-deploy 命令用法
                    [root@node1 ~]#  ceph-deploy mon --help 查询ceph-deploy mod 命令用法
                2）创建目录
                    [root@node1 ~]#  mkdir ceph-cluster
                    [root@node1 ~]#  cd ceph-cluster/   这个脚本只能在这个目录下执行,否则报错

            2.部署Ceph集群
                1）创建Ceph集群配置,在ceph-cluster目录下生成Ceph配置文件。
                    在ceph.conf配置文件中定义monitor主机是谁。
                    [root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3
                    查看配置文件
                    [root@node1 ceph-cluster]# tailf ceph.conf
                    [global]
                    fsid = 6cfa3825-ca90-4d77-ae58-410bfb9cb0fd
                    集群包含的主机
                    mon_initial_members = node1, node2, node3
                    mon_host = 192.168.4.11,192.168.4.12,192.168.4.13
                    ##########需要密码#####密码文件在ceph.mon.keyring 中
                    auth_cluster_required = cephx ####密码占位符
                    auth_service_required = cephx
                    auth_client_required = cephx

                2）给所有节点安装ceph相关软件包。
                    [root@node1 ceph-cluster]# for i in node1 node2 node3
                    do
                        ssh  $i "yum -y install ceph-mon ceph-osd ceph-mds ceph-radosgw"
                    done
                3）初始化所有节点的mon服务，也就是启动mon服务（主机名解析必须对）。
                    [root@node1 ceph-cluster]# ceph-deploy mon create-initial


                4)常见错误及解决方法（非必要操作，有错误可以参考）：
                    如果提示如下错误信息：
                    [node1][ERROR ] admin_socket: exception getting command descriptions: [Error 2] No such file or directory


                解决方案如下（在node1操作）：
                    先检查自己的命令是否是在ceph-cluster目录下执行的！！！！如果确认是在该目录下执行的create-initial命令，依然报错，可以使用如下方式修复。
                    [root@node1 ceph-cluster]# vim ceph.conf      #文件最后追加以下内容
                    public_network = 192.168.4.0/24
                    修改后重新推送配置文件:
                    [root@node1 ceph-cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3


            3.创建OSD
                备注：每台电脑的由vbd1分区而成(vdb1和vdb2)这两个分区用来做存储服务器的journal缓存盘。
                1)准备
                    [root@node1 ceph-cluster]# for i in node1 node2 node3
                    do
                         ssh $i "parted /dev/vdb mklabel gpt"
                         ssh $i "parted /dev/vdb mkpart primary 1 50%"
                         ssh $i "parted /dev/vdb mkpart primary 50% 100%"
                     done
                2)磁盘分区后的默认权限无法让ceph软件对其进行读写操作，需要修改权限。

                    node1、node2、node3都需要操作，这里以node1为例。
                    [root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb1
                    [root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb2
                    #上面的权限修改为临时操作，重启计算机后，权限会再次被重置。
                    #我们还需要将规则写到配置文件实现永久有效。
                    #规则：如果设备名称为/dev/vdb1则设备文件的所有者和所属组都设置为ceph。
                    #规则：如果设备名称为/dev/vdb2则设备文件的所有者和所属组都设置为ceph。
                    ###########################################################
                    [root@node1 ceph-cluster]# vim /etc/udev/rules.d/70-vdb.rules
                    ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
                    ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
                    将配置文件传递node2　node3
                    [root@node1 rules.d]# scp 70-vdb.rules root@node3:/etc/udev/rules.d/
                    [root@node1 rules.d]# scp 70-vdb.rules root@node2:/etc/udev/rules.d/

                3）初始化清空磁盘数据（仅node1操作即可）。 ceph 会远程链接node2 3 进行处理
                    [root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:vdc   node1:vdd
                    [root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:vdc   node2:vdd
                    [root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:vdc   node3:vdd


                4）创建OSD存储空间（仅node1操作即可）
                    重要：很多同学在这里会出错！将主机名、设备名称输入错误！！！
                    [root@node1 ceph-cluster]# ceph-deploy osd create \
                     node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
                    //创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
                    //一个存储设备对应一个缓存设备，缓存需要SSD，不需要很大
                    [root@node1 ceph-cluster]# ceph-deploy osd create \
                     node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
                    [root@node1 ceph-cluster]# ceph-deploy osd create \
                     node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2

                    常见错误及解决方法（非必须操作）。
                    使用osd create创建OSD存储空间时，如提示下面的错误提示：
                    [ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'
                    可以使用如下命令修复文件，重新配置ceph的密钥文件：
                    [root@node1 ceph-cluster]#  ceph-deploy gatherkeys node1 node2 node3


            4.检测状态
                1) 查看集群状态
                    [root@node1 ceph-cluster]# ceph -s
                    cluster 6cfa3825-ca90-4d77-ae58-410bfb9cb0fd
                     health HEALTH_OK
                     monmap e2: 3 mons at {node1=192.168.4.11:6789/0,node2=192.168.4.12:6789/0,node3=192.168.4.13:6789/0}
                            election epoch 8, quorum 0,1,2 node1,node2,node3
                     osdmap e34: 6 osds: 6 up, 6 in
                            flags sortbitwise
                     pgmap v72: 64 pgs, 1 pools, 0 bytes data, 0 objects
                            202 MB used, 119 GB / 119 GB avail
                               64 active+clean

                2）常见错误（非必须操作）。
                     如果查看状态包含如下信息：
                     health: HEALTH_WARN
                            clock skew detected on  node2, node3…
                     clock skew表示时间不同步，解决办法：请先将所有主机的时间都使用NTP时间同步！！！
                     Ceph要求所有主机时差不能超过0.05s，否则就会提示WARN，如果使用NTP还不能精确同步时间，可以手动修改所有主机的ceph.conf，在[MON]下面添加如下一行：
                     mon clock drift allowed = 1
                     如果状态还是失败，可以尝试执行如下命令，重启ceph服务：
                     [root@node1 ~]#  systemctl restart ceph\*.service ceph\*.target


            5.创建Ceph块存储
                    使用Ceph集群的块存储功能:实现
                        创建块存储镜像
                        客户端映射镜像
                        创建镜像快照 #只对块设备有效
                        使用快照还原数据
                        使用快照克隆镜像
                        删除快照与镜像

                1）查看存储池
                    [root@node1 ~]# ceph osd lspools
                    0 rbd,
                2）创建镜像、查看镜像
                    [root@node1 ~]# rbd create demo-image --image-feature  layering --size 10G
                    [root@node1 ~]# rbd create rbd/image --image-feature  layering --size 10G  两种方法创建 镜像

                    #这里的demo-image和image为创建的镜像名称，可以为任意字符。
                    #--image-feature参数指定我们创建的镜像有哪些功能，layering是开启COW功能。
                    #提示：ceph镜像支持很多功能，但很多是操作系统不支持的，我们只开启layering。

                    [root@node1 ~]# rbd list
                    [root@node1 ~]# rbd info demo-image
                    rbd image 'demo-image':
                        size 10240 MB in 2560 objects
                        order 22 (4096 kB objects)
                        block_name_prefix: rbd_data.d3aa2ae8944a
                        format: 2
                        features: layering
            6.动态调整
                1）缩小容量
                    [root@node1 ~]# rbd resize --size 7G image --allow-shrink
                    [root@node1 ~]# rbd info image
                2）扩容容量
                    [root@node1 ~]# rbd resize --size 15G image
                    [root@node1 ~]# rbd info image
            7.通过KRBD访问

                1）客户端通过KRBD访问
                    #客户端需要安装ceph-common软件包
                    #拷贝配置文件（否则不知道集群在哪）
                    #拷贝连接密钥（否则无连接权限）
                    [root@client ~]# yum -y  install ceph-common
                    [root@client ~]# scp 192.168.4.11:/etc/ceph/ceph.conf  /etc/ceph/
                    [root@client ~]# scp 192.168.4.11:/etc/ceph/ceph.client.admin.keyring \
                    /etc/ceph/
                    #######################################################
                    块共享只要用户有权限就可以挂载,可以同时读,但是不可以同时写入.
                    文件系统 共享 就可以同时读写.
                    #######################################################
                    [root@client ~]# rbd map image
                    ###########将块设备连接到本机###########
                    [root@client ~]#  lsblk
                    [root@client ~]# rbd showmapped
                    #####查看块设备的信息#########
                    id pool image snap device
                    0  rbd  image -    /dev/rbd0
                2) 客户端格式化、挂载分区
                    [root@client ~]# mkfs.xfs /dev/rbd0
                    [root@client ~]# mount /dev/rbd0 /mnt/
                    [root@client ~]# echo "test" > /mnt/test.txt

            8.创建镜像快照
                1) 查看镜像快照（默认所有镜像都没有快照）。
                    [root@node1 ~]# rbd snap ls image  (image指的是刚创建的 镜像 )
                2) 给镜像创建快照。
                    [root@node1 ~]# rbd snap create image --snap image-snap1
                    #为image镜像创建快照，快照名称为image-snap1
                    [root@node1 ~]# rbd snap ls image
                    SNAPID NAME            SIZE
                         4 image-snap1 15360 MB
                3) 删除客户端写入的测试文件
                l    [root@client ~]# rm  -rf   /mnt/test.txt
                    不允许在线还原下线后还原
                    [root@client ~]# umount  /mnt
                4) 还原快照
                    [root@node1 ~]# rbd snap rollback image --snap image-snap1
                    #客户端重新挂载分区
                    [root@client ~]# mount /dev/rbd0 /mnt/
                    [root@client ~]# ls  /mnt
            9.创建快照克隆
                1)快照克隆
                     rbd snap protect image --snap image-snap1
                     rbd snap rm image --snap image-snap1 //会失败 ,已经保护了
                     rbd clone \
                     image --snap image-snap1 image-clone --image-feature layering
                    //使用image的快照image-snap1克隆一个新的名称为image-clone镜像

                2)查看克隆镜像与父镜像快照的关系
                    [root@node1 ~]#  rbd info image-clone
                    rbd image 'image-clone':
                        size 15360 MB in 3840 objects
                        order 22 (4096 kB objects)
                        block_name_prefix: rbd_data.d3f53d1b58ba
                        format: 2
                        features: layering
                        flags:
                        parent: rbd/image@image-snap1
                    #克隆镜像很多数据都来自于快照链
                    #如果希望克隆镜像可以独立工作，就需要将父快照中的数据，全部拷贝一份，但比较耗时！！！
                    [root@node1 ~]#  rbd flatten image-clone
                    [root@node1 ~]#  rbd info image-clone
                    rbd image 'image-clone':
                        size 15360 MB in 3840 objects
                        order 22 (4096 kB objects)
                        block_name_prefix: rbd_data.d3f53d1b58ba
                        format: 2
                        features: layering
                        flags:
                    #注意，父快照信息没了！
                    [root@node1 ~]#  rbd snap unprotect image --snap image-snap1     #取消快照保护
                    [root@node1 ~]#  rbd snap rm image --snap image-snap1            #可以删除快照

                3） 客户端撤销磁盘映射
                    [root@client ~]# umount /mnt
                    [root@client ~]# rbd showmapped
                    id pool image        snap device
                    0  rbd  image        -    /dev/rbd0
                    //语法格式:
                    [root@client ~]# rbd unmap /dev/rbd0


        块储存应用案例
            要求:使用上次实验,演示块存储在KVM虚拟化中的应用案例，实现以下功能：
                Ceph创建块存储镜像
                客户端安装部署ceph软件
                客户端部署虚拟机
                客户端创建secret
                设置虚拟机配置文件，调用ceph存储

            方法:
                使用Ceph存储创建镜像。
                KVM虚拟机调用Ceph镜像作为虚拟机的磁盘。
            1)创建磁盘镜像
                [root@node1 ~]# rbd create vm1-image --image-feature  layering --size 10G
                [root@node1 ~]# rbd  list
                [root@node1 ~]# rbd  info  vm1-image
                [root@node1 ~]# qemu-img  info  rbd:rbd/vm1-image
                image: rbd:rbd/vm1-image
                file format: raw
                virtual size: 10G (10737418240 bytes)
                disk size: unavailable
            2）Ceph认证账户（仅查看即可）。
                Ceph默认开启用户认证，客户端需要账户才可以访问，默认账户名称为client.admin，key是账户的密钥。
                可以使用ceph auth添加新账户（案例我们使用默认账户）。
                [root@node1 ~]# cat /etc/ceph/ceph.conf                    //配置文件
                [global]
                mon_initial_members = node1, node2, node3
                mon_host = 192.168.2.10,192.168.2.20,192.168.2.30
                auth_cluster_required = cephx                                //开启认证
                auth_service_required = cephx                                //开启认证
                auth_client_required = cephx                                //开启认证
                [root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring        //账户文件
                [client.admin]
                    key = AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
            3）创建KVM虚拟机（注意：这里使用真实机当客户端！！！）。
                 使用virt-manager创建2台普通的KVM虚拟机。
            4）配置libvirt secret（注意：这里使用真实机当客户端！！！）。
                编写账户信息文件，让KVM知道ceph的账户名称。
                [root@room9pc01 ~]# vim secret.xml            //新建临时文件，内容如下
                <secret ephemeral='no' private='no'>
                        <usage type='ceph'>
                                <name>client.admin secret</name>
                        </usage>
                </secret>
                #使用XML配置文件创建secret
                [root@room9pc01 ~]# virsh secret-define secret.xml
                733f0fd1-e3d6-4c25-a69f-6681fc19802b
                //随机的UUID，这个UUID对应的有账户信息
            5)给secret绑定admin账户的密码，密码参考 node1 /etc/ceph/ceph.client.admin.keyring文件

                [root@room9pc01] virsh secret-set-value \
                --secret 733f0fd1-e3d6-4c25-a69f-6681fc19802b \
                --base64 AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg
                //这里secret后面是之前创建的secret的UUID
                //base64后面是client.admin账户的密码
                //现在secret中既有账户信息又有密钥信息
            6）虚拟机的XML配置文件。
                每个虚拟机都会有一个XML配置文件，包括：
                虚拟机的名称、内存、CPU、磁盘、网卡等信息。
                [root@room9pc01 ~]# vim /etc/libvirt/qemu/vm1.xml
                //修改前内容如下
                <disk type='file' device='disk'>
                      <driver name='qemu' type='qcow2'/>
                      <source file='/var/lib/libvirt/images/vm1.qcow2'/>
                      <target dev='vda' bus='virtio'/>
                      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
                    </disk>
                不推荐直接使用vim修改配置文件，推荐使用virsh edit修改配置文件，效果如下：
                [root@room9pc01] virsh edit vm1                //vm1为虚拟机名称
                <disk type='network' device='disk'>
                      <driver name='qemu' type='raw'/>
                      <auth username='admin'>
                      <secret type='ceph' uuid='733f0fd1-e3d6-4c25-a69f-6681fc19802b'/>
                      </auth>
                      <source protocol='rbd' name='rbd/vm1-image'>          <host name='192.168.4.11' port='6789'/>     </source>
                    <target dev='vda' bus='virtio'/>
                      <address type='pci' domain='0x0000' bus='0x09' slot='0x08' function='0x0'/>
                 </disk>
                注意：如果有设备编号冲突的情况下，需要修改设备编号，任意修改一个数字即可。


                磁盘格式化前 叫块设备 完成后叫 文件系统
                fat32 ntfs ext4 xfs
                格式化后文件系统:
                iNode 默认254字节/每块 : 存放在block中存放的文件(数据,也叫元数据,数据的数据)的详细信息(在哪里存放了什么 权限 属主 大小 创建时间)  : 占磁盘的 少部分
                block 默认4k/每块     : 存放文件 数据 : 占磁盘的大部分

                ######这两部分默认的 大小可以调整####
                当磁盘存小文件的时候就不要用 大的block
                当磁盘存大文件的时候就不要用 小的block
                当block的数量 太大的时候寻址就会太慢
                删 压根就没删除
                写 真的写上去了
                mkfs.ext4 -i -b  /dev/vdbc


        Ceph文件系统

            问题:延续前面的实验，实现Ceph文件系统的功能。具体实现有以下功能：
                部署MDSs节点
                创建Ceph文件系统
                客户端挂载文件系统
            思路:
                创建两个rbd 共享池
                合并两个池子 a b(假装是一块硬盘)
                安装ceph-mds //支持文件系统

            注意 : 一下 方法和 案例中不同
                选取 node3
            1) 拷贝配置文件启动服务
                由于ceph-mds 已经安装到 node1-3 中 所以在 只需要在node1中
                node1 中操作
                先进入目录  才能 使用 ceph-deploy
                cd ceph-cluster/
                ceph-deploy mds create node3
                来启动服务
            2)制作共享池
                ceph中只能创建一个文件系统
                不同于 nfs samba
                node3中操作
                创建共享池
                #######################################################
                [root@node3 ~]# ceph osd pool create cephfs_data 128
                //创建存储池，对应128个PG
                [root@node3 ~]# ceph osd pool create cephfs_metadata 128
                //创建存储池，对应128个PG
                    PG  是个逻辑上的概念 ,  1 osd 对应 若干个  PG ,这里PG 相当于文件夹个数  且数量需为 2^n 个

                metadata 元数据
                创建文件系统
                #######################################################################

                创建Ceph文件系统
                [root@node3 ~]# ceph mds stat                     //查看mds状态
                e2:, 1 up:standby
                [root@node3 ~]# ceph fs new myfs1 cephfs_metadata cephfs_data
                new fs with metadata pool 2 and data pool 1
                //注意，先写medadata池，再写data池
                //默认，只能创建1个文件系统，多余的会报错
                检查
                [root@node3 ~]# ceph fs ls
                name: myfs1, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
                [root@node4 ~]# ceph mds stat
                e4: 1/1/1 up {0=node3=up:creating}
            6）客户端挂载
                [root@client ~]# mount -t ceph 192.168.4.13:/  /mnt/cephfs/ \
                -o name=admin,secret=AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
                //注意:文件系统类型为ceph
                //192.168.4.13为MON节点的IP（不是MDS节点）
                //admin是用户名,secret是密钥
                //密钥可以在/etc/ceph/ceph.client.admin.keyring中找到

        ceph 对象存储
            软件 : ceph-rad


            创建对象存储服务器
                要求:

                延续前面的实验，实现Ceph对象存储的功能。具体实现有以下功能：
                安装部署Rados Gateway
                启动RGW服务
                设置RGW的前端服务与端口
                客户端测试


            1.部署对象存储服务器

                1）准备实验环境，要求如下：
                    IP地址:192.168.4.15
                    主机名:node5
                    配置yum源（包括rhel、ceph的源）
                    与Client主机同步时间
                    node1允许无密码远程node5
                    修改node1的/etc/hosts，并同步到所有node主机
                2）部署RGW软件包
                    [root@node1 ~]# ceph-deploy install --rgw node5
                    同步配置文件与密钥到node5
                    [root@node1 ~]# cd /root/ceph-cluster
                    [root@node1 ~]# ceph-deploy admin node5
                3）新建网关实例
                    启动一个rgw服务
                    [root@node1 ~]# ceph-deploy rgw create node5
                    登陆node5验证服务是否启动
                    [root@node5 ~]# ps aux |grep radosgw
                    ceph      4109  0.2  1.4 2289196 14972 ?       Ssl  22:53   0:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node4 --setuser ceph --setgroup ceph
                    [root@node5 ~]# systemctl  status ceph-radosgw@\*
                4）修改服务端口
                    登陆node5，RGW默认服务端口为7480，修改为8000或80更方便客户端记忆和使用
                    [root@node5 ~]#  vim  /etc/ceph/ceph.conf
                    [client.rgw.node5]
                    host = node5
                    rgw_frontends = "civetweb port=8000"
                    //node5为主机名
                    //civetweb是RGW内置的一个web服务
            2.客户端测试（扩展选做实验）

                1）curl测试
                    [root@client ~]# curl  192.168.4.15:8000
                    <?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
                2）使用第三方软件访问
                                                登陆node5（RGW）创建账户
                    [root@node5 ~]#  radosgw-admin user create \
                    --uid="testuser" --display-name="First User"
                    … …
                    "keys": [
                            {
                                "user": "testuser",
                                "access_key": "5E42OEGB1M95Y49IBG7B",
                                "secret_key": "i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6"
                            }
                        ],
                    ... ...
                    #
                    [root@node5 ~]# radosgw-admin user info --uid=testuser
                    //testuser为用户，key是账户访问密钥
                3）客户端安装软件
                    [root@client ~]#  yum install s3cmd-2.0.1-1.el7.noarch.rpm
                    修改软件配置（注意，除了下面设置的内容，其他提示都默认回车）
                    [root@client ~]#  s3cmd --configure
                    Access Key: 5E42OEGB1M95Y49IBG7BSecret Key: i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6
                    S3 Endpoint [s3.amazonaws.com]: 192.168.4.15:8000
                    [%(bucket)s.s3.amazonaws.com]: %(bucket)s.192.168.4.15:8000
                    Use HTTPS protocol [Yes]: No
                    Test access with supplied credentials? [Y/n] n
                    Save settings? [y/N] y
                    //注意，其他提示都默认回车
                4）创建存储数据的bucket（类似于存储数据的目录）
                    [root@client ~]# s3cmd ls
                    [root@client ~]# s3cmd mb s3://my_bucket
                    Bucket 's3://my_bucket/' created
                    [root@client ~]# s3cmd ls
                    2018-05-09 08:14 s3://my_bucket
                    [root@client ~]# s3cmd put /var/log/messages s3://my_bucket/log/
                    [root@client ~]# s3cmd ls
                    2018-05-09 08:14 s3://my_bucket
                    [root@client ~]# s3cmd ls s3://my_bucket
                    DIR s3://my_bucket/log/
                    [root@client ~]# s3cmd ls s3://my_bucket/log/
                    2018-05-09 08:19 309034 s3://my_bucket/log/messages
                    测试下载功能
                    [root@client ~]# s3cmd get s3://my_bucket/log/messages /tmp/
                    测试删除功能
                    [root@client ~]# s3cmd del s3://my_bucket/log/message

